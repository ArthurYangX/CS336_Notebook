\setchapterabstract{æœ¬æ¬¡è®²åº§å°†ä»åº•å±‚å‡ºå‘ï¼Œç³»ç»Ÿåœ°ä»‹ç»è®­ç»ƒæ¨¡å‹æ‰€éœ€çš„å„ç±» \textbf{primitives}ï¼šä» \textbf{tensors} åˆ° \textbf{models}ã€\textbf{optimizers}ï¼Œå†åˆ°å®Œæ•´çš„ \textbf{training loop}ï¼Œå¹¶ç€é‡è®¨è®ºæ•ˆç‡ä¼˜åŒ–ã€‚\\
åœ¨æ•´ä¸ªæµç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ç²¾ç»†è¡¡é‡å¹¶ç®¡ç†ä¸¤å¤§å…³é”®èµ„æºï¼š\textbf{Memory (GB)} ä¸ \textbf{Compute (FLOPs)}ï¼ŒåŠ©åŠ›å®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„æ¨¡å‹è®­ç»ƒã€‚
}

\vspace{-10cm}
\chapter{PyTorch, Resource Accounting}

\vspace{-2cm}

%%%%%%INSERT TOC BELOW 1ST SECTION%%%%%%%%%%%%

{\chaptoc\noindent\begin{minipage}[inner sep=0,outer sep=0]{0.9\linewidth}\section{Original Transformer}\end{minipage}}

èƒŒæ™¯ï¼šåºåˆ—æ¨å¯¼æ¨¡å‹ï¼ˆSequence Transduction Modelingï¼‰é•¿æœŸä¾èµ– RNN/CNNï¼Œå…¶è®¡ç®—å—åˆ¶äºæ—¶é—´æ­¥éª¤çš„ä¸²è¡Œæ€§ï¼Œéš¾ä»¥å¹¶è¡ŒåŒ–ï¼Œä¸”é•¿è·ç¦»ä¾èµ–æ•è·æ•ˆç‡ä½ã€‚

åˆ›æ–°ç‚¹ï¼šä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†Transformeræ¨¡å‹ï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œå»é™¤äº†å¾ªç¯å’Œå·ç§¯æ“ä½œï¼Œä»è€Œå®ç°äº†å…¨å±€ä¿¡æ¯äº¤äº’ã€‚

æŠ€æœ¯è·¯å¾„ï¼šåˆ©ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Attentionï¼‰+ å‰é¦ˆç½‘ç»œ ï¼ˆFeed Forward Networkï¼‰çš„ç¼–ç å™¨-è§£ç å™¨ï¼Œå»é™¤äº†å¾ªç¯/å·ç§¯ï¼Œä»…é æ³¨æ„åŠ›å®ç°å…¨å±€ä¿¡æ¯äº¤äº’ã€‚ é™¤æ­¤ä¹‹å¤–ï¼Œä»–ä»¬è¿˜è®¾è®¡äº†Scaled Dot-Product Attention å’Œ Multi Head Attentionï¼Œæ¥é™ä½æ•°å€¼å°ºåº¦çš„åŒæ—¶å¹¶è¡Œå­¦ä¹ å¤šå­ç©ºé—´è¡¨ç¤ºã€‚


ä»¥ä¸‹æ˜¯Transformerçš„æ•´ä½“æ¨¡å‹å›¾ï¼š

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\linewidth]{figs/lec2/lec2.01.png}
  \caption{The Transformer-model architecture}
  \label{fig: Transformer architecture}
\end{figure}

\clearpage
å¦‚å›¾Figure \ref{fig: Transformer architecture}æ‰€ç¤ºï¼ŒTransformerç”±
\begin{enumerate}
  \item \textbf{Input Embedding}
  \item \textbf{Positional Encoding}
  \item \textbf{Encoder ç»“æ„}ï¼š Self-Attention, Feed-Forward
    % \begin{enumerate}
    %   \item \textbf{Multi-Head Self-Attention} + \textbf{Add \& Norm}
    %   \item \textbf{Feed-Forward} + \textbf{Add \& Norm}
    % \end{enumerate}
  \item \textbf{Decoder ç»“æ„}: Masked self-Attention, Encoder Attention, Feed-Forward
    % \begin{enumerate}
    %   \item \textbf{Masked Multi-Head Self-Attention} + \textbf{Add \& Norm}
    %   \item \textbf{Encoderâ€“Decoder Attention} + \textbf{Add \& Norm}
    %   \item \textbf{Feed-Forward} + \textbf{Add \& Norm}
    % \end{enumerate}
  \item \textbf{Output Projection \& Softmax}
\end{enumerate}
è¿™å‡ ä¸ªæ¨¡å—ç»„æˆï¼Œæ•´ä½“æ˜¯Encoder-Decoderç»“æ„ï¼Œå…¶ä¸­Encoderå’ŒDecoderå‡ç”±å¤šä¸ªç›¸åŒçš„å±‚å †å è€Œæˆï¼Œæˆ‘ä»¬ç§°å…¶ä¸ºTransformer Blockã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¼šé€ä¸ªä»‹ç»è¿™äº›æ¨¡å—ã€‚

\clearpage

{\chaptoc\noindent
\begin{minipage}[inner sep=0,outer sep=0]{0.9\linewidth}
  \subsection{Input Embedding}
\end{minipage}}

\Insight{Input Embedding}{å°†è¾“å…¥çš„æ–‡æœ¬åºåˆ—é€šè¿‡tokenizerè½¬åŒ–ä¸ºtokenå†ç”±embedding methodè½¬åŒ–ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°å€¼å‘é‡ã€‚}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\linewidth]{figs/lec2/lec2.02.png}
  \caption{Input $\xrightarrow{}$ Tokens $\xrightarrow{}$ Embedding Vectors}
  \label{fig:Input Embedding Box}
\end{figure}

\subsubsection{Tokenization} 
è¯·å‚è€ƒLecture1 Section 4
\subsubsection{Embedding}

\textbf{{\color{tred} Goal: Find the best numerical representation for these tokens that the model can use to calculate and properly model the patterns in the text.}}

\Definition{Embedding Conversion}{Transforming these tokens into numerical representations that capture their meaning and patterns.}

\paragraph{1. Static Token Embeddings vs. Contextualized Embeddings}
\begin{itemize}
    \item \textbf{Static Token Embeddings}ï¼šä¸ºæ¯ä¸ª token åˆ†é…ä¸€ä¸ªå”¯ä¸€ä¸”å›ºå®šçš„å‘é‡è¡¨ç¤ºï¼Œå¸¸è§å®ç°åŒ…æ‹¬ word2vecã€GloVe åŠ fastText ç­‰ã€‚å®ƒä»¬åœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šé¢„å…ˆè®­ç»ƒå®Œæˆåï¼Œæ¯ä¸ª token çš„å‘é‡åœ¨ä»»æ„ä¸Šä¸‹æ–‡ä¸­å‡ä¿æŒä¸å˜ã€‚
    \begin{itemize}
        \item ä¼˜ç‚¹ï¼šæŸ¥è¡¨å¿«é€Ÿã€è®¡ç®—å¼€é”€å°
        \item ç¼ºç‚¹ï¼šæ— æ³•åŒºåˆ†åŒå½¢å¼‚ä¹‰è¯
    \end{itemize}
    \item \textbf{Contextualized Embeddings}ï¼šç”±åŸºäºè¯­è¨€æ¨¡å‹åœ¨å‰å‘ä¼ æ’­æ—¶åŠ¨æ€ç”Ÿæˆã€‚æ¨¡å‹ä¼šç»“åˆæ•´ä¸ªè¾“å…¥åºåˆ—åŠå…¶é‚»è¿‘ä¿¡æ¯ï¼Œå¯¹åŒä¸€ token äº§ç”Ÿä¸åŒçš„å‘é‡ï¼Œå®ç°è¯­ä¹‰æ¶ˆæ­§ä¸æ›´ä¸°å¯Œçš„è¡¨è¾¾ã€‚
\end{itemize}

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/lec2/lec2.03.png}
    \caption{Static Embedding}
    \label{fig:static-embedding}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/lec2/lec2.04.png}
    \caption{Contextualized Embedding}
    \label{fig:contextualized-embedding}
  \end{subfigure}
  \caption{é™æ€åµŒå…¥ vs. ä¸Šä¸‹æ–‡åŒ–åµŒå…¥å¯¹æ¯”}
  \label{fig:embeddings-comparison}
\end{figure}

\MarginImageWithNote
  {figs/lec2/lec2.05.png}
  {\captionof{figure}{A language model operates on raw, static embeddings as its input and produces contextual text embeddings.}}

\paragraph{2. Text and Sentence Embeddings}~{}
        
\Definition{Text/Sentence Embeddings}{Use a single vector to represent the text/ sentence instead of just one token and captures its meaning.}




ä¸ºäº†å¯¹æ•´ä¸ªå¥å­ã€æ®µè½æˆ–æ–‡æ¡£è¿›è¡Œè¯­ä¹‰è¡¨ç¤ºï¼Œé€šå¸¸éœ€è¦å°†å¤šä¸ª token åµŒå…¥åˆæˆä¸ºä¸€ä¸ªå®šé•¿å‘é‡ã€‚å¸¸è§æ–¹æ³•åŒ…æ‹¬ï¼š
\begin{itemize}
  \item \textbf{Mean Poolingï¼ˆå¹³å‡æ± åŒ–ï¼‰}ï¼šå¯¹æ¨¡å‹è¾“å‡ºçš„æ‰€æœ‰ token åµŒå…¥å–åºåˆ—ç»´å¹³å‡ï¼Œ
  \item \textbf{[CLS] å‘é‡}ï¼šå¯¹äº BERT ç³»åˆ—æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨åºåˆ—å¼€å¤´çš„ \texttt{[CLS]} å¯¹åº”éšè—çŠ¶æ€ \(\mathbf{h}_\text{[CLS]}\) ä½œä¸ºæ•´å¥è¡¨ç¤ºï¼Œæ— éœ€é¢å¤–èšåˆæ“ä½œã€‚
  \item \textbf{ä¸“ç”¨ Sentence Embedding æ¨¡å‹}ï¼šå¦‚ Sentence-BERTã€MPNet ç­‰ï¼Œåœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šé’ˆå¯¹å¥å­çº§ä»»åŠ¡å¾®è°ƒï¼Œç›´æ¥è¾“å‡ºé«˜è´¨é‡çš„å¥å­å‘é‡ã€‚
\end{itemize}

\paragraph{3. Word2Vec}~{}

åœ¨\href{https://arxiv.org/abs/1301.3781}{Efficient estimation of word representations in vector space} æå‡ºäº†ä¸¤ç§æ¡†æ¶ï¼Œåˆ†åˆ«æ˜¯Continuous Bag-of-Words Model(CBOW)å’ŒContinuous Skip-gram Model. 


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\linewidth]{figs/lec2/lec2.06.pdf}
  \caption{CBOWå’ŒSkip-gram architectureç¤ºæ„å›¾}
  \label{fig:Word2Vec}
\end{figure}

\begin{itemize}
  \item \textbf{é¢„æµ‹æ–¹å‘ä¸åŒ}ï¼š  
    CBOW ä»¥ä¸Šä¸‹æ–‡è¯ï¼ˆå‰åå„ $m$ ä¸ªè¯ï¼‰ä¸ºè¾“å…¥ï¼Œé¢„æµ‹ä¸­å¿ƒè¯ $w_t$ï¼›  
    Skip-gram åˆ™ä»¥ä¸­å¿ƒè¯ $w_t$ ä¸ºè¾“å…¥ï¼Œåˆ†åˆ«é¢„æµ‹å…¶ä¸Šä¸‹æ–‡è¯ $w_{t\pm j}$ã€‚  
  \item \textbf{è¾“å…¥è¾“å‡ºç»“æ„}ï¼š  
    CBOW å°†ä¸Šä¸‹æ–‡è¯å‘é‡å–å¹³å‡æˆ–æ±‚å’Œåè¾“å…¥ï¼Œè¾“å‡ºä¸­å¿ƒè¯çš„æ¦‚ç‡åˆ†å¸ƒï¼›  
    Skip-gram å¯¹æ¯ä¸ªä¸Šä¸‹æ–‡ä½ç½®éƒ½ç‹¬ç«‹è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œè¾“å‡ºå¤šä¸ªä¸Šä¸‹æ–‡è¯çš„åˆ†å¸ƒã€‚   
\end{itemize}



\clearpage
\subsection{Positional Encoding}
Transformer çš„ä¸€ä¸ªçªç ´æ˜¯å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œå»é™¤äº†å¾ªç¯ï¼ˆRNNï¼‰å’Œå·ç§¯ï¼ˆCNNï¼‰æ“ä½œï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¹¶è¡Œå¤„ç†è¾“å…¥åºåˆ—ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿå¸¦æ¥äº†ä¸€ä¸ªé—®é¢˜ï¼šæ¨¡å‹æ— æ³•æ„ŸçŸ¥è¾“å…¥åºåˆ—ä¸­å„ä¸ª token çš„ç›¸å¯¹æˆ–ç»å¯¹ä½ç½®ã€‚

ä¸ºäº†è®©æ¨¡å‹åœ¨æ³¨æ„åŠ›è®¡ç®—æ—¶æ„ŸçŸ¥æ¯ä¸ª token çš„ä½ç½®ä¿¡æ¯ï¼Œæˆ‘ä»¬åœ¨è¾“å…¥çš„ embedding ä¸­åŠ å…¥ä¸€ä¸ªä½ç½®å‘é‡ï¼ˆPositional Encodingï¼‰ï¼Œä½¿å¾—æ¯ä¸ª token çš„è¡¨ç¤ºä¸ä»…åŒ…å«å…¶è¯­ä¹‰ä¿¡æ¯ï¼Œè¿˜åŒ…å«å…¶åœ¨åºåˆ—ä¸­çš„ä½ç½®ä¿¡æ¯ã€‚

å…·ä½“æ¥è¯´ï¼Œå¯¹äºåºåˆ—ä¸­ä½ç½® $pos$ çš„ tokenï¼Œå…¶æœ€ç»ˆè¾“å…¥è¡¨ç¤ºä¸ºï¼š
\[
\mathbf{z}_{pos} = \mathbf{e}_{pos} + \mathbf{p}_{pos}
\]
å…¶ä¸­ï¼š
$\mathbf{e}_{pos} \in \mathbb{R}^{d_{\text{model}}}$ ä¸ºè¯¥ token çš„è¯å‘é‡ï¼ˆtoken embeddingï¼‰ï¼›
$\mathbf{p}_{pos} \in \mathbb{R}^{d_{\text{model}}}$ ä¸ºå¯¹åº”ä½ç½®çš„ä½ç½®ç¼–ç å‘é‡ã€‚

\paragraph{1.å›ºå®šæ­£ä½™å¼¦ä½ç½®ç¼–ç ï¼ˆVaswaniåŸç‰ˆï¼‰}~{}
\\
\MarginImageWithNote
  {figs/lec2/lec2.07.png}
  {\captionof{figure}{ä½ç½®ç¼–ç ç¤ºæ„å›¾}}

åœ¨åŸå§‹ Transformer ä¸­ï¼Œä½ç½®ç¼–ç  $\mathbf{p}_{pos}$ é‡‡ç”¨\textbf{å›ºå®šçš„æ­£ä½™å¼¦å‡½æ•°}å½¢å¼ï¼š
\[
\text{PE}(pos, 2i) = \sin\left( \frac{pos}{10000^{2i / d_{\text{model}}}} \right), \quad
\text{PE}(pos, 2i+1) = \cos\left( \frac{pos}{10000^{2i / d_{\text{model}}}} \right)
\]
å…¶ä¸­ $i$ ä¸ºå‘é‡ç»´åº¦çš„ä¸‹æ ‡ï¼ˆä» 0 å¼€å§‹ï¼‰ï¼Œå¶æ•°ç»´ä½¿ç”¨æ­£å¼¦ï¼Œå¥‡æ•°ç»´ä½¿ç”¨ä½™å¼¦ã€‚

è¿™ç§ç¼–ç æ–¹å¼çš„ä¼˜åŠ¿åœ¨äºï¼š
\begin{enumerate}
  \item \textbf{æ— å‚æ•°}ï¼šä¸ä¾èµ–è®­ç»ƒï¼Œåˆå§‹åŒ–å³å…·å¤‡åŒºåˆ†é¡ºåºçš„èƒ½åŠ›ï¼›
  \item \textbf{å¯å¤–æ¨}ï¼šæ”¯æŒæ¨ç†æ—¶å¤„ç†æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—ï¼›
  \item \textbf{ä¿ç•™ç›¸å¯¹ä½ç½®ä¿¡æ¯}ï¼šä»»æ„ä¸¤ä¸ªä½ç½®çš„ç¼–ç å·®å€¼ä¸å®ƒä»¬çš„è·ç¦»æˆè§„å¾‹æ€§å˜åŒ–ã€‚
\end{enumerate}

\paragraph{2.å¯å­¦ä¹ ä½ç½®ç¼–ç }~{}
\\
ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¦ä¸€ç§å¸¸ç”¨æ–¹æ³•æ˜¯å¯å­¦ä¹ ä½ç½®ç¼–ç ï¼ˆLearnable Positional Embeddingï¼‰ï¼Œå³ä¸ºæ¯ä¸ªä½ç½®ç›´æ¥åˆ†é…ä¸€ä¸ªå¯è®­ç»ƒå‘é‡ï¼š
\[
\mathbf{p}_{pos} = \text{Embedding}(pos)
\]
è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯çµæ´»æ€§å¼ºã€å¯ä¸ä»»åŠ¡æ•°æ®æ›´å¥½é€‚é…ï¼Œä½†åœ¨åºåˆ—é•¿åº¦è¶…è¿‡è®­ç»ƒèŒƒå›´æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›å¯èƒ½è¾ƒå¼±ã€‚

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
æ–¹æ³• & ä¼˜ç‚¹ & ç¼ºç‚¹ \\
\hline
æ­£ä½™å¼¦ä½ç½®ç¼–ç  & å¤–æ¨æ€§å¥½ï¼Œæ— éœ€è®­ç»ƒ & è¡¨è¾¾èƒ½åŠ›å›ºå®šï¼Œçµæ´»æ€§ä½ \\
å¯å­¦ä¹ ä½ç½®ç¼–ç  & çµæ´»æ€§å¼ºï¼Œé€‚åº”ä»»åŠ¡ & å¤–æ¨æ€§å·®ï¼Œéœ€é¢å¤–å‚æ•° \\
\hline
\end{tabular}
\caption{ä¸¤ç§å¸¸è§ä½ç½®ç¼–ç æ–¹æ³•çš„æ¯”è¾ƒ}
\end{table}


\clearpage
\subsection{Encoder ç»“æ„}
Encoder æœ‰Nä¸ªç›¸åŒçš„å±‚ï¼ˆTransformer Blockï¼‰å †å è€Œæˆï¼Œæ¯ä¸€å±‚éƒ½åŒ…å«ä¸¤ä¸ªå­å±‚ï¼šMulti-Head Self-Attention å’Œ Feed-Forward Networkã€‚æ¯ä¸ªå­å±‚åé¢éƒ½æœ‰ä¸€ä¸ªæ®‹å·®è¿æ¥ï¼ˆAdd \& Normï¼‰ã€‚

\MarginImageWithNote
  {figs/lec2/lec2.08.png}
  {\captionof{figure}{Transdormer Encoder Block from Vaswani et al. (2017)}}

æ¯ä¸€ä¸ªTransformer Blockç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š
\begin{enumerate}
  \item \textbf{Attentionå±‚}ï¼šè®¡ç®—è¾“å…¥åºåˆ—ä¸­å„ä¸ª token ä¹‹é—´çš„å…³ç³»ï¼Œæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
  \item \textbf{Feed-Forwardå±‚}ï¼šå¯¹æ¯ä¸ª token çš„è¡¨ç¤ºè¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚  
\end{enumerate} 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{figs/lec2/lec2.09.png}
  \captionof{figure}{Transformer Encoder Block from Vaswani et al. (2017)}
  \label{fig:Transformer Encoder Block}
\end{figure}

\subsubsection{å¤šå¤´è‡ªæ³¨æ„æœºåˆ¶}
Attentionæœºåˆ¶ä¸»è¦åšäº†ä¸¤ä»¶äº‹ï¼š
\begin{enumerate}
  \item A way to score how {\color{tred}relevant} each of the previous input tokens are to the current token being processed.

  \item Using those scores, we {\color{tred}combine the information} from the various positions into a single output vector.
\end{enumerate}   


To give the Transformer more extensive attention capability, the attention mechanism is {\color{tred}duplicated and executed multiple times in parallel}. Each of these parallel applications of attention is conducted into an {\color{dblue}\textit{attention head}}. This increases the modelâ€™s capacity to model complex patterns in the input sequence that require paying attention to different patterns at once.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{figs/lec2/lec2.10.png}
  \caption{We get better LLMs by doing attention multiple times in parallel, increasing the modelâ€™s capacity to attend to different types of information.}
  \label{fig:Multi-Head Attention}
\end{figure}

\clearpage  
åœ¨åŸç‰ˆçš„Transformerä¸­ï¼ŒMulti-Head Self-Attentionçš„è®¡ç®—å›¾å¦‚ä¸‹ï¼š
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\linewidth]{figs/lec2/lec2.11.png}
  \caption{Multi-Head Self-Attentionè®¡ç®—å›¾}
  \label{fig:Multi-Head Self-Attentionè®¡ç®—å›¾}
\end{figure}

\MarginNote{

\Remark{
\textbf{Self-Attention ä¸ Attention çš„åŒºåˆ«}ï¼š
\begin{itemize}
  \item \textbf{Attention} æ˜¯ä¸€ç§é€šç”¨æœºåˆ¶ï¼ŒQueryã€Keyã€Value å¯ä»¥æ¥è‡ªä¸åŒçš„åºåˆ—æˆ–ä¸åŒæ¨¡æ€çš„æ•°æ®ã€‚ä¾‹å¦‚åœ¨æœºå™¨ç¿»è¯‘ä¸­ï¼Œç¼–ç å™¨è¾“å‡ºä½œä¸º Key/Valueï¼Œè§£ç å™¨å½“å‰çŠ¶æ€ä½œä¸º Queryï¼Œè¿™è¢«ç§°ä¸º \emph{cross-attention}ã€‚
  \item \textbf{Self-Attention} æ˜¯ Attention çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œå…¶ä¸­ Queryã€Keyã€Value éƒ½æ¥è‡ªåŒä¸€ç»„è¾“å…¥åºåˆ—ã€‚å®ƒçš„ä½œç”¨æ˜¯åœ¨åŒä¸€åºåˆ—å†…éƒ¨å»ºç«‹ä½ç½®ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œè®©æ¯ä¸ªä½ç½®èƒ½å¤Ÿæ ¹æ®ç›¸å…³æ€§åŠ¨æ€èåˆæ¥è‡ªå…¶ä»–ä½ç½®çš„ä¿¡æ¯ã€‚
\end{itemize}}  

}


Attention å…¶å®å°±æ˜¯ä¸€ä¸ªåŠ æƒæ±‚å’Œçš„è¿‡ç¨‹ï¼Œå¯ä»¥çœ‹ä½œæ˜¯å¯¹è¾“å…¥å‘é‡çš„åŠ æƒå¹³å‡ã€‚å…¶æ ¸å¿ƒå…¬å¼ä¸ºï¼š
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{Q K^\mathsf{T}}{\sqrt{d_k}} \right) V
\end{equation}  
å…¶ä¸­ï¼š
\begin{itemize}
    \item $Q$ï¼ˆQueryï¼‰ï¼šè¡¨ç¤ºå½“å‰ token æƒ³è¦ä»å…¶ä»–ä½ç½®â€œæ£€ç´¢â€ä¿¡æ¯çš„éœ€æ±‚ï¼Œå†³å®šå½“å‰ token æƒ³æ‰¾ä»€ä¹ˆä¿¡æ¯ï¼›
    \item $K$ï¼ˆKeyï¼‰ï¼šè¡¨ç¤ºæ¯ä¸ª token çš„â€œç‰¹å¾æ ‡ç­¾â€ï¼Œå†³å®šæ¯ä¸ª token èƒ½è¢«åŒ¹é…æˆä»€ä¹ˆä¿¡æ¯ï¼›
    \item $V$ï¼ˆValueï¼‰ï¼šè¡¨ç¤ºå®é™…è¦è¢«ä¼ é€’æˆ–èšåˆçš„å†…å®¹ï¼Œå†³å®šæ¯ä¸ª token èƒ½æä¾›ä»€ä¹ˆä¿¡æ¯ï¼›
    \item $d_k$ï¼šç¼©æ”¾å› å­ï¼Œç”¨äºé˜²æ­¢ç‚¹ç§¯ç»“æœè¿‡å¤§å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚
\end{itemize}

\QA{å¦‚ä½•è·å¾—$Q$,$K$,$V$ä¸‰ä¸ªçŸ©é˜µ}
{
\\
\textbf{è®­ç»ƒé˜¶æ®µ}ï¼š
è®¾å½“å‰å±‚è¾“å…¥ä¸º $X \in \mathbb{R}^{n \times d_{\text{model}}}$ï¼ˆå·²åŒ…å« token embedding ä¸ä½ç½®ç¼–ç ï¼‰ï¼Œ
æ¯ä¸ªæ³¨æ„åŠ›å¤´ $i$ å…·æœ‰ä¸‰ç»„å¯è®­ç»ƒæƒé‡çŸ©é˜µï¼š
\[
W_Q^{(i)} \in \mathbb{R}^{d_{\text{model}} \times d_k},\quad
W_K^{(i)} \in \mathbb{R}^{d_{\text{model}} \times d_k},\quad
W_V^{(i)} \in \mathbb{R}^{d_{\text{model}} \times d_v}.
\]
ï¼ˆå¯é€‰åç½® $b_Q^{(h)},b_K^{(h)},b_V^{(h)}$ï¼‰ã€‚åˆ™æœ‰
é€šè¿‡çº¿æ€§æŠ•å½±è®¡ç®—ï¼š
\[
Q^{(i)} = X W_Q^{(i)} + b_Q^{(i)},\quad
K^{(i)} = X W_K^{(i)} + b_K^{(i)},\quad
V^{(i)} = X W_V^{(i)} + b_V^{(i)}.
\]
è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ\textbf{åå‘ä¼ æ’­ï¼ˆbackpropagationï¼‰}ä¼šæ›´æ–° $W_Q^{(i)},W_K^{(i)},W_V^{(i)}$ï¼Œè€Œ $Q,K,V$ ä»…æ˜¯ä¸­é—´ç»“æœï¼Œä¸ç›´æ¥è¢«å­˜å‚¨æˆ–è®­ç»ƒã€‚

\medskip
\textbf{é¢„æµ‹é˜¶æ®µ}ï¼š
ä½¿ç”¨è®­ç»ƒå¥½çš„ $W_Q^{(i)},W_K^{(i)},W_V^{(i)}$ å¯¹å½“å‰è¾“å…¥ $X$ é‡æ–°è®¡ç®— $Q,K,V$ã€‚
åœ¨è‡ªå›å½’ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä¼šç¼“å­˜å†å²æ­¥éª¤çš„ $K,V$ï¼ˆKV~cacheï¼‰ï¼Œ
æ–°ç”Ÿæˆ token æ—¶ä»…éœ€è®¡ç®—å…¶å¯¹åº”çš„ $Q$ï¼ˆåŠè‡ªèº« $K,V$ï¼‰ï¼Œå¹¶ä¸ç¼“å­˜çš„ $K,V$ è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œä»è€Œé¿å…é‡å¤è®¡ç®—ã€åŠ é€Ÿæ¨ç†ã€‚
}
æˆ‘ä»¬å°†ç”¨ä¸€ä¸ªAttention Headæ¥è¯´æ˜Attentionçš„è®¡ç®—è¿‡ç¨‹ï¼Œå› ä¸ºå…¶ä½™çš„headsé™¤äº†æŠ•å½±çŸ©é˜µï¼ˆ$W_Q^{(i)}$ï¼Œ$W_K^{(i)}$,$W_V^{(i)}$ï¼‰ä¸åŒï¼Œå…¶ä»–è®¡ç®—è¿‡ç¨‹æ˜¯ä¸€æ ·çš„ã€‚
\paragraph{Step0:è¿›è¡ŒSelf-Attentionä¹‹å‰çš„è¾“å…¥å’ŒæŠ•å½±çŸ©é˜µ}~{}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figs/lec2/lec2.12.png}
  \caption{Step0:è¿›è¡ŒSelf-Attentionä¹‹å‰çš„è¾“å…¥å’ŒæŠ•å½±çŸ©é˜µ}
  \label{fig:Step0:è¿›è¡ŒSelf-Attentionä¹‹å‰çš„è¾“å…¥å’ŒæŠ•å½±çŸ©é˜µ}
\end{figure}

\noindent\textbf{è¾“å…¥ï¼š}
\begin{itemize}
  \item {\color{dblue}Current position information}ï¼šå½“å‰ token çš„è¯å‘é‡ï¼ˆtoken embeddingï¼‰ä¸ä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰ä¹‹å’Œã€‚
  \item ä¹‹å‰æ‰€æœ‰ token çš„å‘é‡è¡¨ç¤ºï¼ˆåŒæ ·åŒ…å«è¯å‘é‡å’Œä½ç½®ç¼–ç ï¼‰ã€‚
\end{itemize}

\noindent\textbf{ä¸‰ä¸ªæŠ•å½±çŸ©é˜µï¼š}
\begin{itemize}
  \item {\color{qpurple}Query Projection} $W_Q$ï¼šå°†è¾“å…¥æ˜ å°„åˆ°æŸ¥è¯¢ç©ºé—´ï¼Œè¡¨ç¤ºå½“å‰ token æƒ³è¦ä»å…¶ä»– token è·å–çš„ä¿¡æ¯ç±»å‹ã€‚
  \item {\color{orange}Key Projection} $W_K$ï¼šå°†è¾“å…¥æ˜ å°„åˆ°é”®ç©ºé—´ï¼Œè¡¨ç¤ºæ¯ä¸ª token èƒ½è¢«åŒ¹é…æˆä»€ä¹ˆä¿¡æ¯ã€‚
  \item {\color{zblue}Value Projection} $W_V$ï¼šå°†è¾“å…¥æ˜ å°„åˆ°å€¼ç©ºé—´ï¼Œè¡¨ç¤ºæ¯ä¸ª token èƒ½æä¾›çš„å…·ä½“ä¿¡æ¯ã€‚
\end{itemize}

\noindent\textbf{è¾“å‡ºï¼š}
\begin{itemize}
  \item {\color{tred}Enriched with context information from other positions}ï¼šèåˆäº†ä¸å½“å‰ token é«˜åº¦ç›¸å…³çš„å…¶ä»– token ä¿¡æ¯åçš„æ–°è¡¨ç¤ºï¼Œæ—¢åŒ…å«è‡ªèº«ä¿¡æ¯ï¼Œä¹ŸåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
\end{itemize}

\clearpage
\paragraph{Step1:æ ¹æ®æŠ•å½±çŸ©é˜µæ›´æ–°$Q$,$K$,$V$}~{}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figs/lec2/lec2.13.png}
  \caption{Step1:æ ¹æ®æŠ•å½±çŸ©é˜µæ›´æ–°$Q$,$K$,$V$}
  \label{fig:Step1:æ ¹æ®æŠ•å½±çŸ©é˜µæ›´æ–°$Q$,$K$,$V$}
\end{figure}

åœ¨è¯¥æ­¥éª¤ä¸­ï¼Œè¾“å…¥çŸ©é˜µ $X \in \mathbb{R}^{n \times d_{\text{model}}}$ï¼ˆç”±{\color{dblue}current position informationå’Œother positions in the sequence}ç»„æˆï¼‰ ä¼šåˆ†åˆ«ä¸ä¸‰ä¸ªå¯è®­ç»ƒçš„æŠ•å½±çŸ©é˜µç›¸ä¹˜ï¼š
\[
Q = X W_Q,\quad K = X W_K,\quad V = X W_V
\]
å…¶ä¸­ $W_Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$ã€$W_K \in \mathbb{R}^{d_{\text{model}} \times d_k}$ã€$W_V \in \mathbb{R}^{d_{\text{model}} \times d_v}$æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾—åˆ°çš„æŠ•å½±çŸ©é˜µã€‚

åœ¨ $Q, K, V$ çŸ©é˜µä¸­ï¼Œæ¯ä¸€è¡Œå¯¹åº”åºåˆ—ä¸­ä¸€ä¸ª token çš„è¡¨ç¤ºï¼Œå…¶ä¸­æœ€åä¸€è¡Œæ˜¯å½“å‰ä½ç½® tokenï¼ˆå³{\color{dblue}current position information}ï¼‰çš„ $Q$ã€$K$ã€$V$ å‘é‡ã€‚

è¿™ä¸‰ä¸ªçŸ©é˜µå°†åœ¨åç»­æ­¥éª¤ä¸­å®Œæˆ Attention çš„ä¸¤ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼š
\begin{itemize}
  \item è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼ˆRelevance scoringï¼‰
  \item ç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆCombining informationï¼‰
\end{itemize}

\clearpage
\paragraph{Step2:è®¡ç®—ç›¸å…³æ€§åˆ†æ•°å¹¶è¿›è¡ŒSoftmaxå½’ä¸€åŒ–}~{}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figs/lec2/lec2.14.png}
  \caption{Step2:è®¡ç®—ç›¸å…³æ€§åˆ†æ•°å¹¶è¿›è¡ŒSoftmaxå½’ä¸€åŒ–}
  \label{fig:Step2:è®¡ç®—ç›¸å…³æ€§åˆ†æ•°å¹¶è¿›è¡ŒSoftmaxå½’ä¸€åŒ–}
\end{figure}

åœ¨ {\color{qpurple}ç›¸å…³æ€§è®¡ç®—ï¼ˆRelevance scoringï¼‰} æ­¥éª¤ä¸­ï¼Œé¦–å…ˆå°†å½“å‰ä½ç½®çš„æŸ¥è¯¢å‘é‡ä¸æ•´ä¸ªé”®çŸ©é˜µç›¸ä¹˜ï¼š
\[
\text{scores} = Q_{\text{current}} K^\mathsf{T}
\]
å¾—åˆ°çš„åˆ†æ•°è¡¨ç¤ºå½“å‰ä½ç½®ä¸æ¯ä¸ªä¸Šä¸‹æ–‡ token ä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ã€‚ä¸ºäº†é¿å…åˆ†æ•°éšç»´åº¦ $d_k$ å¢å¤§è€Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œè¿™äº›åˆ†æ•°ä¼šé™¤ä»¥ç¼©æ”¾å› å­ $\sqrt{d_k}$ è¿›è¡Œå½’ä¸€åŒ–ã€‚  

éšåï¼Œå°†ç¼©æ”¾åçš„åˆ†æ•°è¾“å…¥ Softmax å‡½æ•°ï¼Œå¾—åˆ°ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼ˆæ‰€æœ‰æƒé‡ä¹‹å’Œä¸º 1ï¼‰ï¼Œè¯¥åˆ†å¸ƒåæ˜ äº†æ¨¡å‹åœ¨å½“å‰ä½ç½®åº”å¦‚ä½•åˆ†é…å¯¹å„ä¸ªä¸Šä¸‹æ–‡ token çš„æ³¨æ„åŠ›ã€‚  

å› æ­¤ï¼Œè¿™ä¸€æ­¥å¯¹åº”å…¬å¼ï¼ˆ2.1ï¼‰ä¸­çš„ï¼š
\[
\text{softmax}\left(\frac{QK^\mathsf{T}}{\sqrt{d_k}}\right)
\]

\MarginImageWithNote
{figs/lec2/lec2.16.png}
{\captionof{figure}{Teacher Forcing ç¤ºæ„å›¾}\label{fig:TeacherForcing}}
{
\Definition{Teacher Forcing}{
ä¸€ç§åºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼ˆseq2seqï¼‰å¸¸ç”¨çš„{\color{tred}è®­ç»ƒç­–ç•¥}ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œå°†çœŸå®çš„ç›®æ ‡åºåˆ—ï¼ˆground truthï¼‰æŒ‰æ—¶é—´æ­¥ä½œä¸ºè§£ç å™¨çš„è¾“å…¥ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹ä¸Šä¸€æ­¥çš„é¢„æµ‹ç»“æœã€‚è¿™æ ·å¯ä»¥ä¸€æ¬¡æ€§å¹¶è¡Œè®¡ç®—æ•´ä¸ªåºåˆ—çš„è¾“å‡ºï¼Œæå‡è®­ç»ƒé€Ÿåº¦å¹¶ç¨³å®šæ¢¯åº¦ã€‚å³$Q$ä¸ºå®Œæ•´çŸ©é˜µã€‚
}\label{def:teacher-forcing}

\Definition{Auto-regressive Model}{
ä¸€ç§{\color{tred}åºåˆ—ç”Ÿæˆ}æ–¹å¼ï¼Œæ¨¡å‹æŒ‰æ—¶é—´é¡ºåºé€ä¸ªç”Ÿæˆ tokenã€‚æ¯ä¸€æ­¥çš„è¾“å‡ºä»…ä¾èµ–äºå…ˆå‰ç”Ÿæˆçš„ tokenï¼ˆæ»¡è¶³å› æœæ©ç çº¦æŸï¼Œä¸èƒ½çœ‹åˆ°æœªæ¥ tokenï¼‰ã€‚åœ¨æ¨ç†æ—¶éœ€è¦é¡ºåºè®¡ç®—ï¼Œä½†å¯ä»¥åˆ©ç”¨ç¼“å­˜æœºåˆ¶ï¼ˆKV cacheï¼‰åŠ é€Ÿã€‚å³Qä¸ºä»…åŒ…å«å½“å‰ä½ç½®çš„å‘é‡ $Q_{\text{current}}$ã€‚
}\label{def:auto-regressive}

}

\Insight{è®­ç»ƒä¸æ¨ç†ä¸­çš„è®¡ç®—æ–¹å¼çš„ä¸åŒ}{
\begin{itemize}
\item  \textbf{è®­ç»ƒé˜¶æ®µï¼šTeacher Forcing}  \\
$Q$ã€$K$ã€$V$ éƒ½æ˜¯å®Œæ•´çš„çŸ©é˜µï¼Œæ¨¡å‹å¯ä»¥åŒæ—¶è®¡ç®—åºåˆ—ä¸­æ‰€æœ‰ä½ç½®çš„ç›¸å…³æ€§åˆ†æ•°ã€‚

\item \textbf{æ¨ç†é˜¶æ®µï¼šAuto-regressive}\\
è®¡ç®—æ˜¯é€ä½ç½®è¿›è¡Œçš„ï¼š$Q$ é€€åŒ–ä¸ºä»…åŒ…å«å½“å‰ä½ç½®çš„ä¸€ä¸ªå‘é‡ $Q_{\text{current}}$ï¼Œå®ƒä¼šä¸ç¼“å­˜çš„ $K$ã€$V$ï¼ˆKV Cacheï¼‰äº¤äº’è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œé€æ­¥ç”Ÿæˆåºåˆ—ã€‚æ‰¹é‡æ¨ç†å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªåºåˆ—ï¼Œä½†åŒä¸€åºåˆ—å†…éƒ¨çš„ token å¿…é¡»æŒ‰é¡ºåºç”Ÿæˆã€‚
\end{itemize}
}



\clearpage

\paragraph{Step3:ç”¨æ³¨æ„åŠ›æƒé‡å¯¹$ğ‘‰$æƒæ±‚å’Œï¼Œå¾—åˆ°è¾“å‡º}~{}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figs/lec2/lec2.15.png}
  \caption{Step3:ç”¨æ³¨æ„åŠ›æƒé‡å¯¹$ğ‘‰$æƒæ±‚å’Œï¼Œå¾—åˆ°è¾“å‡º}
\label{fig:Step3:ç”¨æ³¨æ„åŠ›æƒé‡å¯¹$ğ‘‰$æƒæ±‚å’Œï¼Œå¾—åˆ°è¾“å‡º}
\end{figure}

åœ¨è·å¾—æ³¨æ„åŠ›æƒé‡  {\color{qpurple}relevance scores} ä¹‹åï¼Œå½“å‰ä½ç½®ä¼šä½¿ç”¨è¿™äº›æƒé‡å¯¹å€¼çŸ©é˜µ $V$ è¿›è¡ŒåŠ æƒæ±‚å’Œï¼š  
\[
\text{Output}_{\text{current}} = \sum_{i=1}^{n} \alpha_i V_i
\]
å…¶ä¸­ $\alpha_i$ æ˜¯å½“å‰ä½ç½®å¯¹ç¬¬ $i$ ä¸ª token çš„æ³¨æ„åŠ›æƒé‡ï¼ˆSoftmax è¾“å‡ºçš„æ¦‚ç‡ï¼‰ï¼Œ$V_i$ æ˜¯è¯¥ token åœ¨å€¼çŸ©é˜µä¸­çš„å‘é‡è¡¨ç¤ºã€‚  

è¿™ä¸€åŠ æƒæ±‚å’Œæ“ä½œï¼Œä½¿å¾—å½“å‰ä½ç½®çš„è¾“å‡ºå‘é‡ä¸ä»…ä¿ç•™äº†è‡ªèº«çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè¿˜èåˆäº†æ¥è‡ªå…¶ä»–ä½ç½®çš„é‡è¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆ{\color{tred}Enriched with context information from other positions}ï¼‰ã€‚  

å› æ­¤ï¼Œè¿™ä¸€æ­¥å¯¹åº”å…¬å¼ï¼ˆ2.1ï¼‰ä¸­çš„ï¼š
\[
\text{Output} = \text{softmax}\left(\frac{QK^\mathsf{T}}{\sqrt{d_k}}\right) V
\]
å…¶ä¸­ï¼Œ\(\text{softmax}\left(\frac{QK^\mathsf{T}}{\sqrt{d_k}}\right)\) å¯è®°ä½œæ³¨æ„åŠ›æƒé‡çŸ©é˜µ \(\alpha\)ï¼Œè¡¨ç¤ºå½“å‰ä½ç½®å¯¹å„ä¸ª token çš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚


\clearpage
\paragraph{Step4: å¤šå¤´æ³¨æ„åŠ›çš„è¾“å‡ºåˆå¹¶}~{}

åœ¨å‰é¢çš„æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†è§£é‡Šäº†ä¸€ä¸ª \emph{Attention Head} çš„è®¡ç®—è¿‡ç¨‹ï¼šé€šè¿‡å°†è¾“å…¥æŠ•å½±ä¸º $Q$ã€$K$ã€$V$ï¼Œè®¡ç®—ç›¸å…³æ€§åˆ†æ•°å¹¶è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œä»è€Œç”Ÿæˆèåˆä¸Šä¸‹æ–‡ä¿¡æ¯çš„è¾“å‡ºè¡¨ç¤ºã€‚  

ç„¶è€Œï¼Œä¸€ä¸ª Attention Head åœ¨ä¸€æ¬¡è®¡ç®—ä¸­åªèƒ½åœ¨\textbf{åŒä¸€ä¸ªè¡¨ç¤ºå­ç©ºé—´}ä¸­å­¦ä¹ ä½ç½®é—´çš„ä¾èµ–å…³ç³»ã€‚ä¸ºäº†è®©æ¨¡å‹åœ¨ä¸åŒçš„ç‰¹å¾å­ç©ºé—´ä¸­\emph{åŒæ—¶}æ•æ‰å¤šç§æ¨¡å¼ï¼ˆä¾‹å¦‚è¯­ä¹‰å…³ç³»ã€å¥æ³•ç»“æ„ã€é•¿ç¨‹ä¾èµ–ç­‰ï¼‰ï¼ŒTransformer ä¼šå°†æ³¨æ„åŠ›æœºåˆ¶\textbf{å¤åˆ¶å¤šä»½å¹¶è¡Œæ‰§è¡Œ}ï¼Œå½¢æˆå¤šä¸ªç‹¬ç«‹çš„ Attention Headã€‚

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figs/lec2/lec2.17.png}
  \caption{Multi-Head Attentionä¸­çš„$Q$,$K$,$V$åœ¨ä¸åŒHeadä¸­æ˜¯ç‹¬ç«‹çš„}
  \label{fig:Multi-Head Attentionè®¡ç®—å›¾}
\end{figure}

\MarginImageWithNote
  {figs/lec2/lec2.18.png}
  {\captionof{figure}{Multi-Head Attentionçš„è¾“å‡ºæ‹¼æ¥ç¤ºæ„å›¾}}


\MarginImageWithNote
  {figs/lec2/lec2.19.png}
  {\captionof{figure}{å¤šå¤´æ³¨æ„åŠ›è¾“å‡ºæ•´åˆå¹¶é€å…¥FFN}}
  {
\begin{enumerate}
  \item \textbf{æ‹¼æ¥å„ä¸ªæ³¨æ„åŠ›å¤´çš„è¾“å‡º}  
  å°† $Z_0, Z_1, \dots, Z_{h-1}$ åœ¨ç‰¹å¾ç»´åº¦ä¸Šè¿›è¡Œæ‹¼æ¥ï¼Œå¾—åˆ°æ‹¼æ¥åçš„çŸ©é˜µï¼š
  \[
  Z_{\text{concat}} = \text{Concat}(Z_0, Z_1, \dots, Z_{h-1})
  \]
  
  \item \textbf{çº¿æ€§å˜æ¢æ•´åˆä¿¡æ¯}  
  å°† $Z_{\text{concat}}$ ä¹˜ä»¥ä¸€ä¸ªå¯è®­ç»ƒçš„æŠ•å½±çŸ©é˜µ $W^O$ï¼Œå¾—åˆ°æœ€ç»ˆçš„å¤šå¤´æ³¨æ„åŠ›è¾“å‡ºï¼š
  \[
  Z = Z_{\text{concat}} W^O
  \]
  è¿™ä¸ªè¿‡ç¨‹çš„ä½œç”¨æ˜¯å°†å¤šä¸ªå¤´çš„ç‰¹å¾é‡æ–°æ•´åˆåˆ°æ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ï¼Œä½¿ä¸åŒå­ç©ºé—´çš„ä¿¡æ¯èƒ½å¤Ÿäº¤äº’èåˆã€‚

  \item \textbf{é€å…¥åç»­çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFFNï¼‰}  
  æœ€ç»ˆçš„ $Z$ ä¼šä½œä¸ºåç»­ Point-Wise FFN çš„è¾“å…¥ï¼Œè¿›ä¸€æ­¥è¿›è¡Œéçº¿æ€§å˜æ¢å’Œç‰¹å¾æå–ã€‚
\end{enumerate}
}

åœ¨åŸå§‹ Transformer è®ºæ–‡ä¸­ï¼ŒMulti-Head Self-Attention çš„å®šä¹‰ä¸ºï¼š
\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
\]
å…¶ä¸­ï¼š
\[
\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)
\]
\begin{itemize}
  \item $h$ æ˜¯å¤´çš„æ•°é‡;
  \item $W_i^Q, W_i^K, W_i^V$ï¼šç¬¬ $i$ ä¸ª Head çš„å¯è®­ç»ƒæŠ•å½±çŸ©é˜µ; 
  \item $W^O$ æ˜¯æœ€ç»ˆçš„çº¿æ€§å˜æ¢çŸ©é˜µï¼Œå°†æ‹¼æ¥åçš„å‘é‡æ˜ å°„å›æ¨¡å‹ç»´åº¦ $d_{\text{model}}$ã€‚
\end{itemize} 
 

åœ¨å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰ä¸­ï¼Œä¸åŒçš„æ³¨æ„åŠ›å¤´ï¼ˆAttention Headï¼‰å¯ä»¥ä»ä¸åŒçš„å­ç©ºé—´ä¸­æ•æ‰åºåˆ—çš„ç‰¹å¾ã€‚  
å®Œæˆæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„è®¡ç®—åï¼Œå¾—åˆ°å¤šä¸ªè¾“å‡ºçŸ©é˜µï¼š
\[
Z_0, Z_1, \dots, Z_{h-1}
\]
å…¶ä¸­ $h$ ä¸ºæ³¨æ„åŠ›å¤´çš„æ•°é‡ã€‚




\clearpage
\paragraph{Step5: Add \& Norm}~{}

åœ¨ Transformer ä¸­ï¼Œæ¯ä¸ªå­å±‚ï¼ˆä¾‹å¦‚å¤šå¤´æ³¨æ„åŠ›å±‚ã€å‰é¦ˆç¥ç»ç½‘ç»œå±‚ï¼‰çš„è¾“å‡ºéƒ½ä¼šç»è¿‡ \textbf{æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰} ä¸ \textbf{å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰}ï¼Œè¿™ä¸€ç»„åˆè¢«ç§°ä¸º \textbf{Add \& Norm} æ­¥éª¤ã€‚

\subparagraph{Layer Normalization}  
Layer Normalizationï¼ˆå±‚å½’ä¸€åŒ–ï¼‰å¯¹æ¯ä¸ªä½ç½®ï¼ˆtokenï¼‰çš„è¡¨ç¤ºå‘é‡å•ç‹¬è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»è€Œä½¿æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ›´åŠ ç¨³å®šå¹¶åŠ å¿«æ”¶æ•›ã€‚  
å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå¯¹æ¯ä¸ªä½ç½®çš„å‘é‡ $\mathbf{x}$ è¿›è¡Œå‡å€¼ä¸æ–¹å·®çš„å½’ä¸€åŒ–å¤„ç†ï¼š
\[
\text{LayerNorm}(\mathbf{x}) = \frac{\mathbf{x} - \mu}{\sigma + \epsilon} \cdot \gamma + \beta
\]
å…¶ä¸­ï¼š
\[
\mu = \frac{1}{d} \sum_{i=1}^{d} x_i, \quad
\sigma = \sqrt{\frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2}
\]
\begin{itemize}
  \item $\mathbf{x} \in \mathbb{R}^d$ï¼šè¡¨ç¤ºä¸€ä¸ª token çš„å‘é‡è¡¨ç¤ºï¼ˆembeddingï¼‰ã€‚
  \item $\mu$ã€$\sigma$ åˆ†åˆ«ä¸ºè¯¥å‘é‡çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚
  \item $d$ ä¸ºå‘é‡çš„ç»´åº¦ï¼ˆå³ embedding sizeï¼‰ã€‚
  \item $\gamma, \beta$ ä¸ºå¯å­¦ä¹ çš„ç¼©æ”¾ä¸å¹³ç§»å‚æ•°ã€‚
  \item $\epsilon$ æ˜¯é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°ï¼Œé€šå¸¸å– $10^{-6}$ã€‚
\end{itemize}

ä¸ Batch Normalization ä¸åŒï¼ŒLayer Normalization æ˜¯\textbf{åœ¨ç‰¹å¾ç»´åº¦ä¸Š}è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸æ˜¯åœ¨ batch ç»´åº¦ä¸Šï¼Œå› æ­¤æ›´é€‚åˆå¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—ã€‚LN å¯¹æ¯ä¸ªä½ç½®ï¼ˆtokenï¼‰å•ç‹¬å½’ä¸€åŒ–ï¼Œé¿å…äº† BN åœ¨å¯å˜åºåˆ—ã€å° batch ä»¥åŠè‡ªå›å½’ç”Ÿæˆä¸­å¯èƒ½å‡ºç°çš„ä¿¡æ¯æ³„æ¼å’Œç»Ÿè®¡ä¸ç¨³å®šé—®é¢˜ã€‚

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{figs/lec2/lec2.20.png}
  \caption{\href{http://proceedings.mlr.press/v119/shen20e/shen20e.pdf}{Layer Normalization vs. Batch Normalization} ç¤ºæ„å›¾}
  \label{fig:layernorm}
\end{figure}

\clearpage
\subparagraph{Residual Connection}  
Residual Connection é€šè¿‡å°†å­å±‚çš„è¾“å‡ºä¸å…¶è¾“å…¥ç›¸åŠ ï¼š
\[
\mathbf{y} = \mathrm{Sublayer}(\mathbf{x}) + \mathbf{x}
\]

\MarginImageWithNote
  {figs/lec2/lec2.21.png}
  {\captionof{figure}{Residual Connection ç¤ºæ„å›¾}}

ä¸ºæ¨¡å‹æä¾›äº†ä¸€æ¡ç›´æ¥çš„â€œæ’ç­‰æ˜ å°„â€è·¯å¾„ï¼ˆIdentity Mappingï¼‰ï¼Œä½¿å¾—ç½‘ç»œæ›´å®¹æ˜“å­¦ä¹ åˆ°æ¥è¿‘æ’ç­‰å˜æ¢çš„åŠŸèƒ½ï¼Œä»è€Œç¼“è§£æ·±å±‚ç½‘ç»œçš„é€€åŒ–é—®é¢˜ã€‚

åœ¨åå‘ä¼ æ’­ä¸­ï¼Œæ®‹å·®è¿æ¥ä¸ºæ¢¯åº¦æä¾›äº†ä¸¤æ¡è·¯å¾„ï¼š  
\begin{align}
\frac{\partial \mathcal{L}}{\partial \mathbf{x}}
&= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \notag\\
&= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \left( \mathbf{I} + \frac{\partial \mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}} \right) \notag\\
&= \overset{\text{straight path}}{\frac{\partial \mathcal{L}}{\partial \mathbf{y}}}
\;+\;
\overset{\text{from output}}{\frac{\partial \mathcal{L}}{\partial \mathbf{y}}}
\,\cdot\,
\overset{\text{through the sub-layer}}{\frac{\partial\,\mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}}}
\tag{2.2}
\end{align}

ç¬¬ä¸€é¡¹æ˜¯ç›´æ¥ä¼ é€’çš„æ¢¯åº¦ï¼Œä¸ä¾èµ–å­å±‚çš„è®¡ç®—ç»“æœï¼Œå³ä½¿å­å±‚æ¢¯åº¦è¶‹è¿‘äº 0ï¼Œä¿¡æ¯ä¹Ÿä¸ä¼šå®Œå…¨ä¸¢å¤±ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

\MarginNote{
\Definition{Constant Error Carousel (CEC)}{
CEC æ˜¯ LSTMï¼ˆLong Short-Term Memoryï¼‰ç»“æ„ä¸­æœ€æ ¸å¿ƒçš„æœºåˆ¶ä¹‹ä¸€ï¼Œç”¨äºåœ¨æ—¶é—´æ­¥ä¹‹é—´â€œæ— è¡°å‡â€åœ°ä¼ é€’æ¢¯åº¦ï¼Œä»è€Œç¼“è§£ RNN åœ¨é•¿åºåˆ—å­¦ä¹ ä¸­å®¹æ˜“å‡ºç°çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

åœ¨ LSTM ä¸­ï¼ŒCEC ç”±å•å…ƒçŠ¶æ€ï¼ˆcell stateï¼‰$c_t$ åŠå…¶æ’ç­‰ä¼ é€’è·¯å¾„æ„æˆã€‚å½“é—å¿˜é—¨ $f_t=1$ ä¸”è¾“å…¥é—¨ $i_t=0$ æ—¶ï¼Œ$c_t$ å¯ä»¥åœ¨å¤šä¸ªæ—¶é—´æ­¥ä¸­ä¿æŒä¸å˜ï¼Œæ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶ä¹Ÿèƒ½æ²¿è¿™æ¡è·¯å¾„ç¨³å®šä¼ é€’ï¼ˆå³æ¢¯åº¦è¿‘ä¼¼ä¸ºå¸¸æ•°ï¼‰ï¼Œå› æ­¤ç§°ä¸º \textit{Constant Error Carousel}ã€‚

è¿™ä¸€è®¾è®¡ä½¿å¾— LSTM èƒ½å¤Ÿé•¿æœŸè®°å¿†å…³é”®ä¿¡æ¯ï¼Œå¹¶åœ¨éœ€è¦æ—¶é€šè¿‡é—¨æ§æœºåˆ¶æœ‰é€‰æ‹©åœ°æ›´æ–°æˆ–è¾“å‡ºï¼Œä»è€Œæœ‰æ•ˆå»ºæ¨¡é•¿æœŸä¾èµ–ã€‚
}
}
\Insight{Residual Connection ä¸ Constant Error Carouselï¼ˆCECï¼‰çš„è”ç³»}{
Residual Connectionï¼ˆæ®‹å·®è¿æ¥ï¼‰ä¸ LSTM ä¸­çš„ Constant Error Carouselï¼ˆCECï¼‰åœ¨æ¢¯åº¦ä¼ æ’­æœºåˆ¶ä¸Šæœ‰ç›¸ä¼¼ä¹‹å¤„ï¼š

\begin{itemize}
  \item \textbf{å…±åŒç‚¹}ï¼š
    \begin{itemize}
      \item éƒ½æ—¨åœ¨ç¼“è§£æ·±å±‚æˆ–é•¿åºåˆ—è®­ç»ƒä¸­çš„\textbf{æ¢¯åº¦æ¶ˆå¤±}é—®é¢˜ã€‚
      \item éƒ½ä¸ºæ¢¯åº¦æä¾›äº†\textbf{ç›´é€šè·¯å¾„}ï¼Œä½¿å¾—åå‘ä¼ æ’­æ—¶æ¢¯åº¦å¯ä»¥ä¸ç»è¿‡å¤æ‚çš„éçº¿æ€§å˜æ¢ç›´æ¥ä¼ é€’ã€‚
      \item éƒ½èƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¿æŒä¿¡æ¯åœ¨ç½‘ç»œä¸­çš„é•¿æœŸæµåŠ¨ã€‚
    \end{itemize}
  \item \textbf{åŒºåˆ«}ï¼š
    \begin{itemize}
      \item Residual Connection æ²¡æœ‰é—¨æ§ï¼Œè¾“å…¥ä¸å­å±‚è¾“å‡º\textbf{æ’ç­‰ç›¸åŠ }ï¼Œç»“æ„æ›´ç®€å•ã€‚
      \item CEC åœ¨ LSTM ä¸­é…åˆé—¨æ§æœºåˆ¶ï¼ˆé—å¿˜é—¨ $f_t$ã€è¾“å…¥é—¨ $i_t$ï¼‰åŠ¨æ€è°ƒèŠ‚ä¿¡æ¯ä¿ç•™ä¸æ›´æ–°çš„æ¯”ä¾‹ã€‚
      \item Residual Connection å¤šç”¨äºæ·±å±‚å‰é¦ˆç½‘ç»œï¼ˆå¦‚ Transformerï¼‰ï¼Œè€Œ CEC ä¸“æ³¨äºå¾ªç¯ç»“æ„ä¸­çš„é•¿æœŸä¾èµ–ã€‚
    \end{itemize}
\end{itemize}

å¯ä»¥å°† Residual Connection çœ‹ä½œæ˜¯ CEC çš„\textbf{æ— é—¨æ§æ’ç­‰æ˜ å°„ç‰ˆæœ¬}ï¼Œè™½ç¼ºä¹åŠ¨æ€è°ƒèŠ‚èƒ½åŠ›ï¼Œä½†åœ¨æ·±å±‚ç½‘ç»œä¸­å·²è¶³å¤Ÿç¼“è§£æ¢¯åº¦è¡°å‡ã€‚}



\clearpage
\subsubsection{Point-wise Feed-Forward Network}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{figs/lec2/lec2.22.png}
  \caption{Point-wise Feed-Forward Network}
  \label{fig:Point-wise Feed-Forward Network}
\end{figure}

\textbf{è¾“å…¥ï¼š} æ¥è‡ª Multi-Head Self-Attention çš„è¾“å‡ºçŸ©é˜µ
\[
Z \in \mathbb{R}^{n \times d_{\text{model}}}
\]
å…¶ä¸­ $n$ ä¸ºåºåˆ—é•¿åº¦ï¼Œ$d_{\text{model}}$ ä¸ºæ¯ä¸ª token çš„å‘é‡ç»´åº¦ã€‚
çŸ©é˜µçš„æ¯ä¸€è¡Œ $z_i$ è¡¨ç¤ºä½ç½® $i$ ä¸Šçš„ token è¡¨ç¤ºï¼Œå·²ç»èåˆäº†æ¥è‡ªå…¶ä»–ä½ç½®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

\textbf{æ“ä½œï¼š} å¯¹æ¯ä¸€ä¸ªä½ç½®çš„å‘é‡ $z_i$ ç‹¬ç«‹åœ°åº”ç”¨ç›¸åŒçš„ä¸¤å±‚å‰é¦ˆç½‘ç»œå’Œéçº¿æ€§æ¿€æ´»ï¼š
\[
\text{FFN}(z_i) = \text{ReLU}(z_i W_1 + b_1) W_2 + b_2
\]
å…¶ä¸­ $W_1, b_1, W_2, b_2$ æ˜¯åœ¨æ‰€æœ‰ä½ç½®é—´å…±äº«çš„å¯å­¦ä¹ å‚æ•°ã€‚

\textbf{è¾“å‡ºï¼š} 
\[
Z' \in \mathbb{R}^{n \times d_{\text{model}}}
\]
çŸ©é˜µçš„æ¯ä¸€è¡Œ $z'_i = \text{FFN}(z_i)$ï¼Œè¡¨ç¤ºç»è¿‡éçº¿æ€§å˜æ¢åçš„ token è¡¨ç¤ºã€‚

\textbf{ç›®çš„ï¼š}
\begin{itemize}
    \item åœ¨ä¸æ”¹å˜ä½ç½®é—´ä¾èµ–å…³ç³»çš„æƒ…å†µä¸‹ï¼Œå¯¹æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºè¿›è¡Œéçº¿æ€§ç‰¹å¾å˜æ¢ã€‚
    \item ä¸ Self-Attention æ¨¡å—é…åˆï¼šSelf-Attention è´Ÿè´£ä½ç½®é—´çš„ä¿¡æ¯äº¤äº’ï¼ŒFFN è´Ÿè´£ä½ç½®å†…éƒ¨çš„ç‰¹å¾æç‚¼ã€‚
    \item Point-Wiseï¼ˆé€ä½ç½®ï¼‰è®¾è®¡ä¾¿äºå¹¶è¡Œè®¡ç®—ï¼Œä¸”æ‰€æœ‰ä½ç½®å…±äº«åŒä¸€ç»„å‚æ•°ï¼Œä¸éšåºåˆ—é•¿åº¦å¢åŠ è€Œè†¨èƒ€ã€‚
\end{itemize}


\clearpage
\subsection{Decoder ç»“æ„}

Decoder ç›¸è¾ƒäºEncoderæœ‰ä¸‰ä¸ªä¸åŒç‚¹ï¼š
\begin{itemize}
  \item \textbf{Masked Multi-Head Self-Attention}ï¼šåœ¨ç›®æ ‡åºåˆ—çš„è‡ªæ³¨æ„åŠ›ä¸­åŠ å…¥\textbf{å› æœ Maskï¼ˆCausal Maskï¼‰}ï¼Œç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å½“å‰ä½ç½®åªèƒ½å…³æ³¨åˆ°è‡ªå·±å’Œä¹‹å‰çš„ tokenï¼Œé¿å…åœ¨è®­ç»ƒæ—¶â€œå·çœ‹â€æœªæ¥ä¿¡æ¯ã€‚
  \item \textbf{Encoder-Decoder Attentionï¼ˆCross-Attentionï¼‰}ï¼šåˆ©ç”¨ Encoder çš„è¾“å‡ºä½œä¸º Key/Valueï¼Œå½“å‰ Decoder çš„éšè—çŠ¶æ€ä½œä¸º Queryï¼Œå®ç°å¯¹æºåºåˆ—ä¿¡æ¯çš„é€‰æ‹©æ€§å…³æ³¨ã€‚
  \item \textbf{Position-wise Feed Forward Network \& Add \& Norm}ï¼šä¸ Encoder ç›¸åŒçš„å‰é¦ˆç½‘ç»œå’Œæ®‹å·®å½’ä¸€åŒ–ç»“æ„ã€‚
\end{itemize}


\subsubsection{Masked Multi-Head Self-Attention}


\MarginImageWithNote
  {figs/lec2/lec2.24.jpeg}
  {\captionof{figure}{Mask Matrix $M$ç¤ºæ„å›¾}\label{fig:Masked Multi-Head Self-Attention}}
  {
  å…¶ä¸­ï¼š$M_{\text{causal}} \in \mathbb{R}^{n \times n}$ æ˜¯å› æœæ©ç çŸ©é˜µï¼Œå®šä¹‰ä¸ºï¼š
  \[
  (M_{\text{causal}})_{ij} =
  \begin{cases}
  0 & j \le i \\
  -\infty & j > i
  \end{cases}
  \]
  å½“ $j > i$ï¼ˆæœªæ¥ä½ç½®ï¼‰æ—¶ï¼Œæ©ç å€¼ä¸º $-\infty$ï¼ŒSoftmax åå¯¹åº”æƒé‡ä¸º 0ã€‚
  }

Transformer çš„ Decoder é‡‡ç”¨ Masked Multi-Head Self-Attention çš„åŸå› æ˜¯ï¼Œåœ¨è®­ç»ƒé˜¶æ®µæˆ‘ä»¬é€šå¸¸ä½¿ç”¨ {\color{tred}\hyperref[def:teacher-forcing]{Teacher Forcing}}ã€‚  
è¿™ç§æ–¹å¼è™½ç„¶é«˜æ•ˆï¼Œä½†ä¼šå¯¼è‡´ Decoder åœ¨è®¡ç®—è‡ªæ³¨æ„åŠ›æ—¶å¯ä»¥â€œçœ‹åˆ°â€å½“å‰ä½ç½®ä¹‹åçš„ tokenï¼Œå³æœªæ¥ä¿¡æ¯ï¼Œä»è€Œé€ æˆæ•°æ®æ³„éœ²ã€‚  
ä¸ºäº†é¿å…è¿™ç§â€œå·çœ‹â€æœªæ¥ token çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦åœ¨ Self-Attention çš„ç›¸å…³æ€§è®¡ç®—ä¸­å¼•å…¥æ©ç ï¼ˆmaskï¼‰çŸ©é˜µï¼Œå°†å½“å‰ä½ç½®ä¹‹åçš„æ³¨æ„åŠ›æƒé‡å±è”½ä¸º 0ã€‚  
åœ¨æ¨ç†é˜¶æ®µ{\color{dblue}\hyperref[def:auto-regressive]{Auto-regressive}}ä¸­ï¼Œè¿™ç§æ©ç åŒæ ·é€‚ç”¨ï¼Œå› ä¸ºç”Ÿæˆæ˜¯é€æ­¥è¿›è¡Œçš„ã€‚

å…·ä½“æ¥è¯´ï¼Œå› æœæ³¨æ„åŠ›ï¼ˆCausal Attentionï¼‰çš„è®¡ç®—å…¬å¼ä¸ºï¼š
\begin{equation}
\text{CausalAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^\mathsf{T}}{\sqrt{d_k}} + M_{\text{causal}}\right)V
\tag{2.3}
\end{equation}


è¿™ç§æ©ç æœºåˆ¶ä¿è¯äº†æ— è®ºæ˜¯åœ¨ Teacher Forcing çš„è®­ç»ƒé˜¶æ®µï¼Œè¿˜æ˜¯åœ¨ Auto-regressive æ¨ç†é˜¶æ®µï¼Œå½“å‰ä½ç½®çš„è¾“å‡ºéƒ½åªèƒ½ä¾èµ–äºå½“å‰ä½ç½®åŠå…¶ä¹‹å‰çš„ tokenï¼Œä»è€Œä¸¥æ ¼éµå®ˆå› æœæ€§çº¦æŸã€‚


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/lec2/lec2.23.png}
  \caption{Self-Attention vs. Masked Self-Attention}
  \label{fig:Self-Attention vs. Masked Self-Attention}
\end{figure}


\clearpage

\subsubsection{Encoder-Decoder Attention(cross-attention)}


\subsubsection{Encoder-Decoder Attention (Cross-Attention)}

åœ¨ Cross-Attention ä¸­ï¼š

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{figs/lec2/lec2.25.jpeg}
  \caption{Cross-Attention}
  \label{fig:Cross-Attention}
\end{figure}

\begin{itemize}
  \item {\color{qpurple}Query ($Q$)} æ¥è‡ª Decoder ä¸Šä¸€å­å±‚çš„è¾“å‡ºï¼ˆç›®æ ‡ä¾§éšè—çŠ¶æ€ï¼‰ï¼›
  \item {\color{orange}Key ($K$)} ä¸ {\color{zblue}Value ($V$)} æ¥è‡ª Encoder çš„æœ€ç»ˆè¾“å‡ºï¼ˆæºä¾§éšè—çŠ¶æ€ï¼‰ã€‚
\end{itemize}
è¿™æ ·ï¼ŒDecoder åœ¨ç”Ÿæˆç›®æ ‡ token æ—¶ï¼Œå°±èƒ½æ ¹æ®è‡ªèº«çš„è§£ç çŠ¶æ€ï¼ˆQueryï¼‰å»â€œæ£€ç´¢â€å¹¶é€‰æ‹©æ€§åœ°åˆ©ç”¨ Encoder ç¼–ç çš„æºåºåˆ—ä¿¡æ¯ï¼ˆKey/Valueï¼‰ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œä»è€Œå®ç°æº-ç›®æ ‡ä¹‹é—´çš„ä¿¡æ¯äº¤äº’ã€‚

Cross-Attention çš„è®¡ç®—å…¬å¼ä¸ºï¼š
\[
\text{CrossAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^\mathsf{T}}{\sqrt{d_k}}\right)V
\]
è¿™é‡Œçš„ $Q \in \mathbb{R}^{n_{\text{tgt}} \times d_k}$ æ¥è‡ª Decoderï¼Œ$K, V \in \mathbb{R}^{n_{\text{src}} \times d_k}$ æ¥è‡ª Encoderã€‚

\Remark{
\textbf{ä¸‰ç§Attentionçš„åŒºåˆ«}
\begin{itemize}
  \item åœ¨ {\color{tred}Self-Attention} ä¸­ï¼Œ$Q, K, V$ éƒ½æ¥è‡ªåŒä¸€ä¸ªåºåˆ—ï¼ˆEncoder å†…éƒ¨æˆ– Decoder å†…éƒ¨ï¼‰ã€‚
  \item åœ¨ {\color{tred}Masked Self-Attention} ä¸­ï¼ˆDecoder ç¬¬ä¸€å­å±‚ï¼‰ï¼Œ$Q, K, V$ ä¹Ÿæ¥è‡ª Decoder å†…éƒ¨ï¼Œä½†ä¼šé€šè¿‡æ©ç å±è”½æœªæ¥ tokenã€‚
  \item åœ¨ {\color{tred}Cross-Attention} ä¸­ï¼Œ$Q$ æ¥è‡ª Decoderï¼Œ$K$ å’Œ $V$ æ¥è‡ª Encoderï¼Œå› æ­¤å®ƒè¿æ¥äº†æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„ä¿¡æ¯ã€‚
\end{itemize}
}

è¿™ç§è®¾è®¡ä¿è¯äº† Decoder åœ¨ç”Ÿæˆæ¯ä¸€ä¸ªç›®æ ‡ token æ—¶ï¼Œéƒ½å¯ä»¥ç›´æ¥è®¿é—®å¹¶åˆ©ç”¨ Encoder ç¼–ç å¥½çš„å…¨å±€æºåºåˆ—ä¿¡æ¯ï¼Œè€Œæ— éœ€æ‹…å¿ƒæœªæ¥ä¿¡æ¯æ³„éœ²é—®é¢˜ã€‚

\subsubsection{è®­ç»ƒä¸æ¨ç†å·®å¼‚}

åœ¨ Decoder ä¸­ï¼Œè®­ç»ƒä¸æ¨ç†çš„ä¸»è¦å·®å¼‚åœ¨äºè¾“å…¥å†…å®¹ä¸ç”Ÿæˆæ–¹å¼ï¼š
\begin{itemize}
  \item \hyperref[def:teacher-forcing]{\textbf{è®­ç»ƒï¼ˆTeacher Forcingï¼‰}}ï¼šDecoder è¾“å…¥å®Œæ•´çš„ç›®æ ‡åºåˆ—ï¼ˆå³ç§»ä¸€ä½ä½œä¸ºè¾“å…¥ï¼‰ï¼Œæ¯ä¸ªæ—¶é—´æ­¥éƒ½èƒ½çœ‹åˆ°çœŸå®çš„å†å² tokenï¼Œé€šè¿‡ Masked Self-Attention é¿å…è®¿é—®æœªæ¥ tokenã€‚
  \item \hyperref[def:auto-regressive]{\textbf{æ¨ç†ï¼ˆAuto-regressiveï¼‰}}ï¼šDecoder æ¯æ¬¡ä»…è¾“å…¥å·²ç”Ÿæˆçš„ tokenï¼ˆåˆå§‹ä»…æœ‰ \texttt{<BOS>} èµ·å§‹ç¬¦ï¼‰ï¼Œä¾æ¬¡é€šè¿‡ Masked Self-Attention ç”Ÿæˆä¸‹ä¸€ä¸ª tokenï¼Œç›´åˆ°ç”Ÿæˆç»“æŸç¬¦æˆ–è¾¾åˆ°æœ€å¤§é•¿åº¦ã€‚
\end{itemize}



\clearpage
\subsection{Output Projection \& Softmax}

Decoder å †æ ˆåœ¨æ¯ä¸ªæ—¶é—´æ­¥ $t$ ä¼šè¾“å‡ºä¸€ä¸ªéšè—å‘é‡ $\mathbf{h}_t \in \mathbb{R}^d$ã€‚ä¸ºäº†å°†è¿™ä¸ªå‘é‡è½¬åŒ–ä¸ºå…·ä½“çš„è¯ï¼Œæˆ‘ä»¬éœ€è¦ä¸¤æ­¥ï¼š

\MarginNote{
  \Definition{Logits}{
    ç¥ç»ç½‘ç»œæœ€åä¸€å±‚åœ¨ Softmax ä¹‹å‰çš„{\color{tred}åŸå§‹æœªå½’ä¸€åŒ–åˆ†æ•°å‘é‡}ã€‚
    å®ƒçš„é•¿åº¦ç­‰äºè¯æ±‡è¡¨å¤§å°ï¼Œæ¯ä¸ªåˆ†é‡å¯¹åº”ä¸€ä¸ªè¯çš„æ‰“åˆ†ï¼Œå¯èƒ½ä¸ºæ­£æˆ–è´Ÿï¼Œæ€»å’Œä¸ä¸€å®šä¸º 1ã€‚
    Softmax ä¼šå°† logits è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œç”¨äºç”Ÿæˆä¸‹ä¸€ä¸ª tokenã€‚
    \label{def:logits}
  }
}


\begin{enumerate}
  \item \textbf{çº¿æ€§æ˜ å°„ï¼ˆLinear Layerï¼‰}  
  ä½¿ç”¨ä¸€ä¸ªå…¨è¿æ¥å±‚å°† $\mathbf{h}_t$ æŠ•å½±åˆ°è¯æ±‡è¡¨å¤§å° $|V|$ çš„ \hyperref[def:logits]{logits} å‘é‡ï¼š
  \begin{equation}
  \mathbf{z}_t = \mathbf{h}_t W_o + \mathbf{b}_o,
  \end{equation}
  å…¶ä¸­ $W_o \in \mathbb{R}^{d \times |V|}$ï¼Œ$\mathbf{b}_o \in \mathbb{R}^{|V|}$ã€‚  
  è‹¥è¯è¡¨å¤§å°ä¸º $10{,}000$ï¼Œåˆ™ $\mathbf{z}_t$ çš„æ¯ä¸ªåˆ†é‡å¯¹åº”ä¸€ä¸ªè¯çš„åˆ†æ•°ã€‚
  
  \item \textbf{Softmax æ¦‚ç‡å½’ä¸€åŒ–}  
  å°† logits è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼š
  \begin{equation}
  p_t^{(i)} = \frac{\exp(z_t^{(i)})}{\sum_{j=1}^{|V|} \exp(z_t^{(j)})},
  \end{equation}
  ä½¿å¾—æ‰€æœ‰æ¦‚ç‡å‡ä¸ºæ­£ä¸”æ€»å’Œä¸º $1$ã€‚æ¦‚ç‡æœ€å¤§çš„è¯å³ä¸ºè¯¥æ—¶é—´æ­¥ç”Ÿæˆçš„è¾“å‡ºã€‚
\end{enumerate}

Softmax çš„è¾“å‡ºå¯ä»¥ç”¨äºä¸åŒç”Ÿæˆç­–ç•¥ï¼ˆå¦‚ $\arg\max$ é€‰å–ã€éšæœºé‡‡æ ·æˆ– Top-$k$ / nucleus é‡‡æ ·ï¼‰ï¼Œä»è€Œå¾—åˆ°ä¸‹ä¸€ä¸ª tokenï¼Œå¹¶å°†å…¶åé¦ˆåˆ°ä¸‹ä¸€æ­¥è§£ç ã€‚

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/lec2/lec2.26.png}
  \caption{Linear + Softmax}
  \label{fig:Linear + Softmax}
\end{figure}









\clearpage
{\chaptoc\noindent\begin{minipage}[inner sep=0,outer sep=0]{0.9\linewidth}\section{Memory Accounting}\end{minipage}}

\subsection{Tensors Basics}
Tensors are the fundamental building block in PyTorch, used to store {\color{dblue}parameters, gradients, optimizer state, data, and activations}.

è¿™é‡Œæ˜¯Pytorchçš„tensorsæ‰‹å†Œï¼š\href{https://pytorch.org/docs/stable/tensors.html}{Pytorch docs on tensors}

åˆ›å»ºtensorçš„æ–¹å¼æœ‰å¾ˆå¤šç§ï¼Œå¸¸ç”¨çš„æœ‰ï¼š
\begin{itemize}
    \item \texttt{torch.tensor([[1., 2, 3], [4, 5, 6]])} --- Create from data.
    \item \texttt{torch.zeros(4, 8)}, \texttt{torch.ones(4, 8)} --- Filled with zeros or ones.
    \item \texttt{torch.randn(4, 8)} --- Samples from $\mathcal{N}(0, 1)$.
    \item \texttt{torch.empty(4, 8)} --- Allocate without initialization (\texttt{torch.empty()} åªåˆ†é…å†…å­˜ï¼Œä¸åšåˆå§‹åŒ–ï¼Œæ‰€ä»¥å¾—åˆ°çš„tensoræ˜¯åˆ†é…åˆ°çš„å†…å­˜å—é‡Œçš„åŸå§‹æ•°æ®).
\end{itemize}


\MarginNote{
\textbf{\texttt{nn.init.trunc\_normal\_}}  \\
å°†å¼ é‡ç”¨æˆªæ–­æ­£æ€åˆ†å¸ƒ $\mathcal{N}(\mu, \sigma^2)$ çš„å€¼è¿›è¡Œåˆå§‹åŒ–ï¼Œ  
ä»…ä¿ç•™è½åœ¨åŒºé—´ $[a, b]$ å†…çš„æ ·æœ¬ï¼Œå…¶ä½™ä¼šé‡æ–°é‡‡æ ·ã€‚  

è¯¥æ“ä½œä¸ºåŸåœ°ï¼ˆin-placeï¼‰ä¿®æ”¹ï¼Œä¼šç›´æ¥æ”¹å˜ä¼ å…¥å¼ é‡çš„å†…å®¹ã€‚  
å¸¸ç”¨äºç½‘ç»œæƒé‡åˆå§‹åŒ–ï¼Œä»¥é¿å…å‡ºç°è¿‡å¤§æˆ–è¿‡å°çš„æç«¯å€¼ã€‚ 
}

\Example{Custom initialization}
{
æœ‰æ—¶ä½ ä¼šå…ˆåˆ†é…ä¸€ä¸ªæœªåˆå§‹åŒ–çš„å¼ é‡ï¼ˆä¾‹å¦‚ \texttt{torch.empty}ï¼‰ï¼Œ
å› ä¸ºä½ å¸Œæœ›ç¨åç”¨è‡ªå®šä¹‰é€»è¾‘æ¥è®¾ç½®å®ƒçš„å€¼ã€‚

ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ \texttt{nn.init.trunc\_normal\_} å°†å¼ é‡æŒ‰æˆªæ–­æ­£æ€åˆ†å¸ƒ

$\mathcal{N}(\mu, \sigma^2)$ åˆå§‹åŒ–ï¼Œé™åˆ¶å€¼åœ¨åŒºé—´ $[a, b]$ å†…ï¼š

\texttt{nn.init.trunc\_normal\_(x, mean=0, std=1, a=-2, b=2)  \# æˆªæ–­åœ¨ [-2, 2]}
}



\subsection{Tensors Memory}
\Remark{Almost everything (parameters, gradients, activations, optimizer states) are stored as floating point numbers.}

åœ¨å­¦ä¹ ä¸åŒData Typeä¹‹å‰æˆ‘ä»¬éœ€è¦å›é¡¾ä¸€ä¸‹Sign bit, Exponent widthå’ŒSignificand precisionä¸‰ä¸ªåŸºæœ¬æ¦‚å¿µã€‚

\Definition{Sign Bit}{
æµ®ç‚¹æ•°æœ€é«˜ä½ç”¨äºè¡¨ç¤ºæ­£è´Ÿå·ï¼Œç§°ä¸ºç¬¦å·ä½ï¼ˆSign bitï¼‰ã€‚  
ç¬¦å·ä½ä¸º $0$ è¡¨ç¤ºæ­£æ•°ï¼Œç¬¦å·ä½ä¸º $1$ è¡¨ç¤ºè´Ÿæ•°ã€‚
}

\Definition{Exponent Width}{
æŒ‡æ•°å®½åº¦ï¼ˆExponent widthï¼‰æŒ‡æµ®ç‚¹æ•°ç”¨äºå­˜å‚¨æŒ‡æ•°çš„äºŒè¿›åˆ¶ä½æ•°ã€‚  
æŒ‡æ•°çš„ä½æ•°å†³å®šäº†æµ®ç‚¹æ•°å¯è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´ï¼ˆèŒƒå›´è¶Šå¤§ï¼Œèƒ½è¡¨ç¤ºçš„æ•°è¶Šå¤§æˆ–è¶Šå°ï¼‰ã€‚
}

\Definition{Significand Precision}{
æœ‰æ•ˆæ•°ç²¾åº¦ï¼ˆSignificand precisionï¼‰æŒ‡æµ®ç‚¹æ•°ç”¨äºå­˜å‚¨æœ‰æ•ˆæ•°å­—çš„äºŒè¿›åˆ¶ä½æ•°ã€‚  
æœ‰æ•ˆä½æ•°è¶Šå¤šï¼Œæ•°å€¼è¡¨ç¤ºå¾—è¶Šç²¾ç¡®ï¼Œè®¡ç®—ç»“æœçš„ç²¾åº¦ä¹Ÿè¶Šé«˜ã€‚
}

\Definition{Floating-Point Representation}{
æµ®ç‚¹æ•°åœ¨è®¡ç®—æœºä¸­çš„è¡¨ç¤ºé€šå¸¸éµå¾ª IEEE 754 æ ‡å‡†ï¼Œå…¶é€šå¼ä¸ºï¼š
\[
(-1)^{\text{sign}} \times (1.\text{fraction})_2 \times 2^{\text{exponent} - \text{bias}}
\]
å…¶ä¸­ï¼š
\begin{itemize}
    \item \textbf{sign} --- ç¬¦å·ä½ï¼ˆSign bitï¼‰ï¼Œå†³å®šæ•°å€¼æ­£è´Ÿã€‚
    \item \textbf{fraction} --- æœ‰æ•ˆæ•°å­—éƒ¨åˆ†ï¼ˆSignificandï¼‰ï¼Œç”¨äºŒè¿›åˆ¶è¡¨ç¤ºã€‚
    \item \textbf{exponent} --- æŒ‡æ•°éƒ¨åˆ†ï¼Œå†³å®šæ•°å€¼çš„æ•°é‡çº§ã€‚
    \item \textbf{bias} --- æŒ‡æ•°åç§»é‡ï¼ˆBiasï¼‰ï¼Œç”±æŒ‡æ•°ä½æ•°å†³å®šï¼Œä¾‹å¦‚ 8 ä½æŒ‡æ•°æ—¶ \(\text{bias} = 127\)ã€‚
\end{itemize}
}


\subsubsection{float32}
åœ¨ PyTorch ä¸­ï¼Œé»˜è®¤æ•°æ®ç±»å‹ä¸º \texttt{\href{https://en.wikipedia.org/wiki/Single-precision_floating-point_format}{float32}}ï¼ˆå•ç²¾åº¦ï¼Œfp32ï¼‰ï¼Œåœ¨ç§‘å­¦è®¡ç®—ä¸­ï¼Œ\texttt{float32} æ˜¯åŸºå‡†ç±»å‹ï¼Œä¹Ÿå¯åœ¨éœ€è¦æ—¶ä½¿ç”¨åŒç²¾åº¦ \texttt{float64}ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ•°å€¼ç²¾åº¦è¦æ±‚ç›¸å¯¹å®½æ¾ã€‚

\MarginNote{
\Example{å†…å­˜è®¡ç®—ç¤ºä¾‹}{
\texttt{
x = torch.zeros(4, 8)\\
assert x.dtype == torch.float32 \# é»˜è®¤ç±»å‹\\
assert x.numel() == 4 * 8 \# å…ƒç´ æ•°é‡\\
assert x.element\_size() == 4 \# float32 å  4 å­—èŠ‚\\
assert get\_memory\_usage(x) == 4 * 8 * 4 \# 128 å­—èŠ‚
}
}
}

\Remark{
\textbf{å¼ é‡çš„å†…å­˜å ç”¨ç”±ä»¥ä¸‹ä¸¤éƒ¨åˆ†å†³å®šï¼š}
\begin{enumerate}
    \item \textbf{å…ƒç´ æ•°é‡}ï¼ˆ\texttt{numel}ï¼‰ï¼šå¼ é‡ä¸­å€¼çš„æ€»ä¸ªæ•°ã€‚
    \item \textbf{æ•°æ®ç±»å‹å¤§å°}ï¼ˆ\texttt{element\_size}ï¼‰ï¼šæ¯ä¸ªå…ƒç´ å ç”¨çš„å­—èŠ‚æ•°ã€‚
\end{enumerate}
}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/lec2/lec2.27.png}
  \caption{float32 å†…å­˜ç»“æ„ç¤ºæ„å›¾}
  \label{fig:float32 å†…å­˜ç»“æ„ç¤ºæ„å›¾}
\end{figure}



The IEEE 754 standard specifies a binary32 as having:
\begin{itemize}
    \item Sign bit: 1 bit
    \item Exponent width: 8 bits
    \item Significand precision: 24 bits (23 explicitly stored)
\end{itemize} 

\MarginImageWithNote
  {figs/lec2/lec2.28.jpeg}
  {\captionof{figure}{FP32çš„ä¸€äº›ç‰¹æ€§}}
  {å›¾æºæœ¬äººçš„CS61Cçš„Study Sheet}



\clearpage
\subsubsection{float16}

\texttt{float16}ï¼ˆåŠç²¾åº¦ï¼Œfp16ï¼‰å°†æ¯ä¸ªæµ®ç‚¹æ•°çš„å†…å­˜å ç”¨ä» \texttt{float32} çš„ 4 å­—èŠ‚å‡å°‘åˆ° 2 å­—èŠ‚ï¼Œå¯ä»¥æ˜¾è‘—èŠ‚çœæ˜¾å­˜ã€‚

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/lec2/lec2.29.png}
  \caption{float16 å†…å­˜ç»“æ„ç¤ºæ„å›¾}
  \label{fig:float16 å†…å­˜ç»“æ„ç¤ºæ„å›¾}
\end{figure}


\MarginNote{
\Definition{Overflow and Underflow}{
\textbf{æº¢å‡ºï¼ˆOverflowï¼‰}æ˜¯æŒ‡è®¡ç®—ç»“æœçš„ç»å¯¹å€¼è¶…è¿‡äº†è¯¥æ•°æ®ç±»å‹æ‰€èƒ½è¡¨ç¤ºçš„æœ€å¤§æœ‰é™å€¼ã€‚  
å½“å‘ç”Ÿæº¢å‡ºæ—¶ï¼Œç»“æœä¼šè¢«è¡¨ç¤ºä¸º $+\infty$ æˆ– $-\infty$ï¼ˆå³ IEEE 754 çš„ Infinityï¼‰ã€‚

ä¾‹å¦‚ï¼Œåœ¨ \texttt{float32} ä¸­ï¼š
\[
x = 1\times 10^{40} \quad \Rightarrow \quad x \to +\infty
\]

\textbf{ä¸‹æº¢ï¼ˆUnderflowï¼‰}æ˜¯æŒ‡è®¡ç®—ç»“æœçš„ç»å¯¹å€¼å°äºè¯¥æ•°æ®ç±»å‹æ‰€èƒ½è¡¨ç¤ºçš„æœ€å°æ­£æ•°ï¼ˆåŒ…æ‹¬æ¬¡æ­£è§„æ•°èŒƒå›´ï¼‰ã€‚  
å½“å‘ç”Ÿä¸‹æº¢æ—¶ï¼Œç»“æœå¯èƒ½ä¼šå˜ä¸º \texttt{0}ï¼ˆflush-to-zeroï¼‰æˆ–ä»¥è¾ƒä½ç²¾åº¦çš„æ¬¡æ­£è§„æ•°è¡¨ç¤ºã€‚

ä¾‹å¦‚ï¼Œåœ¨ \texttt{float16} ä¸­ï¼š
\[
x = 1\times 10^{-8} \quad \Rightarrow \quad x \to 0
\]
}
}

The IEEE 754 standard specifies a binary16 as having the following format:
\begin{itemize}
  \item Sign bit: 1 bit
  \item Exponent width: 5 bits
  \item Significand precision: 11 bits (10 explicitly stored)
\end{itemize} 

\texttt{float16} çš„åŠ¨æ€èŒƒå›´è¾ƒå·®ï¼Œå®¹æ˜“å‘ç”Ÿä¸‹æº¢ï¼ˆUnderflowï¼‰ã€‚


\subsubsection{bfloat16}

Google Brain åœ¨ 2018 å¹´æå‡ºäº† \texttt{bfloat16}ï¼ˆBrain Floating Pointï¼‰ï¼Œ
ç›®çš„æ˜¯ç¼“è§£ \texttt{float16} åŠ¨æ€èŒƒå›´è¿‡å°çš„é—®é¢˜ã€‚
\texttt{bfloat16} ä¸ \texttt{float16} å ç”¨ç›¸åŒå†…å­˜ï¼ˆ2 å­—èŠ‚ï¼‰ï¼Œ
ä½†å…¶åŠ¨æ€èŒƒå›´ä¸ \texttt{float32} ç›¸åŒï¼
ä»£ä»·æ˜¯ç²¾åº¦ï¼ˆæœ‰æ•ˆä½æ•°ï¼‰æ›´ä½ï¼Œä¸è¿‡åœ¨æ·±åº¦å­¦ä¹ ä¸­é€šå¸¸æ— ç¢ã€‚

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/lec2/lec2.30.png}
  \caption{float16 å†…å­˜ç»“æ„ç¤ºæ„å›¾}
  \label{fig:float16 å†…å­˜ç»“æ„ç¤ºæ„å›¾}
\end{figure}

bfloat16 has the following format:
\begin{itemize}
  \item Sign bit: 1 bit
  \item Exponent width: 8 bits
  \item Significand precision: 8 bits (7 explicitly stored, with an implicit leading bit), as opposed to 24 bits in a classical single-precision floating-point format
\end{itemize}

\MarginImageWithNote
  {figs/lec2/lec2.31.png}
  {\captionof{figure}{ä¸‰ç§Data Typeçš„æ€§èƒ½æ¯”è¾ƒ}}


\clearpage
\subsubsection{fp8}
2022 å¹´ï¼ŒFP8 æ ¼å¼è¢«æ ‡å‡†åŒ–ï¼Œä¸»è¦æ˜¯ä¸ºæœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½ä¼˜åŒ–ã€‚
NVIDIA H100 æ”¯æŒä¸¤ç§ FP8 å˜ä½“ï¼š

\textbf{E4M3}ï¼š4 ä½æŒ‡æ•°ï¼Œ3 ä½å°¾æ•°ï¼Œæœ‰æ•ˆèŒƒå›´çº¦ä¸º $[-448, 448]$ï¼Œç²¾åº¦è¾ƒé«˜ã€èŒƒå›´çª„ã€‚

\textbf{E5M2}ï¼š5 ä½æŒ‡æ•°ï¼Œ2 ä½å°¾æ•°ï¼Œæœ‰æ•ˆèŒƒå›´çº¦ä¸º $[-57344, 57344]$ï¼ŒèŒƒå›´å¹¿ã€ç²¾åº¦ä½ã€‚

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/lec2/lec2.32.png}
  \caption{fp8 å†…å­˜ç»“æ„ç¤ºæ„å›¾}
  \label{fig:fp8 å†…å­˜ç»“æ„ç¤ºæ„å›¾}
\end{figure}

\Remark{
è®­ç»ƒæ—¶ï¼š
\begin{itemize}
    \item ä½¿ç”¨ \texttt{float32} è®­ç»ƒç¨³å®šï¼Œä½†æ˜¾å­˜å ç”¨å¤§ã€‚
    \item ç›´æ¥ä½¿ç”¨ \texttt{fp8}ã€\texttt{float16} æˆ– \texttt{bfloat16} è®­ç»ƒï¼Œé£é™©è¾ƒé«˜ï¼Œæ˜“æ•°å€¼ä¸ç¨³å®šã€‚
    \item è§£å†³æ–¹æ¡ˆï¼šæ··åˆç²¾åº¦è®­ç»ƒï¼ˆ\texttt{mixed\_precision\_training}ï¼‰ã€‚
\end{itemize}
}

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l l l l l}
\hline
\textbf{Property} & \textbf{fp32} & \textbf{fp16} & \textbf{bfloat16} & \textbf{fp8 (E4M3 / E5M2)} \\
\hline
Sign bit & 1 & 1 & 1 & 1 \\
Exponent width & 8 & 5 & 8 & 4 / 5 \\
Significand precision & 23 bits & 10 bits & 7 bits & 3 bits / 2 bits \\
Largest finite value & $3.4\times 10^{38}$ & $6.55\times 10^{4}$ & $3.4\times 10^{38}$ & $\approx 448$ / $\approx 5.73\times 10^{4}$ \\
Smallest positive normal & $1.17\times 10^{-38}$ & $6.10\times 10^{-5}$ & $1.17\times 10^{-38}$ & $\approx 2^{-6}$ / $\approx 2^{-14}$ \\
Element size & 4 bytes & 2 bytes & 2 bytes & 1 byte \\
Supports Infinity / NaN & Yes & Yes & Yes & Yes \\
Underflow risk & Very low & High & Very low & Very high \\
\hline
\end{tabular}
\caption{Comparison of common floating-point formats for deep learning}
\label{tab:fp_all_compare}
\end{table}

\clearpage
\subsection{Mixed Precision Training \href{https://arxiv.org/pdf/1710.03740.pdf}{[Micikevicius et al., 2017]}}

\paragraph{æŠ€æœ¯èƒŒæ™¯}~{}
\begin{itemize}
    \item æ·±åº¦å­¦ä¹ æ¨¡å‹å¤æ‚åŒ–ä¸æ•°æ®é›†è§„æ¨¡ä¸æ–­å¢é•¿ï¼Œæ˜¾è‘—å¢åŠ æ˜¾å­˜å’Œè®¡ç®—èµ„æºéœ€æ±‚ã€‚
    \item ä½¿ç”¨ä½ç²¾åº¦ï¼ˆå¦‚ FP16ï¼‰å¯å‡å°‘æ˜¾å­˜å ç”¨å¹¶æå‡è®¡ç®—ååç‡ï¼Œåœ¨æ–° GPUä¸Šå¯å¸¦æ¥ 2--8 å€ç®—åŠ›æå‡ã€‚
    \item ç›´æ¥ç”¨ FP16 è®­ç»ƒä¼šå› æ•°å€¼èŒƒå›´æœ‰é™è€Œå¼•å‘è®­ç»ƒä¸ç¨³å®šæˆ–ç²¾åº¦ä¸‹é™ã€‚
\end{itemize}


\MarginImageWithNote
  {figs/lec2/lec2.34.png}
  {\captionof{figure}{FP32å’ŒFP16çš„æ€§èƒ½å¯¹æ¯”}}
  {
\footnotesize
ç¥ç»ç½‘ç»œè®­ç»ƒä¸æ¨ç†çš„æ€§èƒ½ç“¶é¢ˆä¸»è¦æœ‰ä¸‰ç±»ï¼š
\begin{itemize}[leftmargin=0.5em]
    \item ç®—æœ¯å¸¦å®½ï¼ˆArithmetic Bandwidthï¼‰
    \item å†…å­˜å¸¦å®½ï¼ˆMemory Bandwidthï¼‰
    \item å»¶è¿Ÿï¼ˆLatencyï¼‰
\end{itemize}
é‡‡ç”¨ä½ç²¾åº¦å¯ç¼“è§£å‰ä¸¤è€…ï¼š
\begin{itemize}[leftmargin=0.5em]
    \item æ›´å°‘æ¯”ç‰¹å­˜å‚¨ï¼Œé™ä½å†…å­˜å¸¦å®½å‹åŠ›
    \item é«˜ååä½ç²¾åº¦è¿ç®—ï¼Œå‡å°‘ç®—æœ¯æ—¶é—´
\end{itemize}
ä¾‹å¦‚åœ¨æ–°ä¸€ä»£ GPU ä¸­ï¼ŒåŠç²¾åº¦ååç‡å¯è¾¾å•ç²¾åº¦çš„ 2--8 å€ï¼Œ
ä¸”è®­ç»ƒæ˜¾å­˜éœ€æ±‚ä¹Ÿéšä¹‹é™ä½ã€‚
}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/lec2/lec2.33.png}
  \caption{FP32 vs. FP16}
  \label{fig:FP32 vs. FP16}
\end{figure}

\Remark{Mixed Precision Training çš„ç›®æ ‡æ˜¯åœ¨ç¼“è§£å†…å­˜å‹åŠ›ã€æå‡è®¡ç®—ååçš„åŒæ—¶ä¿æŒæ¨¡å‹ç²¾åº¦ï¼Œ
é€šè¿‡åœ¨å…³é”®ç¯èŠ‚ä¿ç•™é«˜ç²¾åº¦ã€éå…³é”®ç¯èŠ‚ä½¿ç”¨ä½ç²¾åº¦ï¼Œä»è€Œå…¼é¡¾æ€§èƒ½ä¸å‡†ç¡®æ€§ã€‚}


\paragraph{æ ¸å¿ƒæŒ‘æˆ˜â€”â€”FP16çš„ä½ç²¾åº¦å’ŒNarrow dynamic range:}~{}
\begin{itemize}
    \item \textbf{ç²¾åº¦æŸå¤±}ï¼šå¤šæ¬¡ç´¯åŠ  FP16 ä¹˜ç§¯æ—¶ç²¾åº¦æŸå¤±æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯çŸ©é˜µä¹˜æ³•ã€å¤§è§„æ¨¡æ±‚å’Œç­‰è¿ç®—ä¸­ã€‚
    \item \textbf{Narrow dynamic range}ï¼šFP16ä½ç²¾åº¦å¯èƒ½å¯¼è‡´æ¢¯åº¦æ›´æ–°ä¸ç¨³å®šï¼Œæ¢¯åº¦åˆ†å¸ƒä¸­å¤§é‡å°å€¼å‘ç”Ÿ underflowï¼Œå½±å“æ¨¡å‹æ”¶æ•›ã€‚
\end{itemize}

\paragraph{æŠ€æœ¯è·¯å¾„}~{}

\textbf{{\color{tred}Single-precision master weights and updates, loss-scaling, and accumulating FP16 products into FP32}}


\begin{enumerate}
    \item \textbf{FP32 Master Copy of Weights}ï¼šåœ¨å‰å‘ä¸åå‘ä¼ æ’­ä¸­ï¼Œæƒé‡ã€æ¿€æ´»ã€æ¢¯åº¦å‡ç”¨ FP16 å­˜å‚¨å’Œè®¡ç®—ï¼›ä½†åœ¨ä¼˜åŒ–å™¨æ›´æ–°æ—¶ï¼Œç»´æŠ¤ä¸€ä»½ FP32 ä¸»å‰¯æœ¬æ¥ç´¯ç§¯æ¢¯åº¦å¹¶æ›´æ–°æƒé‡ï¼Œå†å°†å…¶è½¬æ¢ä¸º FP16 ç”¨äºä¸‹ä¸€è½®è¿­ä»£ã€‚è¿™å¯ä»¥é¿å…æ›´æ–°é‡è¿‡å°è€Œåœ¨ FP16 ä¸‹èˆå…¥ä¸ºé›¶çš„é—®é¢˜ã€‚

    \item \textbf{Loss Scaling}ï¼šç”±äºæ¢¯åº¦å€¼åˆ†å¸ƒä¸­å¤§é‡æ•°å€¼å¾ˆå°ï¼Œå®¹æ˜“åœ¨ FP16 ä¸‹å‘ç”Ÿä¸‹æº¢ï¼ˆunderflowï¼‰è€Œå˜æˆé›¶ï¼Œè®­ç»ƒä¼šå‘æ•£ã€‚é€šè¿‡åœ¨åå‘ä¼ æ’­å‰å°†æŸå¤±ä¹˜ä»¥ä¸€ä¸ªå¸¸æ•°å› å­ï¼ˆå¦‚ $8$ã€$128$ï¼‰ï¼Œå¯åŒæ­¥æ”¾å¤§æ‰€æœ‰æ¢¯åº¦ï¼Œä½¿å®ƒä»¬è½å…¥ FP16 å¯è¡¨ç¤ºèŒƒå›´ï¼›åœ¨æ›´æ–°å‰å†é™¤å›è¯¥å› å­ï¼Œä¿æŒä¸ FP32 è®­ç»ƒä¸€è‡´çš„æ›´æ–°å¹…åº¦ã€‚
    
    \item \textbf{Arithmetic Precision}ï¼šå¯¹å‘é‡ç‚¹ç§¯ã€å·ç§¯ç­‰æ“ä½œï¼Œè¾“å…¥ç”¨ FP16 å­˜å‚¨ï¼Œä½†ä¹˜æ³•å…ˆè½¬æ¢åˆ° FP32 ç²¾åº¦è®¡ç®—ï¼Œå¹¶åœ¨ FP32 ç´¯åŠ å™¨ä¸­ç´¯ç§¯ç»“æœï¼Œæœ€åå†å­˜å› FP16ï¼›å¤§è§„æ¨¡æ±‚å’Œï¼ˆå¦‚ Batch Norm ç»Ÿè®¡é‡ã€Softmax å½’ä¸€åŒ–ï¼‰ä¹Ÿåœ¨ FP32 ä¸‹è¿›è¡Œï¼›é€å…ƒç´ æ“ä½œåˆ™å¯ç›´æ¥ç”¨ FP16ã€‚
\end{enumerate}


\subparagraph{FP32 Master Copy of Weights}~{}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.1\linewidth]{figs/lec2/lec2.36.png}
  \caption{Mixed precision training iteration for a layer.}
  \label{Mixed precision training iteration for a layer.}
\end{figure}

\subparagraph{Loss Scaling}~{}


\MarginImageWithNote
  {figs/lec2/lec2.38.png}
  {\captionof{figure}{Multibox SSD ç½‘ç»œè®­ç»ƒæ—¶æ¿€æ´»æ¢¯åº¦çš„åˆ†å¸ƒç›´æ–¹å›¾}}
  {
  \footnotesize
  æ¨ªè½´ä¸ºæ¢¯åº¦å¤§å°çš„ $\log_2$ï¼Œçºµè½´ä¸ºè¯¥èŒƒå›´æ¢¯åº¦å æ‰€æœ‰æ¢¯åº¦çš„ç™¾åˆ†æ¯”ï¼Œæœ€å·¦ä¾§å•ç‹¬çš„æŸ±è¡¨ç¤ºæ¢¯åº¦ä¸º 0 çš„æ¯”ä¾‹ã€‚

  \textbf{å…³é”®æ ‡è®°ï¼š}
  \begin{itemize}[leftmargin=0.5em]
      \item \textcolor{red}{çº¢çº¿}ï¼šFP16 normsï¼ˆ$\approx 2^{-24}$ï¼‰ï¼Œå·¦ä¾§æ¢¯åº¦åœ¨ FP16 ä¸‹ç›´æ¥å˜ä¸º 0
      \item \textcolor{blue}{è“çº¿}ï¼šFP16 denormsä¸‹é™ï¼ˆ$\approx 2^{-14}$ï¼‰ï¼ŒèŒƒå›´å†…ç²¾åº¦æ˜¾è‘—ä¸‹é™
      \item \textcolor{black}{é»‘è‰²ç®­å¤´}ï¼šFP16 çš„æ•´ä½“å¯è¡¨ç¤ºèŒƒå›´ä¸Šé™
  \end{itemize}

  ç›´æ¥ç”¨ FP16 å­˜å‚¨æ¢¯åº¦æ—¶ï¼Œä¼šä¸¥é‡ \emph{underflow}ï¼Œçº¦ $67\%$ çš„æ¢¯åº¦å€¼ä¸ºé›¶ã€‚
  å› æ­¤éœ€è¦åœ¨åå‘ä¼ æ’­ä¸­é‡‡ç”¨ \textbf{Loss Scaling} æ¥ç¼“è§£ã€‚
  }


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figs/lec2/lec2.37.png}
  \caption{Loss Scaling}
\label{Loss Scaling}
\end{figure}


\clearpage
\subparagraph{Arithmetic Precision}~{}

åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸­ï¼Œç°ä»£ GPUï¼ˆå¦‚ Volta/Turing/Ampere çš„ Tensor Coresï¼‰å¸¸é‡‡ç”¨
\textbf{FP16 inputsã€FP32 multiply-accumulate (MAC)} çš„ç­–ç•¥ï¼Œä»¥å…¼é¡¾ååä¸ç²¾åº¦ï¼š

\begin{itemize}[leftmargin=1em]
    \item \textbf{å­˜å‚¨ä¸æ¬è¿é˜¶æ®µ}ï¼š\textit{weights} ä¸ \textit{activations} åœ¨æ˜¾å­˜å’Œå¯„å­˜å™¨ä¸­ä»¥ FP16 å­˜å‚¨ï¼Œé™ä½æ˜¾å­˜å ç”¨ä¸å†…å­˜å¸¦å®½å‹åŠ›ï¼›
    \item \textbf{ç¡¬ä»¶æ‰§è¡Œé˜¶æ®µï¼ˆTensor Core å†…éƒ¨ï¼‰}ï¼š
    \begin{enumerate}[label= (\alph*), leftmargin=1.5em]
        \item æ¯å¯¹ FP16 æ“ä½œæ•°åœ¨è¿›å…¥è¿ç®—ç®¡çº¿å‰ï¼Œç”±ç¡¬ä»¶é€»è¾‘\emph{è‡ªåŠ¨}æ‰©å±•ï¼ˆcastï¼‰ä¸º FP32ï¼›
        \item åœ¨ FP32 ç²¾åº¦ä¸‹å®Œæˆä¹˜æ³•ï¼ˆmultiplyï¼‰ï¼›
        \item ä¹˜ç§¯ç›´æ¥é€å…¥ FP32 ç´¯åŠ å™¨ï¼ˆaccumulatorï¼‰è¿›è¡ŒåŠ æ³•ç´¯ç§¯ï¼ˆaccumulateï¼‰ï¼Œä¿ç•™ 23 ä½å°¾æ•°ç²¾åº¦ï¼›
    \end{enumerate}
    \item \textbf{è¾“å‡ºé˜¶æ®µ}ï¼šç´¯ç§¯å®Œæˆåï¼Œå¯é€‰æ‹©ä¿æŒ FP32ï¼ˆç²¾åº¦æ•æ„Ÿè·¯å¾„ï¼‰æˆ–å‹ç¼©å› FP16ï¼ˆèŠ‚çœå­˜å‚¨ä¸å¸¦å®½ï¼‰ã€‚
\end{itemize}

ä»¥çŸ©é˜µä¹˜æ³•ä¸ºä¾‹ï¼š
\[
C_{ij} = \sum_{k=1}^n 
\mathrm{cast}(A_{ik})_{\mathrm{fp16}\to\mathrm{fp32}}
\times
\mathrm{cast}(B_{kj})_{\mathrm{fp16}\to\mathrm{fp32}}
\ \xrightarrow[\text{FP32 multiply}]{\text{Tensor Core}}\ 
\mathrm{product}_{\mathrm{fp32}}
\ \xrightarrow{\text{FP32 accumulate}}\ 
\mathrm{accumulator}_{\mathrm{fp32}}
\]


å…¶ä¸­ $A_{ik}$ ä¸ $B_{kj}$ è™½ä»¥ FP16 å­˜å‚¨ï¼Œä½†åœ¨è¿›å…¥ Tensor Core è®¡ç®—ç®¡çº¿æ—¶å³è¢«æ‰©å±•åˆ° FP32ï¼Œ
æ•´ä¸ª multiply-accumulate æµç¨‹å‡åœ¨ç¡¬ä»¶å†…éƒ¨çš„ FP32 æ•°æ®é€šè·¯ä¸Šå®Œæˆã€‚

è¿™ç§æ–¹å¼ç»“åˆäº†ï¼š
\begin{itemize}[leftmargin=1em]
    \item \textbf{FP16 å­˜å‚¨é€šè·¯}ï¼šé™ä½æ˜¾å­˜å’Œå¯„å­˜å™¨å‹åŠ›ï¼Œæå‡å†…å­˜å¸¦å®½ä¸å¹¶è¡Œåº¦ï¼›
    \item \textbf{FP32 è®¡ç®—é€šè·¯}ï¼šåœ¨ä¹˜æ³•ä¸ç´¯åŠ é˜¶æ®µä¿ç•™é«˜ç²¾åº¦ï¼Œæœ‰æ•ˆæŠ‘åˆ¶æ•°å€¼æ¼‚ç§»ã€‚
\end{itemize}

åœ¨è¯¥æ¨¡å¼ä¸‹ï¼ŒV100 çš„çŸ©é˜µä¹˜æ³•ååé‡å¯è¾¾ FP32 çš„ 8 å€ï¼ˆ125 TFLOPsï¼‰ï¼Œ
A100 æ›´å¯è¾¾ 16 å€ï¼ˆ312 TFLOPsï¼‰ã€‚

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figs/lec2/lec2.35.png}
  \caption{
    FP16 å­˜å‚¨ã€Tensor Core å†…éƒ¨ FP32 multiply-accumulate çš„æ•°æ®æµç¤ºæ„ã€‚
    è¾“å…¥å­˜å‚¨ä¸ºä½ç²¾åº¦ä»¥æé«˜ååï¼Œè®¡ç®—ä¸ç´¯åŠ åœ¨ç¡¬ä»¶å†…éƒ¨ä¿æŒé«˜ç²¾åº¦ä»¥ç¡®ä¿æ•°å€¼ç¨³å®šæ€§ã€‚
  }
\label{fig:arithmetic_precision}
\end{figure}


\subparagraph{AMP: Automatic Mixed Precision}~{}

\MarginImageWithNote
  {figs/lec2/lec2.39.png}
  {\captionof{figure}{AMP code example}}




\begin{itemize}
    \item PyTorch æä¾›äº† \href{https://pytorch.org/docs/stable/amp.html}{Automatic Mixed Precision (AMP)} åº“ï¼Œ
    å¯è‡ªåŠ¨åœ¨ \texttt{float32} ä¸ä½ç²¾åº¦ä¹‹é—´åˆ‡æ¢ã€‚
    \item NVIDIA çš„ \href{https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/}{æ€§èƒ½ä¼˜åŒ–æŒ‡å—}
    å’Œ \href{https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/}{Transformer Engine}
    æ”¯æŒåœ¨çº¿æ€§å±‚ä¸­ä½¿ç”¨ FP8ï¼Œå¹¶åœ¨è®­ç»ƒä¸­å¹¿æ³›ä½¿ç”¨.
\end{itemize}



\clearpage
{\chaptoc\noindent\begin{minipage}[inner sep=0,outer sep=0]{0.9\linewidth}\section{Compute Accounting}\end{minipage}}

\subsection{Tensors On Gpus}~{}

é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorch åˆ›å»ºçš„å¼ é‡ä¼šå­˜å‚¨åœ¨ CPU å†…å­˜ä¸­ï¼š

ä¸ºäº†åˆ©ç”¨ GPU çš„å¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œéœ€è¦å°†å¼ é‡ç§»åŠ¨åˆ° GPU å†…å­˜ã€‚

\begin{lstlisting}[language=Python]
    # é»˜è®¤åˆ›å»ºåœ¨ CPU
    x = torch.zeros(32, 32)
    assert x.device == torch.device("cpu")

    if torch.cuda.is_available():
        # GPU å±æ€§
        for i in range(torch.cuda.device_count()):
            print(torch.cuda.get_device_properties(i))

        mem0 = torch.cuda.memory_allocated()

        # CPU -> GPU0
        y = x.to("cuda:0")
        assert y.device == torch.device("cuda", 0)

        # æˆ–è€…ç›´æ¥åœ¨ GPU åˆ›å»º
        z = torch.zeros(32, 32, device="cuda:0")
\end{lstlisting}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/lec2/lec2.40.png}
  \caption{Move tensor from CPU to GPUs}
\end{figure}

\par

\begin{itemize}
    \item æ£€æŸ¥ GPU å†…å­˜ä½¿ç”¨æƒ…å†µï¼šå¯ä»¥ä½¿ç”¨ \texttt{torch.cuda.memory\_allocated()} æŸ¥çœ‹å½“å‰ GPU ä¸Šå·²åˆ†é…çš„å†…å­˜ã€‚
    \item é‡Šæ”¾ GPU å†…å­˜ï¼šä½¿ç”¨ \texttt{torch.cuda.empty\_cache()} å¯ä»¥é‡Šæ”¾æœªä½¿ç”¨çš„ GPU å†…å­˜ï¼Œä½†ä¸ä¼šå½±å“å½“å‰å·²åˆ†é…çš„å¼ é‡ã€‚
\end{itemize}



\clearpage
\subsection{Tensor Operations}

\subsubsection{Tensor Storage}

PyTorch å¼ é‡ï¼ˆTensorï¼‰æœ¬è´¨ä¸Šæ˜¯æŒ‡å‘å·²åˆ†é…å†…å­˜çš„æŒ‡é’ˆï¼Œå¤–åŠ æè¿°å¦‚ä½•è®¿é—®ä»»æ„å…ƒç´ çš„å…ƒæ•°æ®ï¼ˆmetadataï¼‰ã€‚

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/lec2/lec2.41.png}
  \caption{Tensor Storage}
  \label{fig:Tensor Storage}
\end{figure}  

\begin{lstlisting}[language=Python]

# åˆ°ä¸‹ä¸€è¡Œï¼ˆdim 0ï¼‰éœ€è¦è·³è¿‡ 4 ä¸ªå…ƒç´ 
assert x.stride(0) == 4

# åˆ°ä¸‹ä¸€åˆ—ï¼ˆdim 1ï¼‰éœ€è¦è·³è¿‡ 1 ä¸ªå…ƒç´ 
assert x.stride(1) == 1

# è®¡ç®—å…ƒç´  (r, c) çš„å­˜å‚¨ç´¢å¼•
r, c = 1, 2
index = r * x.stride(0) + c * x.stride(1)
assert index == 6
\end{lstlisting}

\subsubsection{Tensor Slicing}

å¾ˆå¤šæ“ä½œä»…ä»…æä¾›å¼ é‡çš„ä¸åŒ\textbf{è§†å›¾ï¼ˆviewï¼‰}ï¼Œä¸ä¼šå¤åˆ¶æ•°æ®ï¼Œå› æ­¤ä¸€ä¸ªå¼ é‡çš„ä¿®æ”¹ä¼šå½±å“å¦ä¸€ä¸ªã€‚
\MarginNote{
{\color{qpurple}\textbf{\texttt{torch.transpose(input, dim0, dim1)}}}  

äº¤æ¢å¼ é‡çš„ä¸¤ä¸ªç»´åº¦ï¼ˆ\texttt{dim0} ä¸ \texttt{dim1}ï¼‰ã€‚  
\vspace{0.5em}

\textbf{ä¾‹ 1ï¼ˆäºŒç»´çŸ©é˜µï¼‰}ï¼š  
è¾“å…¥å½¢çŠ¶ $(2, 3)$ï¼Œäº¤æ¢ 0ã€1 è½´ï¼š  
\[
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}
\to
\begin{bmatrix}
1 & 4 \\
2 & 5 \\
3 & 6
\end{bmatrix}
\]  

\textbf{ä¾‹ 2ï¼ˆä¸‰ç»´å¼ é‡ï¼‰}ï¼š  
è¾“å…¥å½¢çŠ¶ $(2, 3, 4)$ï¼Œäº¤æ¢ç¬¬ 1ã€2 è½´ï¼š  
åŸç´¢å¼• $(b, h, w)$ å˜ä¸º $(b, w, h)$ã€‚  
å‡è®¾ $x.shape = (2, 3, 4)$ï¼š  
\[
x.transpose(1, 2) \ \Rightarrow\ \text{shape} = (2, 4, 3)
\]  

\textbf{å…±äº«å†…å­˜}ï¼š  
è‹¥è¾“å…¥æ˜¯ \emph{strided tensor}ï¼ˆå¯†é›†å¼ é‡ï¼‰ï¼Œç»“æœä¸è¾“å…¥å…±äº«åº•å±‚å­˜å‚¨ï¼Œå› æ­¤ä¿®æ”¹å…¶ä¸­ä¸€ä¸ªä¼šå½±å“å¦ä¸€ä¸ªã€‚  
\vspace{0.3em}
è‹¥æ˜¯ç¨€ç–å¼ é‡ï¼ˆsparse tensorï¼‰ï¼Œæ˜¯å¦å…±äº«å–å†³äºå­˜å‚¨å¸ƒå±€ï¼ˆCSRã€CSC ç­‰ï¼‰ã€‚  
}

\MarginNote{
\fontsize{10pt}{12pt}\selectfont
\textbf{éè¿ç»­ (non-contiguous) å¼ é‡}ï¼š
PyTorch å¼ é‡åº•å±‚ç”± \emph{ä¸€ç»´å†…å­˜å— (storage)} åŠ  \emph{æ­¥å¹… (stride)} å†³å®šè®¿é—®æ–¹å¼ã€‚
æŸäº›æ“ä½œï¼ˆå¦‚ \texttt{transpose}ã€\texttt{permute}ï¼‰åªä¼šæ”¹å˜ strideï¼Œè€Œä¸ç§»åŠ¨å†…å­˜ã€‚
è¿™æ ·è™½ç„¶é€»è¾‘é¡ºåºæ­£ç¡®ï¼Œä½†ç‰©ç†å†…å­˜ä¸å†æŒ‰è¡Œä¼˜å…ˆç´§å¯†æ’åˆ—ã€‚

\textbf{å½±å“}ï¼š\texttt{.view()} åªèƒ½åœ¨è¿ç»­ (contiguous) å†…å­˜ä¸Šä½¿ç”¨ï¼Œéè¿ç»­å¼ é‡ä¼šæŠ¥é”™ã€‚

\textbf{è§£å†³}ï¼šç”¨ \texttt{.contiguous()} é‡æ–°æ‹·è´æ•°æ®ï¼Œä½¿å…¶åœ¨å†…å­˜ä¸­è¿ç»­å­˜å‚¨ï¼Œå† \texttt{.view()}ã€‚
}



\begin{lstlisting}[language=Python]
x = torch.tensor([[1., 2, 3], [4, 5, 6]])

# è·å–ç¬¬ 0 è¡Œ
y = x[0]
% assert torch.equal(y, torch.tensor([1., 2, 3]))
% assert same_storage(x, y)

# è·å–ç¬¬ 1 åˆ—
y = x[:, 1]
assert torch.equal(y, torch.tensor([2, 5]))
assert same_storage(x, y)

# è§†ä½œ 3x2 çŸ©é˜µ
y = x.view(3, 2)
assert torch.equal(y, torch.tensor([[1, 2], [3, 4], [5, 6]]))
assert same_storage(x, y)

# è½¬ç½®
y = x.transpose(1, 0)
assert torch.equal(y, torch.tensor([[1, 4], [2, 5], [3, 6]]))
assert same_storage(x, y)

# ä¿®æ”¹ x ä¼šå½±å“ y
x[0][0] = 100
assert y[0][0] == 100

# éè¿ç»­å¼ é‡è§†å›¾æ— æ³•ç›´æ¥ .view()
y = x.transpose(1, 0)
assert not y.is_contiguous()
try:
    y.view(2, 3)
except RuntimeError as e:
    assert "view size is not compatible" in str(e)

# å…ˆè½¬ä¸ºè¿ç»­å† .view()
y = x.transpose(1, 0).contiguous().view(2, 3)
assert not same_storage(x, y)
\end{lstlisting}
\vspace{-1em}



\subsubsection{Elementwise Operations}

é€å…ƒç´ è¿ç®—ä¼šå¯¹å¼ é‡çš„æ¯ä¸ªå…ƒç´ åº”ç”¨ç›¸åŒçš„æ“ä½œï¼Œå¹¶è¿”å›ç›¸åŒå½¢çŠ¶çš„\textbf{æ–°å¼ é‡}ã€‚

\begin{lstlisting}[language=Python]
x = torch.tensor([1, 4, 9])
assert torch.equal(x.pow(2), torch.tensor([1, 16, 81]))
assert torch.equal(x.sqrt(), torch.tensor([1, 2, 3]))
assert torch.equal(x.rsqrt(), torch.tensor([1, 1/2, 1/3])) # i -> 1/sqrt(x_i)
assert torch.equal(x + x, torch.tensor([2, 8, 18]))
assert torch.equal(x * 2, torch.tensor([2, 8, 18]))
assert torch.equal(x / 0.5, torch.tensor([2, 8, 18]))

# ä¸Šä¸‰è§’çŸ©é˜µ
x = torch.ones(3, 3).triu()
assert torch.equal(x, torch.tensor([
    [1, 1, 1],
    [0, 1, 1],
    [0, 0, 1],
]))
\end{lstlisting}

\MarginNote{
{\color{qpurple}\textbf{\texttt{torch.triu(input, diagonal=0)}}}  

è¿”å›çŸ©é˜µï¼ˆ2D å¼ é‡ï¼‰æˆ–çŸ©é˜µæ‰¹æ¬¡çš„\textbf{ä¸Šä¸‰è§’éƒ¨åˆ†}ï¼Œå…¶ä½™å…ƒç´ ç½®ä¸º $0$ã€‚  

\vspace{0.3em}
\textbf{é»˜è®¤}ï¼š\texttt{diagonal=0} ä¿ç•™ä¸»å¯¹è§’çº¿åŠå…¶ä¸Šæ–¹å…ƒç´ ã€‚  
\[
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix}
\ \xrightarrow{\ \texttt{triu}\ }\ 
\begin{bmatrix}
1 & 2 & 3 \\
0 & 5 & 6 \\
0 & 0 & 9
\end{bmatrix}
\]  

\vspace{0.3em}
\textbf{å‚æ•° diagonal}ï¼š  
\begin{itemize}
    \item \texttt{0}ï¼šä¸»å¯¹è§’çº¿åŠä»¥ä¸Š  
    \item \texttt{>0}ï¼šæ’é™¤æ›´é ä¸Šçš„å¯¹è§’çº¿ï¼ˆå‘ä¸Šç§»ï¼‰  
    \item \texttt{<0}ï¼šåŒ…å«ä¸»å¯¹è§’çº¿ä¸‹æ–¹çš„æ›´å¤šå…ƒç´ ï¼ˆå‘ä¸‹ç§»ï¼‰
\end{itemize}  

ä¾‹å¦‚ï¼š
\[
\texttt{triu(A, 1)} \Rightarrow
\begin{bmatrix}
0 & 2 & 3 \\
0 & 0 & 6 \\
0 & 0 & 0
\end{bmatrix}
\]  
}

\begin{itemize}[leftmargin=1.5em]
    \item å¸¸ç”¨åœ¨æ•°å­¦å˜æ¢ï¼ˆå¹‚ã€å¼€æ–¹ã€åŠ å‡ä¹˜é™¤ç­‰ï¼‰ã€‚
    \item \verb|triu| å¯ç”¨äºç”Ÿæˆå› æœæ³¨æ„åŠ›æ©ç ã€‚
\end{itemize}


\subsubsection{Matrix Multiplication}

æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ“ä½œä¹‹ä¸€æ˜¯çŸ©é˜µä¹˜æ³•ï¼ˆmatmulï¼‰ã€‚

\begin{lstlisting}[language=Python]
# åŸºæœ¬çŸ©é˜µä¹˜æ³•
x = torch.ones(16, 32)
w = torch.ones(32, 2)
y = x @ w
assert y.size() == torch.Size([16, 2])

# æ‰¹é‡çŸ©é˜µä¹˜æ³•
x = torch.ones(4, 8, 16, 32)
w = torch.ones(32, 2)
y = x @ w
assert y.size() == torch.Size([4, 8, 16, 2])
\end{lstlisting}

\MarginNote{
{\color{qpurple}\textbf{\texttt{x @ w} æ‰¹é‡çŸ©é˜µä¹˜æ³•}}  

è¾“å…¥ï¼š  
\texttt{x.shape = [4, 8, 16, 32]}  
\texttt{w.shape = [32, 2]}  

\vspace{0.3em}
\textbf{è§„åˆ™}ï¼š  
\begin{itemize}
  \item å‰ä¸¤ç»´ \texttt{[4, 8]} ä½œä¸ºæ‰¹æ¬¡ç»´åº¦ä¿ç•™  
  \item æ¯ä¸ª \texttt{[16, 32]} å­çŸ©é˜µä¸ \texttt{[32, 2]} ç›¸ä¹˜  
  \item å¾—åˆ° \texttt{[16, 2]}ï¼Œå†æ‹¼å›æ‰¹æ¬¡ç»´åº¦ $\Rightarrow$ \texttt{[4, 8, 16, 2]}
\end{itemize}

\vspace{0.3em}
In this case, we iterate over values of the first 2 dimensions of x and multiply by w.
}

\begin{itemize}[leftmargin=1.5em]
    \item æ”¯æŒæ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼Œè‡ªåŠ¨åœ¨æ‰¹æ¬¡ç»´ä¸Šå¹¿æ’­ã€‚
    \item è®¡ç®—æ—¶ä¼šåœ¨å‰å‡ ä¸ªç»´åº¦è¿­ä»£è¿›è¡Œä¹˜æ³•ã€‚
\end{itemize}

\clearpage
\subsection{Tensor Einops}
Einops æ˜¯ä¸€ä¸ªç”¨äº\textit{æŒ‰åå­—æ“ä½œç»´åº¦}çš„åº“ï¼Œå®˜æ–¹æ•™ç¨‹ï¼š\\
\url{https://einops.rocks/1-einops-basics/}

\subsubsection{Einops: Motivation}

ä¼ ç»Ÿå†™æ³•å®¹æ˜“ \emph{æé”™ç»´åº¦}ï¼ˆä¾‹å¦‚ \verb|-2, -1| åˆ°åº•æ˜¯å“ªä¸¤ç»´ï¼‰ï¼š
\begin{lstlisting}[language=Python]
# Traditional PyTorch code
x = torch.ones(2, 2, 3)  # batch, sequence, hidden
y = torch.ones(2, 2, 3)  # batch, sequence, hidden
z = x @ y.transpose(-2, -1)  # batch, sequence, sequence
\end{lstlisting}




\subsubsection{Jaxtyping: Basics}

ç”¨ \texttt{jaxtyping} ç»™å¼ é‡\textbf{æ ‡æ³¨ç»´åº¦å}ï¼ˆç±»å‹æ³¨è§£ï¼Œ\emph{æ–‡æ¡£åŒ–ä¸ºä¸»}ï¼‰ï¼š
\begin{lstlisting}[language=Python]
# Old way
x = torch.ones(2, 2, 1, 3)  # batch seq heads hidden

# New (jaxtyping) way
from jaxtyping import Float
x: Float[torch.Tensor, "batch seq heads hidden"] = torch.ones(2, 2, 1, 3)
\end{lstlisting}

\subsubsection{Einsum = Named MatMul with Bookkeeping}

\begin{lstlisting}[language=Python]
from jaxtyping import Float
from einops import einsum

# Define two tensors:
x: Float[torch.Tensor, "batch seq1 hidden"] = torch.ones(2, 3, 4)
y: Float[torch.Tensor, "batch seq2 hidden"] = torch.ones(2, 3, 4)

# Old way
z = x @ y.transpose(-2, -1)  # -> [batch, seq1, seq2]

# New way (einops)
z = einsum(x, y, "batch seq1 hidden, batch seq2 hidden -> batch seq1 seq2")
# Dimensions that are not named in the output are summed over.


# ä½¿ç”¨ ... è¡¨ç¤ºâ€œå…¶ä»–ä»»æ„æ‰¹ç»´â€
z = einsum(x, y, "... seq1 hidden, ... seq2 hidden -> ... seq1 seq2")
\end{lstlisting}

\clearpage
\subsubsection{Einops: Reduce (èšåˆ)}

\begin{lstlisting}[language=Python]
from einops import reduce
from jaxtyping import Float

x: Float[torch.Tensor, "batch seq hidden"] = torch.ones(2, 3, 4)

# Old way
y = x.mean(dim=-1)  # -> [batch, seq]

# New way (einops)
y = reduce(x, "... hidden -> ...", "mean")  # ä¹Ÿå¯ "sum"/"max"/"min"
\end{lstlisting}

\subsubsection{Einops: Rearrange + Einsumï¼ˆå±•å¼€/é‡æ’ + çº¿æ€§å˜æ¢ï¼‰}
Sometimes, a dimension represents two dimensions
...and you want to operate on one of them.

\begin{lstlisting}[language=Python]
from einops import rearrange, einsum
from jaxtyping import Float
import torch

# total_hidden = heads * hidden1 = 2 * 4 = 8
x: Float[torch.Tensor, "batch seq total_hidden"] = torch.ones(2, 3, 8)
w: Float[torch.Tensor, "hidden1 hidden2"] = torch.ones(4, 4)

# 1) æ‹†åˆ† total_hidden ç»´åº¦
#   (heads hidden1) è¡¨ç¤º total_hidden æ˜¯ç”± heads å’Œ hidden1 ä¸¤ä¸ªç»´åº¦æ‹¼æ¥çš„
#   è¿™é‡Œ heads=2ï¼Œå› æ­¤ hidden1 è‡ªåŠ¨æ¨æ–­ä¸º 4
x = rearrange(x, "... (heads hidden1) -> ... heads hidden1", heads=2)
# ç°åœ¨å½¢çŠ¶ä¸º [batch=2, seq=3, heads=2, hidden1=4]

# 2) åœ¨ hidden1 ç»´åº¦ä¸Šåšçº¿æ€§å˜æ¢ï¼Œæ˜ å°„åˆ° hidden2
# Perform the transformation by w:
# einsum è§„åˆ™ï¼š
#     "... hidden1, hidden1 hidden2 -> ... hidden2"
#     å…¶ä¸­ hidden1 æ˜¯æ±‚å’Œæ¶ˆæ‰çš„ç»´åº¦ï¼Œå…¶å®ƒæ‰¹æ¬¡ç»´ (...) ä¿æŒä¸å˜
x = einsum(x, w, "... hidden1, hidden1 hidden2 -> ... hidden2")
# ç°åœ¨å½¢çŠ¶ä¸º [2, 3, 2, hidden2=4]

# 3) å°† heads ä¸ hidden2 åˆå¹¶å›å•ä¸ªç»´åº¦
x = rearrange(x, "... heads hidden2 -> ... (heads hidden2)")
# æœ€ç»ˆå½¢çŠ¶ä¸º [2, 3, 8]

\end{lstlisting}





\clearpage
\subsection{Tensor Operations Flops}

A \textbf{floating-point operation} (FLOP) æŒ‡ä¸€æ¬¡åŠ æ³• (\(x + y\)) æˆ–ä¹˜æ³• (\(x \times y\)) ç­‰åŸºæœ¬è¿ç®—ã€‚

\begin{itemize}
    \item \textbf{FLOPs}ï¼šæ€»çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼ˆè¡¡é‡è®¡ç®—é‡ï¼‰
    \item \textbf{FLOP/s} æˆ– \textbf{FLOPS}ï¼šæ¯ç§’å¯æ‰§è¡Œçš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼ˆè¡¡é‡ç¡¬ä»¶é€Ÿåº¦ï¼‰
\end{itemize}
è¯¦ç»†çš„å®šä¹‰å‚è€ƒï¼š~\ref{def:FLOPs-FLOPS}.

\Example{ç›´è§‚é‡çº§ç¤ºä¾‹}{
\vspace{-1em}
FLOPs:
\begin{itemize}
    \item GPT-3 (2020) è®­ç»ƒæ€» FLOPsï¼š\(\approx 3.14\times 10^{23}\)
    \item GPT-4 (2023) è®­ç»ƒæ€» FLOPsï¼ˆæ¨æµ‹ï¼‰ï¼š\(\approx 2\times 10^{25}\)
\end{itemize}
FLOP/s:
\begin{itemize}
    \item A100ï¼š\(312\ \text{TFLOP/s} \quad (312\times 10^{12})\)
    \item H100ï¼ˆç¨€ç–ï¼‰ï¼š\(1979\ \text{TFLOP/s}\)ï¼Œç¨ å¯†æ—¶å‡åŠ
\end{itemize}
}


\paragraph{çŸ©é˜µä¹˜æ³•çš„ FLOPs}~{}

å‡è®¾ä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼š  
è¾“å…¥ä¸º \(B\) ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ç»´åº¦ä¸º \(D\)ï¼Œæ˜ å°„åˆ° \(K\) ä¸ªè¾“å‡ºï¼š
\[
Y = X W,\quad X\in\mathbb{R}^{B\times D},\quad W\in\mathbb{R}^{D\times K}
\]

We have one multiplication (x[i][j] * w[j][k]) and one addition per (i, j, k) triple.
\[
\text{FLOPs} = 2 \times B \times D \times K
\]

\paragraph{å…¶ä»–å¸¸è§æ“ä½œçš„ FLOPs}~{}

\begin{itemize}
    \item å…ƒç´ çº§æ“ä½œï¼ˆå¦‚å¯¹\(m\times n\) çŸ©é˜µè¿›è¡Œå…ƒç´ åŠ æ³•ï¼‰ï¼š\(O(mn)\) FLOPs
    \item ä¸¤ä¸ª \(m\times n\) çŸ©é˜µç›¸åŠ ï¼š\(mn\)  FLOPs
    \item å¯¹äºè¶³å¤Ÿå¤§çš„çŸ©é˜µï¼ŒçŸ©é˜µä¹˜æ³•çš„è®¡ç®—é‡è¿œè¶…å…¶ä»–æ“ä½œ
\end{itemize}

\paragraph{Transformer çš„è¿‘ä¼¼ FLOPs}~{}

å¯¹äº Transformerï¼Œå‰å‘ä¼ æ’­ FLOPsï¼ˆé¦–é˜¶è¿‘ä¼¼ï¼‰ï¼š
\[
\text{FLOPs} \approx 2\times (\#\text{tokens})\times (\#\text{parameters})
\]

% \clearpage
\paragraph{Model FLOPs Utilization (MFU)}~{}
\MarginNote{
{\color{dblue}\textbf{å®é™… FLOP/s}}\\
1. è®¡ç®—æ€» FLOPsï¼ˆä¾‹å¦‚çŸ©é˜µä¹˜æ³• $B\times D \ @\ D\times K$ï¼‰\\
2. è®°å½•è¿ç®—è€—æ—¶ï¼ˆç§’ï¼‰\\
3. ç”¨ $\text{æ€» FLOPs} \ /\ \text{è€—æ—¶}$ å¾—åˆ°å®é™… FLOP/s
}

\MarginNote{
{\color{dblue}\textbf{ç¡¬ä»¶å³°å€¼ FLOP/s}}\\
- æ¥è‡ª GPU å‚å•†è§„æ ¼è¡¨ï¼ˆspec sheetï¼‰\\
- ä¸åŒæ•°æ®ç±»å‹å³°å€¼ä¸åŒï¼ˆFP32, bfloat16, FP8ï¼‰\\
- ä»£ç ä¸­å¯ç”¨ \texttt{get\_promised\_flop\_per\_sec()} æŸ¥è¯¢
}

\Definition{Model FLOPs Utilization (MFU)\label{def:mfu}}{
\[
\text{MFU} = \frac{\text{å®é™… FLOP/s}}{\text{ç¡¬ä»¶å³°å€¼ FLOP/s}}
\]
\textbf{è¯´æ˜}ï¼š
\begin{itemize}
    \item MFU $\ge 0.5$ é€šå¸¸è¡¨ç°å¾ˆå¥½ï¼ˆå°¤å…¶å½“çŸ©é˜µä¹˜æ³•å ä¸»è¦è®¡ç®—æ—¶ï¼‰ã€‚
    \item Data Typeå½±å“æ˜¾è‘—:bfloat16 é€šå¸¸æ¯” float32 æ‹¥æœ‰æ›´é«˜çš„å®é™… FLOP/sã€‚
\end{itemize}
}

\clearpage
\subsection{Gradients Basics}

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»æ„å»ºäº†å¼ é‡ï¼ˆå¯¹åº”å‚æ•°æˆ–æ•°æ®ï¼‰å¹¶å®Œæˆäº†å‰å‘è®¡ç®—ï¼ˆforwardï¼‰ã€‚  
ç°åœ¨ï¼Œæˆ‘ä»¬è¦è®¡ç®—æ¢¯åº¦ï¼ˆbackwardï¼‰ã€‚  

\paragraph{ç¤ºä¾‹ï¼šç®€å•çº¿æ€§æ¨¡å‹}  
\[
y = 0.5 \cdot (x \cdot w - 5)^2
\]
- \(\mathbf{x}\)ï¼šè¾“å…¥æ•°æ®  \\
- \(\mathbf{w}\)ï¼šéœ€è¦è®¡ç®—æ¢¯åº¦çš„å‚æ•°


\begin{lstlisting}[language=Python]
import torch

# è¾“å…¥æ•°æ®
x = torch.tensor([1., 2, 3])
# å‚æ•° wï¼Œè®¾ç½® requires_grad=True æ‰ä¼šè¿½è¸ªæ¢¯åº¦
w = torch.tensor([1., 1, 1], requires_grad=True)

# å‰å‘è®¡ç®— (forward)
pred_y = x @ w
loss = 0.5 * (pred_y - 5).pow(2)

# åå‘ä¼ æ’­ (backward)
loss.backward()

# æ£€æŸ¥æ¢¯åº¦
assert loss.grad is None       # æ ‡é‡ä¸å­˜å‚¨æ¢¯åº¦
assert pred_y.grad is None     # ä¸­é—´å˜é‡ä¸å­˜å‚¨æ¢¯åº¦
assert x.grad is None          # é requires_grad å¼ é‡ä¸å­˜å‚¨æ¢¯åº¦
assert torch.equal(w.grad, torch.tensor([1, 2, 3]))
\end{lstlisting}

\paragraph{è¦ç‚¹æ€»ç»“}
\begin{itemize}
    \item åªæœ‰ \verb|requires_grad = True| çš„å¼ é‡ä¼šåœ¨è®¡ç®—å›¾ä¸­è¢«è¿½è¸ªå¹¶å­˜å‚¨æ¢¯åº¦
    \item \verb|loss.backward()| ä¼šæ²¿è®¡ç®—å›¾åå‘ä¼ æ’­æ¢¯åº¦
    \item ä¸­é—´ç»“æœï¼ˆå¦‚ \verb|pred_y|ï¼‰é»˜è®¤ä¸ä¿ç•™æ¢¯åº¦
    \item æœ¬ä¾‹ä¸­ \(\nabla_w \ \text{loss} = [1, 2, 3]\)
\end{itemize}

\clearpage
\subsection{Gradients Flops}
åœ¨è®¡ç®—æ¢¯åº¦ï¼ˆåå‘ä¼ æ’­ï¼‰æ—¶ï¼ŒFLOPs æ•°é‡é€šå¸¸æ¯”å‰å‘ä¼ æ’­å¤šã€‚  
æˆ‘ä»¬ä»¥ä¸€ä¸ªä¸¤å±‚çº¿æ€§æ¨¡å‹ä¸ºä¾‹ï¼š  
\[
x \cdot w_1 \;\rightarrow\; h_1,\quad
h_1 \cdot w_2 \;\rightarrow\; h_2,\quad
\mathrm{Loss} = \ell(h_2)
\]
- $B$ï¼šBatch size\\
- $D$ï¼šéšè—å±‚ç»´åº¦\\
- $K$ï¼šè¾“å‡ºç»´åº¦

\begin{lstlisting}[language=Python]
device = get_device()
x  = torch.ones(B, D, device=device)
w1 = torch.randn(D, D, device=device, requires_grad=True)
w2 = torch.randn(D, K, device=device, requires_grad=True)

# Forward pass
h1   = x @ w1
h2   = h1 @ w2
loss = h2.pow(2).mean()
\end{lstlisting}

\paragraph{Forward FLOPs}~{}
\\
- $x @ w_1$ï¼š$2 B D D$ FLOPs \\
- $h_1 @ w_2$ï¼š$2 B D K$ FLOPs \\
\[
\text{num\_forward\_flops} = 2BD^2 + 2BDK
\]

\paragraph{Backward FLOPs (ç¤ºä¾‹ï¼š$w_2$)}~{}
\\
1.~è®¡ç®— $w_2$ çš„æ¢¯åº¦ï¼š
\[
\frac{\partial \mathcal{L}}{\partial W_2}
= H_1^\top \frac{\partial \mathcal{L}}{\partial H_2}
\quad\Rightarrow\quad 2 B D K \ \text{FLOPs}
\]

2.~è®¡ç®— $h_1$ çš„æ¢¯åº¦ï¼š
\[
\frac{\partial \mathcal{L}}{\partial H_1}
= \frac{\partial \mathcal{L}}{\partial H_2} W_2^\top
\quad\Rightarrow\quad 2 B D K \ \text{FLOPs}
\]
è¿™é‡Œ $H_1 \in \mathbb{R}^{B \times D}$ï¼Œ$W_2 \in \mathbb{R}^{D \times K}$ã€‚



\paragraph{Backward FLOPsï¼ˆç¤ºä¾‹ï¼š$w_1$ï¼‰}~{}

1.~è®¡ç®— $w_1$ çš„æ¢¯åº¦ï¼š
\[
\frac{\partial \mathcal{L}}{\partial W_1}
= X^\top \frac{\partial \mathcal{L}}{\partial H_1}
\quad\Rightarrow\quad 2 B D^2 \ \text{FLOPs}
\]

2.~è®¡ç®— $x$ çš„æ¢¯åº¦ï¼ˆæ­¤å¤„å¯çœç•¥ï¼‰ï¼š
\[
\frac{\partial \mathcal{L}}{\partial X}
= \frac{\partial \mathcal{L}}{\partial H_1} W_1^\top
\quad\Rightarrow\quad 2 B D^2 \ \text{FLOPs}
\]
è¿™é‡Œ $X \in \mathbb{R}^{B \times D}$ï¼Œ$W_1 \in \mathbb{R}^{D \times D}$ã€‚


 \paragraph{æ€» FLOPs å…¬å¼}
\[
\#\text{data} = B,
\quad \#\text{params} = D \times D \ (\text{$W_1$}) + D \times K \ (\text{$W_2$})
\]
\[
\text{Forward} = 2B \,[\,D^2 + D K\,] = 2 \ \text{(\# data points) (\# parameters) FLOPs}
\]
\[
\text{Backward} = 4B \,[\,D^2 + D K\,] = 4 \ \text{(\# data points) (\# parameters) FLOPs}
\]
\[
\text{Total} = 6B \,[\,D^2 + D K\,] = 6 \ \text{(\# data points) (\# parameters) FLOPs}
\]





\clearpage
{\chaptoc\noindent\begin{minipage}[inner sep=0,outer sep=0]{0.9\linewidth}\section{Models}
\end{minipage}}
\subsection{Module Parameters \& Parameters Initialization}~{}
\Remark{
In PyTorch, model parameters are stored as \texttt{nn.Parameter} objects, 
which behave like tensors but are automatically registered in \texttt{model.parameters()} 
and participate in gradient updates.
}

\paragraph{Parameter Initialization}~{}

\begin{lstlisting}[language=Python]
import torch, torch.nn as nn
import numpy as np

# Example: single weight matrix
input_dim, output_dim = 16384, 32
w = nn.Parameter(torch.randn(input_dim, output_dim))
assert isinstance(w, torch.Tensor)
assert type(w.data) == torch.Tensor  # Access underlying tensor
\end{lstlisting}

If $x \in \mathbb{R}^{\text{input\_dim}}$ and 
$w \in \mathbb{R}^{\text{input\_dim} \times \text{output\_dim}}$ 
are initialized with standard normal entries, 
each element of the output $y = x^\top w$ scales as:
\[
\mathbb{E}[y^2] \propto \text{input\_dim}
\]
This causes large variance for large \text{input\_dim}, 
leading to gradient explosion.

To make variance invariant to \text{input\_dim}, 
we scale by $1/\sqrt{\text{input\_dim}}$:
\[
w_{ij} \sim \mathcal{N}\left(0, \frac{1}{\text{input\_dim}}\right)
\]
This is the core of \textbf{\href{https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}{Xavier initialization}}.

\begin{lstlisting}[language=Python]
# Rescale by 1/sqrt(input_dim)
w = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim))

# Safer: truncate to [-3, 3] to avoid outliers
w = nn.Parameter(nn.init.trunc_normal_(
    torch.empty(input_dim, output_dim),
    std=1 / np.sqrt(input_dim),
    a=-3, b=3
))
\end{lstlisting}

\clearpage
\subsection{Custom Deep Linear Model}~{}

To better understand how parameters are defined and counted in PyTorch, we will build a simple deep linear network \emph{from scratch} using \texttt{nn.Parameter}.  
This also allows us to directly connect model structure with parameter count and FLOPs calculations.

\paragraph{Design idea.}~{}  
We consider a model of dimension $D$ and $L$ hidden layers:
\begin{itemize}
    \item Each hidden layer is a simple linear transformation $W \in \mathbb{R}^{D \times D}$ without bias.
    \item The final layer maps from $\mathbb{R}^D$ to $\mathbb{R}^1$.
    \item We initialize weights with a scaled Gaussian $\mathcal{N}(0, \frac{1}{\sqrt{\text{input\_dim}}})$ to keep the output variance independent of input dimension (Xavier-like initialization).
\end{itemize}

\paragraph{Code implementation.}~{}
\begin{lstlisting}[language=Python]
class Linear(nn.Module):
    """Simple linear layer without bias."""
    def __init__(self, input_dim: int, output_dim: int):
        super().__init__()
        # Xavier-like initialization
        self.weight = nn.Parameter(
                      torch.randn(input_dim, output_dim) / 
                      np.sqrt(input_dim)
        )
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x @ self.weight

class Cruncher(nn.Module):
    """Deep linear stack of L layers + final head."""
    def __init__(self, dim: int, num_layers: int):
        super().__init__()
        # L hidden layers
        self.layers = nn.ModuleList([
            Linear(dim, dim) for _ in range(num_layers)
        ])
        # Output head
        self.final = Linear(dim, 1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for layer in self.layers:
            x = layer(x)
        x = self.final(x)
        return x.squeeze(-1)
\end{lstlisting}

\MarginNote{
{\color{qpurple}\textbf{\texttt{torch.numel}}} \\
\texttt{torch.numel(input: Tensor) $\to$ int} \\
Returns the total number of elements in the input tensor. \\
\textbf{Parameters:} \texttt{input (Tensor)} â€“ the input tensor. \\
\textbf{Example:} \\
\lstinline|a = torch.randn(1, 2, 3, 4, 5); torch.numel(a)  # 120| \\
\lstinline|a = torch.zeros(4,4); torch.numel(a)  # 16|
}



\paragraph{Example: building and checking the model.}~{}
% Inspect parameter sizes
\begin{lstlisting}[language=Python]
param_sizes = [(n, p.numel()) for n, p in model.state_dict().items()]
assert param_sizes == [
    ("layers.0.weight", D * D),
    ("layers.1.weight", D * D),
    ("final.weight", D),
]


# Count total parameters
def get_num_parameters(model: nn.Module) -> int:
    return sum(param.numel() for param in model.parameters())

num_parameters = get_num_parameters(model)
assert num_parameters == (D * D) + (D * D) + D
\end{lstlisting}

    
\paragraph{Parameter count formula.}~{}

For a $D$-dimensional model with $L$ hidden layers:
\[
\#\text{params} = L \cdot (D \times D) + (D \times 1)
\]


\paragraph{Connecting to FLOPs.}~{}
If each parameter contributes 2 FLOPs in forward pass and 4 FLOPs in backward pass,  
total computation per batch ($B$ data points) is:
\[
\text{FLOPs}_{\text{total}} 
= 6B \cdot \#\text{params} 
= 6B \,[\,L D^2 + D\,]
\]
This formula directly matches the style used earlier in the lecture.

\paragraph{Mini workflow (from build to one training step)}
\begin{enumerate}
    \item \textbf{Build} the model with dimension $D$ and number of layers $L$.
    \item \textbf{Inspect} per-parameter sizes via \texttt{state\_dict()}.
    \item \textbf{Count} total parameters with \texttt{get\_num\_parameters}.
    \item \textbf{Move} the model (and data) to GPU with \texttt{.to(device)}.
    \item \textbf{Prepare} a batch $x \in \mathbb{R}^{B \times D}$.
    \item \textbf{Forward}: compute $y = \texttt{model}(x)$.
    \item \textbf{Loss \& Backward}: compute loss, call \texttt{loss.backward()}.
    \item \textbf{Update}: call \texttt{optimizer.step()} and \texttt{optimizer.zero\_grad()}.
\end{enumerate}


\clearpage
{\chaptoc\noindent\begin{minipage}[inner sep=0,outer sep=0]{0.9\linewidth}\section{Training Loop And Best Practices}
\end{minipage}}

Randomness appears in many aspects of training deep learning models:
\begin{itemize}
    \item \textbf{Parameter initialization} â€” random weight values at the start.
    \item \textbf{Dropout masks} â€” different neurons are dropped each time.
    \item \textbf{Data ordering} â€” shuffling changes the sequence of samples.
\end{itemize}
\Remark{
For reproducibility (especially during debugging), set a \emph{random seed} in all relevant libraries.  
}

Here is a recommended pattern:

\begin{lstlisting}[language=Python]
# Torch
seed = 0
torch.manual_seed(seed)

# NumPy
import numpy as np
np.random.seed(seed)

# Python's built-in RNG
import random
random.seed(seed)
\end{lstlisting}

\noindent
Setting seeds across all libraries ensures that experiments can be repeated exactly, which is crucial for isolating bugs.

\subsection{Data Loading for Language Modeling}
In language modeling, the dataset is typically a long sequence of integers produced by the tokenizer.  

We can serialize it using NumPy and store on disk:
\begin{lstlisting}[language=Python]
orig_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.int32)
orig_data.tofile("data.npy")
\end{lstlisting}

\MarginNote{
{\color{qpurple}\textbf{\texttt{numpy.memmap}}} \\
\texttt{class numpy.memmap(filename, dtype, mode, ...)} \\
\small
Creates a \textbf{memory-mapped array} stored in a binary file on disk.  

Used to access small parts of a large file \emph{without loading the whole file into memory}.  
Behaves like a NumPy array but data is read/written lazily.

\textbf{Common \texttt{mode}:}
\begin{itemize}
    \item \texttt{'r'}: Read-only
    \item \texttt{'r+'}: Read/write (default)
    \item \texttt{'w+'}: Create or overwrite
    \item \texttt{'c'}: Copy-on-write (no save to disk)
\end{itemize}
}

When loading, for huge datasets (e.g., LLaMA's 2.8TB corpus), we avoid reading the entire file into RAM.  
Instead, use \texttt{np.memmap} to lazily load only the required parts:
\begin{lstlisting}[language=Python]
data = np.memmap("data.npy", dtype=np.int32)
assert np.array_equal(data, orig_data)
\end{lstlisting}

\clearpage
\subsection{Batch Sampling Utility}

We implement a minimal batch sampler that extracts $B$ sub-sequences of length $L$ from a long token stream (e.g., memmapped integers). It samples random start positions, slices contiguous spans, and returns a tensor of shape $[B, L]$.

\begin{lstlisting}[language=Python]
def get_batch(data: np.array, batch_size: int, sequence_length: int, device: str) -> torch.Tensor:
    # 1) Sample random start indices in [0, len(data) - sequence_length)
    start_indices = torch.randint(len(data) - sequence_length, (batch_size,))
    assert start_indices.size() == torch.Size([batch_size])

    # 2) Slice contiguous spans [start : start + sequence_length)
    #    Works with np.ndarray and np.memmap alike.
    x = torch.tensor([data[start:start + sequence_length] for start in start_indices])

    # 3) Sanity check on shape: [B, L]
    assert x.size() == torch.Size([batch_size, sequence_length])
    return x.to(device)
\end{lstlisting}

\paragraph{What this does (step-by-step).}
\begin{itemize}
  \item \textbf{Random sampling.} Draw $B$ start positions uniformly from 
        $\{0,\dots,|data|-L\}$, so each subsequence has length $L$.
  \item \textbf{Slicing.} For each start $s$, take a contiguous span 
        \(\big[data[s : s+L]\big]\). This is cache-friendly and works with both
        \texttt{np.ndarray} and \texttt{np.memmap}.
  \item \textbf{Batching.} Stack the $B$ slices into a tensor of shape \([B, L]\).
  \item \textbf{Device move.} Return the batch on \texttt{device} (CPU or GPU).
\end{itemize}

\paragraph{Usage.}~{}
A data loader then extracts training batches:
\begin{lstlisting}[language=Python]
B = 2  # Batch size
L = 4  # Sequence length
x = get_batch(data, batch_size=B, sequence_length=L, device=get_device())
assert x.size() == torch.Size([B, L])
\end{lstlisting}

\noindent
This approach minimizes memory usage and speeds up training when working with massive datasets, 
especially when \texttt{data} is a large memmapped array. It avoids loading the entire dataset into RAM and only slices the needed spans.

\paragraph{Notes and practical tips.}
\begin{itemize}
  \item \textbf{Bounds.} Ensure \(\,|data| \ge L+1\,\) so that sampling is well-defined.
  \item \textbf{Dtype.} For tokenized corpora, \texttt{int32} is typical; cast as needed before converting to a tensor.
  \item \textbf{Pinned memory (optional).} If you fetch on CPU and then copy to GPU, consider 
        \texttt{x = x.pin\_memory(); x.to(device, non\_blocking=True)} for faster async H2D copies.
  \item \textbf{Reproducibility.} Set seeds for \texttt{torch}, \texttt{numpy}, and \texttt{random} when you need deterministic sampling.
  \item \textbf{Overlapping vs. non-overlapping.} Random starts can overlap; if you want non-overlapping windows, stride your starts accordingly.
  \item \textbf{Complexity.} Time and memory scale as \(O(BL)\) for each batch; slicing from \texttt{np.memmap} keeps peak RAM low.
\end{itemize}

\paragraph{Pinned memory for faster GPU transfer.}~{}

é»˜è®¤æƒ…å†µä¸‹ï¼ŒCPU ä¸Šçš„å¼ é‡å­˜å‚¨åœ¨ \emph{paged memory}ï¼ˆåˆ†é¡µå†…å­˜ï¼‰ä¸­ï¼Œè¿™ç§å†…å­˜é¡µåœ¨æ‹·è´åˆ° GPU æ—¶éœ€è¦é˜»å¡ç­‰å¾…å®Œæˆã€‚  
å¦‚æœæ•°æ®åœ¨ CPU ä¸Šå‡†å¤‡å¥½åï¼Œè¿˜éœ€è¦ä¼ è¾“åˆ° GPUï¼Œå¯ä»¥æ˜¾å¼å°†å…¶è½¬ä¸º \emph{pinned memory}ï¼ˆé¡µé”å®šå†…å­˜ï¼‰ï¼š

\begin{lstlisting}[language=Python]
if torch.cuda.is_available():
    x = x.pin_memory()  # é¡µé”å®šï¼Œå…è®¸å¼‚æ­¥ H2D æ‹·è´
x = x.to(device, non_blocking=True)
\end{lstlisting}

\noindent
è¿™æ ·åšçš„å¥½å¤„ï¼š
\begin{itemize}
  \item \textbf{å¼‚æ­¥ä¼ è¾“ï¼ˆAsynchronous transferï¼‰ï¼š} é…åˆ \texttt{non\_blocking=True}ï¼Œå¯åœ¨ GPU è®­ç»ƒå½“å‰æ‰¹æ¬¡æ—¶ï¼ŒCPU åŒæ—¶å‡†å¤‡å¹¶æ‹·è´ä¸‹ä¸€æ‰¹æ•°æ®ã€‚
  \item \textbf{éšè—æ•°æ®åŠ è½½å»¶è¿Ÿï¼š} æ•°æ®åŠ è½½ä¸è®¡ç®—é‡å ï¼Œå‡å°‘ GPU ç©ºé—²æ—¶é—´ã€‚
  \item \textbf{é€‚ç”¨åœºæ™¯ï¼š} å¤§æ‰¹é‡è®­ç»ƒã€æ•°æ®é¢„å¤„ç†å¤æ‚ã€æˆ–æ•°æ®æºä¸º \texttt{np.memmap} ç­‰æ…¢é€Ÿ I/O æ—¶ã€‚
\end{itemize}

\paragraph{Pipeline overlap.}~{}
å…¸å‹çš„æµæ°´çº¿è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹ï¼š
\begin{enumerate}
  \item CPU é‡‡æ ·å¹¶åˆ‡ç‰‡ä¸‹ä¸€æ‰¹æ•°æ®ã€‚
  \item å°†æ•°æ® pin åˆ°å†…å­˜å¹¶å¼‚æ­¥ä¼ è¾“åˆ° GPUã€‚
  \item GPU ç»§ç»­è®¡ç®—å½“å‰æ‰¹æ¬¡çš„å‰å‘ä¸åå‘ä¼ æ’­ã€‚
\end{enumerate}
è¿™ç§ CPU$\leftrightarrow$GPU çš„å¹¶è¡Œå¯ä»¥æ˜¾è‘—æå‡ååé‡ï¼Œå°¤å…¶æ˜¯åœ¨ I/O å’Œé¢„å¤„ç†æˆæœ¬è¾ƒé«˜çš„ä»»åŠ¡ä¸­ã€‚
 
\clearpage
\subsection{Optimizer}

\paragraph{Recap and relationships.}~{}

å›é¡¾æˆ‘ä»¬ä¹‹å‰çš„æ¨¡å‹ï¼š
\begin{lstlisting}[language=Python]
B = 2
D = 4
num_layers = 2
model = Cruncher(dim=D, num_layers=num_layers).to(get_device())
\end{lstlisting}

å¸¸è§ä¼˜åŒ–å™¨çš„å…³ç³»å¯ä»¥ç®€åŒ–ä¸ºï¼š
\begin{itemize}
  \item \textbf{Momentum:} SGD + æ¢¯åº¦çš„æŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼ˆexponential averaging of gradï¼‰ã€‚
  \item \textbf{AdaGrad:} SGD + æŒ‰æ¢¯åº¦å¹³æ–¹çš„ç´¯ç§¯å¹³å‡åšç¼©æ”¾ï¼ˆaveraging by grad$^2$ï¼‰ã€‚
  \item \textbf{RMSProp:} AdaGrad + æŒ‡æ•°æ»‘åŠ¨å¹³å‡æ¢¯åº¦å¹³æ–¹ï¼ˆexponentially averaging of grad$^2$ï¼‰ã€‚
  \item \textbf{Adam:} RMSProp + Momentumï¼ˆåŒæ—¶è·Ÿè¸ªæ¢¯åº¦çš„ä¸€é˜¶ä¸äºŒé˜¶çŸ©ï¼‰ã€‚
\end{itemize}

\paragraph{Code: AdaGrad on our model.}
\begin{lstlisting}[language=Python]
# Define AdaGrad optimizer
optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)

# Forward pass
x = torch.randn(B, D, device=get_device())
y = torch.tensor([4., 5.], device=get_device())
pred_y = model(x)
loss = F.mse_loss(input=pred_y, target=y)

# Backward pass
loss.backward()

# Update parameters
optimizer.step()

# Optional: free memory
optimizer.zero_grad(set_to_none=True)
\end{lstlisting}

\noindent
è¿™é‡Œçš„ \texttt{optimizer.zero\_grad(set\_to\_none=True)} ä¼šå°†æ¢¯åº¦å¼•ç”¨è®¾ä¸º \texttt{None}ï¼Œæœ‰åŠ©äºé‡Šæ”¾æ˜¾å­˜ï¼Œå¹¶åœ¨ä¸‹æ¬¡åå‘ä¼ æ’­æ—¶å‡å°‘åˆ†é…å¼€é”€ã€‚

\newpage
\Definition{å¸¸è§ä¼˜åŒ–å™¨çš„æ•°å­¦å…¬å¼}{
AdaGrad åŸè®ºæ–‡å¯å‚è€ƒï¼š\href{https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf}{Duchi et al., 2011}ã€‚
è®¾ï¼š
\begin{itemize}
  \item $w_t$: ç¬¬ $t$ æ¬¡è¿­ä»£æ—¶çš„æ¨¡å‹å‚æ•°ï¼ˆå‘é‡æˆ–çŸ©é˜µï¼‰ã€‚
  \item $g_t = \nabla_w L(w_t)$: å½“å‰æ¢¯åº¦ï¼Œæ¥è‡ªæŸå¤±å‡½æ•° $L$ å¯¹å‚æ•°çš„åå¯¼ã€‚
  \item $\eta$: å­¦ä¹ ç‡ï¼ˆlearning rateï¼‰ï¼Œæ§åˆ¶æ¯æ­¥æ›´æ–°å¹…åº¦ã€‚
  \item $\beta$: åŠ¨é‡è¡°å‡ç³»æ•°ï¼Œ$0 < \beta < 1$ã€‚
  \item $\beta_1, \beta_2$: Adam ä¸­ä¸€é˜¶ã€äºŒé˜¶çŸ©ä¼°è®¡çš„è¡°å‡ç³»æ•°ã€‚
  \item $\epsilon$: é˜²æ­¢é™¤é›¶çš„å¾®å°å¸¸æ•°ï¼ˆé€šå¸¸ $10^{-8}$ï¼‰ã€‚
\end{itemize}

\noindent
å¸¸è§ä¼˜åŒ–å™¨å…¬å¼å¦‚ä¸‹ï¼š

\begin{itemize}
  \item \textbf{SGD}ï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰  
    \[
    w_{t+1} = w_t - \eta \, g_t
    \]  
    è§£é‡Šï¼šç›´æ¥ç”¨å½“å‰æ¢¯åº¦çš„è´Ÿæ–¹å‘æ›´æ–°å‚æ•°ï¼Œç›¸å½“äºâ€œä¸‹å¡èµ°ä¸€æ­¥â€ã€‚ä¼˜ç‚¹æ˜¯ç®€å•é«˜æ•ˆï¼Œç¼ºç‚¹æ˜¯å¯èƒ½åœ¨å³¡è°·å‹æŸå¤±é¢éœ‡è¡ã€‚

  \item \textbf{SGD + Momentum}  
    \[
    v_t = \beta v_{t-1} + (1 - \beta) g_t
    \]
    \[
    w_{t+1} = w_t - \eta \, v_t
    \]  
    è§£é‡Šï¼šå…ˆç»´æŠ¤ä¸€ä¸ªâ€œé€Ÿåº¦å‘é‡â€ $v_t$ï¼ˆæ¢¯åº¦çš„æŒ‡æ•°åŠ æƒå¹³å‡ï¼‰ï¼Œæ›´æ–°æ—¶ç”¨è¿™ä¸ªé€Ÿåº¦ä»£æ›¿ç›´æ¥ç”¨æ¢¯åº¦ã€‚è¿™æ ·èƒ½ä¿ç•™è¿‡å»çš„æ¢¯åº¦æ–¹å‘ä¿¡æ¯ï¼Œä½¿ä¼˜åŒ–å™¨åœ¨å¹³ç¼“æ–¹å‘ä¸ŠåŠ é€Ÿï¼Œåœ¨éœ‡è¡æ–¹å‘ä¸Šå‡é€Ÿã€‚

  \item \textbf{AdaGrad}  
    \[
    G_t = G_{t-1} + g_t^2
    \]
    \[
    w_{t+1} = w_t - \frac{\eta}{\sqrt{G_t} + \epsilon} \, g_t
    \]  
    è§£é‡Šï¼šå¯¹æ¯ä¸ªå‚æ•°ç»´æŠ¤ç´¯è®¡çš„å¹³æ–¹æ¢¯åº¦ $G_t$ï¼Œå‡ºç°å¤§æ¢¯åº¦çš„å‚æ•°ä¼šå¾—åˆ°æ›´å°çš„å­¦ä¹ ç‡ï¼Œå°æ¢¯åº¦çš„å‚æ•°å­¦ä¹ ç‡æ›´å¤§ã€‚è¿™å¯¹äºç¨€ç–ç‰¹å¾ç‰¹åˆ«æœ‰ç”¨ï¼Œä½†å­¦ä¹ ç‡ä¼šä¸æ–­è¡°å‡ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒæå‰åœæ»ã€‚

  \item \textbf{RMSProp}  
    \[
    E[g^2]_t = \beta E[g^2]_{t-1} + (1 - \beta) g_t^2
    \]
    \[
    w_{t+1} = w_t - \frac{\eta}{\sqrt{E[g^2]_t} + \epsilon} \, g_t
    \]  
    è§£é‡Šï¼šç±»ä¼¼ AdaGradï¼Œä½†ç”¨æŒ‡æ•°åŠ æƒå¹³å‡ $E[g^2]_t$ ä»£æ›¿ç´¯è®¡å’Œï¼Œé˜²æ­¢å­¦ä¹ ç‡å¿«é€Ÿè¡°å‡ã€‚å®ƒåœ¨æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­éå¸¸å¸¸ç”¨ã€‚

  \item \textbf{Adam}ï¼ˆAdaptive Moment Estimationï¼‰  
    \[
    m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \quad\text{(ä¸€é˜¶çŸ©: å¹³å‡æ¢¯åº¦)}
    \]
    \[
    v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \quad\text{(äºŒé˜¶çŸ©: å¹³å‡å¹³æ–¹æ¢¯åº¦)}
    \]
    åå·®ä¿®æ­£ï¼š
    \[
    \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad
    \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
    \]
    å‚æ•°æ›´æ–°ï¼š
    \[
    w_{t+1} = w_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \, \hat{m}_t
    \]  
    è§£é‡Šï¼šAdam åŒæ—¶ä½¿ç”¨äº†åŠ¨é‡ï¼ˆå¹³æ»‘æ¢¯åº¦æ–¹å‘ï¼‰å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆç¼©æ”¾æ›´æ–°å¹…åº¦ï¼‰ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„ä¼˜åŒ–å™¨ä¹‹ä¸€ã€‚
\end{itemize}
}



\paragraph{Memory accounting for one training step.}~{}

ä¸€æ¬¡è®­ç»ƒè¿­ä»£çš„æ˜¾å­˜æ¶ˆè€—å¯ä»¥åˆ†è§£ä¸ºä»¥ä¸‹å‡ éƒ¨åˆ†ï¼ˆå‡è®¾æ•°æ®ç±»å‹ä¸º \texttt{float32}ï¼Œå³æ¯ä¸ªæ•°å€¼å  $4$ å­—èŠ‚ï¼‰ï¼š

\begin{itemize}
  \item \textbf{Parameters:} æ¨¡å‹æƒé‡æ•°é‡  
    \[
    \texttt{num\_parameters} = (D \times D \times \texttt{num\_layers}) + D
    \]
    å…¶ä¸­ $D$ ä¸ºéšè—å±‚ç»´åº¦ï¼Œ\(\texttt{num\_layers}\) ä¸ºå±‚æ•°ï¼Œæœ€åçš„ $+D$ æ¥è‡ªåç½®é¡¹ã€‚

  \item \textbf{Activations:} å‰å‘ä¼ æ’­ä¸­æ¯å±‚çš„æ¿€æ´»å€¼  
    \[
    \texttt{num\_activations} = B \times D \times \texttt{num\_layers}
    \]
    å…¶ä¸­ $B$ ä¸º batch sizeã€‚

  \item \textbf{Gradients:} æ¯ä¸ªå‚æ•°å¯¹åº”çš„ä¸€é˜¶æ¢¯åº¦  
    \[
    \texttt{num\_gradients} = \texttt{num\_parameters}
    \]

  \item \textbf{Optimizer states:}  
    ä»¥ \texttt{AdaGrad} ä¸ºä¾‹ï¼Œéœ€è¦ä¸ºæ¯ä¸ªå‚æ•°å­˜å‚¨æ¢¯åº¦å¹³æ–¹çš„ç´¯ç§¯å€¼ï¼š
    \[
    \texttt{num\_optimizer\_states} = \texttt{num\_parameters}
    \]
\end{itemize}

å°†ä¸Šè¿°éƒ¨åˆ†ç›¸åŠ ï¼Œå¾—åˆ°ä¸€æ¬¡è¿­ä»£çš„æ€»æ˜¾å­˜å ç”¨ï¼š
\[
\texttt{total\_memory} = 4 \times (\texttt{num\_parameters} + \texttt{num\_activations} + \texttt{num\_gradients} + \texttt{num\_optimizer\_states})
\]
å…¶ä¸­ç³»æ•° $4$ è¡¨ç¤ºæ¯ä¸ª \texttt{float32} å…ƒç´ çš„å­—èŠ‚æ•°ã€‚

\paragraph{FLOPs for one step.}~{}
åœ¨ä¸€æ¬¡åå‘ä¼ æ’­ä¸­ï¼Œå‡è®¾æ¯ä¸ªå‚æ•°çš„è®¡ç®—é‡çº¦ä¸º $6B$ æ¬¡æµ®ç‚¹è¿ç®—ï¼š
\[
\texttt{flops} = 6 \times B \times \texttt{num\_parameters}
\]

\noindent
è¿™ç§æ˜¾å­˜ä¸è®¡ç®—é‡çš„ç²¾ç¡®ä¼°ç®—æ–¹æ³•æœ‰åŠ©äºæˆ‘ä»¬åœ¨ä¸åŒç¡¬ä»¶æ¡ä»¶ä¸‹åšæ‰¹é‡å¤§å° (\(B\)) ä¸æ¨¡å‹è§„æ¨¡ (\(D, \texttt{num\_layers}\)) çš„æƒè¡¡ã€‚

\clearpage
\section{Train-loop}

\begin{lstlisting}[language=Python]
def train(name: str, get_batch,
          D: int, num_layers: int,
          B: int, num_train_steps: int, lr: float):
    device = get_device()
    model = Cruncher(dim=D, num_layers=num_layers).to(device)

    # Optimizer (SGD here, could swap to Adam/Adagrad/RMSProp)
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)

    # Optional: simple running loss for logging
    running = 0.0

    for step in range(1, num_train_steps + 1):
        # 1) Data
        x, y = get_batch(B)

        # 2) Forward
        pred_y = model(x)
        loss = F.mse_loss(pred_y, y)

        # 3) Backward
        optimizer.zero_grad(set_to_none=True)  # clear grads first
        loss.backward()

        # 4) Update
        optimizer.step()

        # 5) (Optional) Log
        running += loss.item()
        if step % max(1, (num_train_steps // 5)) == 0:
            avg = running / max(1, (num_train_steps // 5))
            print(f"[{name}] step={step}  loss={avg:.4e}  lr={lr}")
            running = 0.0
\end{lstlisting}

\paragraph{æµç¨‹è¯´æ˜ã€‚}
\begin{enumerate}
  \item \textbf{æ„å»ºæ¨¡å‹ï¼š} \(\texttt{Cruncher(D, num\_layers)}\) æ˜¯å‰é¢å®šä¹‰çš„æ·±åº¦çº¿æ€§å †å ï¼›
  \item \textbf{é€‰æ‹©ä¼˜åŒ–å™¨ï¼š} æ­¤å¤„ç”¨ \texttt{SGD(lr=lr)}ï¼Œä¹Ÿå¯æ›¿æ¢ä¸º Adamã€AdaGradã€RMSPropï¼›
  \item \textbf{è®­ç»ƒå¾ªç¯ï¼š} æ¯æ­¥åŒ…å«æ•°æ®å–æ ·ã€å‰å‘ã€è®¡ç®—æŸå¤±ã€æ¢¯åº¦æ¸…é›¶ã€åå‘ã€å‚æ•°æ›´æ–°ã€æ—¥å¿—ï¼›
\end{enumerate}
  
\clearpage
\section{Checkpointing (Save/Load)}

é•¿æ—¶é—´è®­ç»ƒå¾ˆå¯èƒ½ä¸­æ–­ï¼Œå› æ­¤éœ€è¦å‘¨æœŸæ€§ä¿å­˜æ¨¡å‹ä¸ä¼˜åŒ–å™¨çŠ¶æ€ã€‚

\begin{lstlisting}[language=Python]
def checkpointing():
    device = get_device()
    model = Cruncher(dim=64, num_layers=3).to(device)
    optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)

    # --- Save checkpoint ---
    checkpoint = {
        "model": model.state_dict(),
        "optimizer": optimizer.state_dict(),
        "meta": {"step": 1000}  # e.g., current step/epoch, LR, RNG state...
    }
    torch.save(checkpoint, "model_checkpoint.pt")

    # --- Load checkpoint ---
    loaded = torch.load("model_checkpoint.pt", map_location=device)
    model.load_state_dict(loaded["model"])
    optimizer.load_state_dict(loaded["optimizer"])
    step0 = loaded.get("meta", {}).get("step", 0)

    model.train()  # ensure training mode after loading if you continue training
    print(f"Resumed from step={step0}")
\end{lstlisting}

\paragraph{è¦ç‚¹ä¸å®è·µæç¤ºã€‚}
\begin{itemize}
  \item \textbf{ä¿å­˜ä»€ä¹ˆï¼Ÿ} \texttt{model.state\_dict()} å’Œ \texttt{optimizer.state\_dict()} å¿…é¡»ä¸€èµ·ä¿å­˜ï¼Œç»§ç»­è®­ç»ƒæ—¶æ‰èƒ½ä¿æŒå­¦ä¹ ç‡è°ƒåº¦ä¸è‡ªé€‚åº”ç»Ÿè®¡ï¼ˆå¦‚ Adam çš„ä¸€äºŒé˜¶çŸ©ï¼‰ã€‚
  \item \textbf{è¿˜å¯ä¿å­˜ï¼š} å½“å‰æ­¥æ•°ã€epochã€å­¦ä¹ ç‡ã€éšæœºæ•°ç§å­çŠ¶æ€ï¼ˆ\texttt{torch}/\texttt{numpy}/\texttt{random}ï¼‰ï¼Œä¾¿äºå®Œå…¨å¤ç°ã€‚
  \item \textbf{è·¨è®¾å¤‡æ¢å¤ï¼š} ä½¿ç”¨ \texttt{map\_location=device} å¯åœ¨ CPU/GPU é—´æ— ç—›åˆ‡æ¢åŠ è½½ã€‚
  \item \textbf{è¯„ä¼°/æ¨ç†ï¼š} åŠ è½½ååšè¯„ä¼°è¯·è°ƒç”¨ \texttt{model.eval()}ï¼›ç»§ç»­è®­ç»ƒåˆ™ \texttt{model.train()}ã€‚
\end{itemize}

% \clearpage
