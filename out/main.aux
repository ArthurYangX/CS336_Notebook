\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces fraction of FLOPs spent in attention versus MLP changes with scale.}}{1}{figure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Emergent Abilities of Large Language Models}}{1}{figure.1.2}\protected@file@percent }
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Overview and Tokenization}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@1}}
\newmarginnote{note.1.1}{{1}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Why this course exists}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}The industrialization of language models }{1}{subsection.1.1.1}\protected@file@percent }
\newlabel{def:FLOPs-FLOPS}{{1.1.1}{1}{The industrialization of language models }{tcb@cnt@Definition.1}{}}
\newlabel{def:FLOPs-FLOPS@cref}{{[subsection][1][1,1]1.1.1}{[1][1][]1}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}What can we learn in this class that transfers to frontier models? }{2}{subsection.1.1.2}\protected@file@percent }
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@1}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@2}}
\newmarginnote{note.3.1}{{3}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Current Landscape}{3}{section.1.2}\protected@file@percent }
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@2}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@3}}
\newmarginnote{note.5.1}{{5}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Course Components}{5}{section.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Design Decisions}}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:design decisions}{{1.3}{5}{Design Decisions}{figure.caption.1}{}}
\newlabel{fig:design decisions@cref}{{[figure][3][1]1.3}{[1][5][]5}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Tokenizer Example}}{6}{figure.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Transformer}}{6}{figure.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Basics}{6}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Tokenization}{6}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture}{6}{tcb@cnt@Definition.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training}{6}{figure.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces GPU Kernels}}{7}{figure.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Systems}{7}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Kernels}{7}{subsection.1.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Memory 与 Compute之间的数据传输带宽开销示意}}{7}{figure.caption.2}\protected@file@percent }
\newlabel{fig:bandwidth-cost}{{1.7}{7}{Memory 与 Compute之间的数据传输带宽开销示意}{figure.caption.2}{}}
\newlabel{fig:bandwidth-cost@cref}{{[figure][7][1]1.7}{[1][7][]7}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Inference}}{8}{figure.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Parallelism}{8}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Inference}{8}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Scaling Laws}{9}{subsection.1.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Scaling Laws}}{9}{figure.caption.3}\protected@file@percent }
\newlabel{fig:scaling laws}{{1.9}{9}{Scaling Laws}{figure.caption.3}{}}
\newlabel{fig:scaling laws@cref}{{[figure][9][1]1.9}{[1][9][]9}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Data}{9}{subsection.1.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Composition of Data}}{9}{figure.caption.4}\protected@file@percent }
\newlabel{fig:data}{{1.10}{9}{Composition of Data}{figure.caption.4}{}}
\newlabel{fig:data@cref}{{[figure][10][1]1.10}{[1][9][]9}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation}{9}{figure.caption.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces SFT Code 示意}}{10}{figure.1.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Data curation}{10}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Data processing}{10}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Alignment}{10}{subsection.1.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Supervised Fine-Tuning (SFT)}{10}{figure.1.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Learn from feedback示意 }}{11}{figure.1.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Learning from Feedback}{11}{figure.1.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces GPT-2 Tokenizer}}{12}{figure.1.14}\protected@file@percent }
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@3}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@4}}
\newmarginnote{note.12.1}{{12}{13468164sp}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Tokenization}{12}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Intro}{12}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Observations}{12}{subsection.1.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Tokenization}}{12}{figure.caption.6}\protected@file@percent }
\newlabel{fig:Tokenize}{{1.13}{12}{Tokenization}{figure.caption.6}{}}
\newlabel{fig:Tokenize@cref}{{[figure][13][1]1.13}{[1][12][]12}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Character-based tokenization}}{13}{figure.1.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Byte-based tokenization}}{13}{figure.1.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Character Tokenizer}{13}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Problems}{13}{figure.1.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Byte Tokenizer}{13}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Drawback}{13}{figure.1.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces Word-based tokenization}}{14}{figure.1.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}Word Tokenizer}{14}{subsection.1.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Drawback:}{14}{figure.1.17}\protected@file@percent }
\newmarginnote{note.14.1}{{14}{13468164sp}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}BPE Tokenizer}{14}{subsection.1.4.6}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Byte $-$ Pair Encoding}}{14}{algocf.1}\protected@file@percent }
\newlabel{alg:bpe}{{1}{14}{BPE Tokenizer}{algocf.1}{}}
\newlabel{alg:bpe@cref}{{[algocf][1][]1}{[1][14][]14}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.7}Tokenizer Properties}{16}{subsection.1.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{There are three major groups of design choices that determine how the tokenizer will break down text:}{16}{subsection.1.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The tokenization method}{16}{Item.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The initialization parameters}{16}{Item.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The domain of the data the tokenizer targets}{16}{Item.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.8}Summary of Tokenization Methods}{17}{subsection.1.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}PyTorch, Resource Accounting}{19}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@4}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@5}}
\newmarginnote{note.19.1}{{19}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Original Transformer}{19}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Transformer-model architecture}}{19}{figure.caption.8}\protected@file@percent }
\newlabel{fig: Transformer architecture}{{2.1}{19}{The Transformer-model architecture}{figure.caption.8}{}}
\newlabel{fig: Transformer architecture@cref}{{[figure][1][2]2.1}{[1][19][]19}{}{}{}}
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@5}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@6}}
\newmarginnote{note.21.1}{{21}{2752512sp}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Input Embedding}{21}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Input $\xrightarrow  {}$ Tokens $\xrightarrow  {}$ Embedding Vectors}}{21}{figure.caption.9}\protected@file@percent }
\newlabel{fig:Input Embedding Box}{{2.2}{21}{Input $\xrightarrow {}$ Tokens $\xrightarrow {}$ Embedding Vectors}{figure.caption.9}{}}
\newlabel{fig:Input Embedding Box@cref}{{[figure][2][2]2.2}{[1][21][]21}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Tokenization}{21}{figure.caption.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Embedding}{21}{figure.caption.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Static Token Embeddings vs. Contextualized Embeddings}{21}{tcb@cnt@Definition.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A language model operates on raw, static embeddings as its input and produces contextual text embeddings.}}{22}{figure.2.4}\protected@file@percent }
\newlabel{fig:static-embedding}{{2.3a}{22}{Static Embedding}{figure.caption.10}{}}
\newlabel{fig:static-embedding@cref}{{[subfigure][1][2,3]2.3a}{[1][21][]22}{}{}{}}
\newlabel{sub@fig:static-embedding}{{a}{22}{Static Embedding}{figure.caption.10}{}}
\newlabel{sub@fig:static-embedding@cref}{{[subfigure][1][2,3]2.3a}{[1][21][]22}{}{}{}}
\newlabel{fig:contextualized-embedding}{{2.3b}{22}{Contextualized Embedding}{figure.caption.10}{}}
\newlabel{fig:contextualized-embedding@cref}{{[subfigure][2][2,3]2.3b}{[1][21][]22}{}{}{}}
\newlabel{sub@fig:contextualized-embedding}{{b}{22}{Contextualized Embedding}{figure.caption.10}{}}
\newlabel{sub@fig:contextualized-embedding@cref}{{[subfigure][2][2,3]2.3b}{[1][21][]22}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces 静态嵌入 vs. 上下文化嵌入对比}}{22}{figure.caption.10}\protected@file@percent }
\newlabel{fig:embeddings-comparison}{{2.3}{22}{静态嵌入 vs. 上下文化嵌入对比}{figure.caption.10}{}}
\newlabel{fig:embeddings-comparison@cref}{{[figure][3][2]2.3}{[1][21][]22}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{2. Text and Sentence Embeddings}{22}{figure.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Word2Vec}{22}{tcb@cnt@Definition.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces CBOW和Skip-gram architecture示意图}}{22}{figure.caption.11}\protected@file@percent }
\newlabel{fig:Word2Vec}{{2.5}{22}{CBOW和Skip-gram architecture示意图}{figure.caption.11}{}}
\newlabel{fig:Word2Vec@cref}{{[figure][5][2]2.5}{[1][22][]22}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces 位置编码示意图}}{23}{figure.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Positional Encoding}{23}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1.固定正余弦位置编码（Vaswani原版）}{23}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2.可学习位置编码}{23}{Item.23}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces 两种常见位置编码方法的比较}}{23}{table.caption.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Transdormer Encoder Block from Vaswani et al. (2017)}}{24}{figure.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Encoder 结构}{24}{subsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Transformer Encoder Block from Vaswani et al. (2017)}}{24}{figure.caption.13}\protected@file@percent }
\newlabel{fig:Transformer Encoder Block}{{2.8}{24}{Transformer Encoder Block from Vaswani et al. (2017)}{figure.caption.13}{}}
\newlabel{fig:Transformer Encoder Block@cref}{{[figure][8][2]2.8}{[1][24][]24}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{多头自注意机制}{24}{figure.caption.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces We get better LLMs by doing attention multiple times in parallel, increasing the model’s capacity to attend to different types of information.}}{24}{figure.caption.14}\protected@file@percent }
\newlabel{fig:Multi-Head Attention}{{2.9}{24}{We get better LLMs by doing attention multiple times in parallel, increasing the model’s capacity to attend to different types of information}{figure.caption.14}{}}
\newlabel{fig:Multi-Head Attention@cref}{{[figure][9][2]2.9}{[1][24][]24}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Multi-Head Self-Attention计算图}}{25}{figure.caption.15}\protected@file@percent }
\newlabel{fig:Multi-Head Self-Attention计算图}{{2.10}{25}{Multi-Head Self-Attention计算图}{figure.caption.15}{}}
\newlabel{fig:Multi-Head Self-Attention计算图@cref}{{[figure][10][2]2.10}{[1][25][]25}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Step0:进行Self-Attention之前的输入和投影矩阵}{26}{equation.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Step0:进行Self-Attention之前的输入和投影矩阵}}{26}{figure.caption.16}\protected@file@percent }
\newlabel{fig:Step0:进行Self-Attention之前的输入和投影矩阵}{{2.11}{26}{Step0:进行Self-Attention之前的输入和投影矩阵}{figure.caption.16}{}}
\newlabel{fig:Step0:进行Self-Attention之前的输入和投影矩阵@cref}{{[figure][11][2]2.11}{[1][26][]26}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Step1:根据投影矩阵更新$Q$,$K$,$V$}{27}{figure.caption.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Step1:根据投影矩阵更新$Q$,$K$,$V$}}{27}{figure.caption.17}\protected@file@percent }
\newlabel{fig:Step1:根据投影矩阵更新$Q$,$K$,$V$}{{2.12}{27}{Step1:根据投影矩阵更新$Q$,$K$,$V$}{figure.caption.17}{}}
\newlabel{fig:Step1:根据投影矩阵更新$Q$,$K$,$V$@cref}{{[figure][12][2]2.12}{[1][27][]27}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Teacher Forcing 示意图}}{28}{figure.2.14}\protected@file@percent }
\newlabel{fig:TeacherForcing}{{2.14}{28}{Teacher Forcing 示意图}{figure.2.14}{}}
\newlabel{fig:TeacherForcing@cref}{{[figure][14][2]2.14}{[1][28][]28}{}{}{}}
\newlabel{def:teacher-forcing}{{2.1.3}{28}{Step2:计算相关性分数并进行Softmax归一化}{tcb@cnt@Definition.6}{}}
\newlabel{def:teacher-forcing@cref}{{[subsection][3][2,1]2.1.3}{[1][28][]28}{}{}{}}
\newlabel{def:auto-regressive}{{2.1.3}{28}{Step2:计算相关性分数并进行Softmax归一化}{tcb@cnt@Definition.7}{}}
\newlabel{def:auto-regressive@cref}{{[subsection][3][2,1]2.1.3}{[1][28][]28}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Step2:计算相关性分数并进行Softmax归一化}{28}{figure.caption.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Step2:计算相关性分数并进行Softmax归一化}}{28}{figure.caption.18}\protected@file@percent }
\newlabel{fig:Step2:计算相关性分数并进行Softmax归一化}{{2.13}{28}{Step2:计算相关性分数并进行Softmax归一化}{figure.caption.18}{}}
\newlabel{fig:Step2:计算相关性分数并进行Softmax归一化@cref}{{[figure][13][2]2.13}{[1][28][]28}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Step3:用注意力权重对$𝑉$权求和，得到输出}{29}{tcb@cnt@Definition.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Step3:用注意力权重对$𝑉$权求和，得到输出}}{29}{figure.caption.19}\protected@file@percent }
\newlabel{fig:Step3:用注意力权重对$𝑉$权求和，得到输出}{{2.15}{29}{Step3:用注意力权重对$𝑉$权求和，得到输出}{figure.caption.19}{}}
\newlabel{fig:Step3:用注意力权重对$𝑉$权求和，得到输出@cref}{{[figure][15][2]2.15}{[1][29][]29}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces Multi-Head Attention的输出拼接示意图}}{30}{figure.2.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces 多头注意力输出整合并送入FFN}}{30}{figure.2.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step4: 多头注意力的输出合并}{30}{figure.caption.19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Multi-Head Attention中的$Q$,$K$,$V$在不同Head中是独立的}}{30}{figure.caption.20}\protected@file@percent }
\newlabel{fig:Multi-Head Attention计算图}{{2.16}{30}{Multi-Head Attention中的$Q$,$K$,$V$在不同Head中是独立的}{figure.caption.20}{}}
\newlabel{fig:Multi-Head Attention计算图@cref}{{[figure][16][2]2.16}{[1][30][]30}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Step5: Add \& Norm}{31}{Item.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Layer Normalization}{31}{Item.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces \href  {http://proceedings.mlr.press/v119/shen20e/shen20e.pdf}{Layer Normalization vs. Batch Normalization} 示意图}}{31}{figure.caption.21}\protected@file@percent }
\newlabel{fig:layernorm}{{2.19}{31}{\href {http://proceedings.mlr.press/v119/shen20e/shen20e.pdf}{Layer Normalization vs. Batch Normalization} 示意图}{figure.caption.21}{}}
\newlabel{fig:layernorm@cref}{{[figure][19][2]2.19}{[1][31][]31}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces Residual Connection 示意图}}{32}{figure.2.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Residual Connection}{32}{figure.caption.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Point-wise Feed-Forward Network}{33}{tcb@cnt@Definition.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces Point-wise Feed-Forward Network}}{33}{figure.caption.24}\protected@file@percent }
\newlabel{fig:Point-wise Feed-Forward Network}{{2.21}{33}{Point-wise Feed-Forward Network}{figure.caption.24}{}}
\newlabel{fig:Point-wise Feed-