\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces fraction of FLOPs spent in attention versus MLP changes with scale.}}{1}{figure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Emergent Abilities of Large Language Models}}{1}{figure.1.2}\protected@file@percent }
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Overview and Tokenization}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@1}}
\newmarginnote{note.1.1}{{1}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Why this course exists}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}The industrialization of language models }{1}{subsection.1.1.1}\protected@file@percent }
\newlabel{def:FLOPs-FLOPS}{{1.1.1}{1}{The industrialization of language models }{tcb@cnt@Definition.1}{}}
\newlabel{def:FLOPs-FLOPS@cref}{{[subsection][1][1,1]1.1.1}{[1][1][]1}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}What can we learn in this class that transfers to frontier models? }{2}{subsection.1.1.2}\protected@file@percent }
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@1}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@2}}
\newmarginnote{note.3.1}{{3}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Current Landscape}{3}{section.1.2}\protected@file@percent }
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@2}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@3}}
\newmarginnote{note.5.1}{{5}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Course Components}{5}{section.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Design Decisions}}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:design decisions}{{1.3}{5}{Design Decisions}{figure.caption.1}{}}
\newlabel{fig:design decisions@cref}{{[figure][3][1]1.3}{[1][5][]5}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Tokenizer Example}}{6}{figure.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Transformer}}{6}{figure.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Basics}{6}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Tokenization}{6}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture}{6}{tcb@cnt@Definition.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training}{6}{figure.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces GPU Kernels}}{7}{figure.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Systems}{7}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Kernels}{7}{section*.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Memory 与 Compute之间的数据传输带宽开销示意}}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:bandwidth-cost}{{1.7}{7}{Memory 与 Compute之间的数据传输带宽开销示意}{figure.caption.4}{}}
\newlabel{fig:bandwidth-cost@cref}{{[figure][7][1]1.7}{[1][7][]7}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Inference}}{8}{figure.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Parallelism}{8}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Inference}{8}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Scaling Laws}{9}{subsection.1.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Scaling Laws}}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:scaling laws}{{1.9}{9}{Scaling Laws}{figure.caption.5}{}}
\newlabel{fig:scaling laws@cref}{{[figure][9][1]1.9}{[1][9][]9}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Data}{9}{subsection.1.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Composition of Data}}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:data}{{1.10}{9}{Composition of Data}{figure.caption.6}{}}
\newlabel{fig:data@cref}{{[figure][10][1]1.10}{[1][9][]9}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation}{9}{figure.caption.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces SFT Code 示意}}{10}{figure.1.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Data curation}{10}{figure.caption.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Data processing}{10}{figure.caption.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Alignment}{10}{subsection.1.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Supervised Fine-Tuning (SFT)}{10}{figure.1.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Learn from feedback示意 }}{11}{figure.1.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Learning from Feedback}{11}{figure.1.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces GPT-2 Tokenizer}}{12}{figure.1.14}\protected@file@percent }
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@3}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@4}}
\newmarginnote{note.12.1}{{12}{13468164sp}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Tokenization}{12}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Intro}{12}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Observations}{12}{subsection.1.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Tokenization}}{12}{figure.caption.8}\protected@file@percent }
\newlabel{fig:Tokenize}{{1.13}{12}{Tokenization}{figure.caption.8}{}}
\newlabel{fig:Tokenize@cref}{{[figure][13][1]1.13}{[1][12][]12}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Character-based tokenization}}{13}{figure.1.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Byte-based tokenization}}{13}{figure.1.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Character Tokenizer}{13}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Problems}{13}{figure.1.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Byte Tokenizer}{13}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Drawback}{13}{figure.1.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces Word-based tokenization}}{14}{figure.1.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}Word Tokenizer}{14}{subsection.1.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Drawback:}{14}{figure.1.17}\protected@file@percent }
\newmarginnote{note.14.1}{{14}{13468164sp}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}BPE Tokenizer}{14}{subsection.1.4.6}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Byte $-$ Pair Encoding}}{14}{algocf.1}\protected@file@percent }
\newlabel{alg:bpe}{{1}{14}{BPE Tokenizer}{algocf.1}{}}
\newlabel{alg:bpe@cref}{{[algocf][1][]1}{[1][14][]14}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.7}Tokenizer Properties}{16}{subsection.1.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{There are three major groups of design choices that determine how the tokenizer will break down text:}{16}{subsection.1.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The tokenization method}{16}{Item.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The initialization parameters}{16}{Item.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The domain of the data the tokenizer targets}{16}{Item.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.8}Summary of Tokenization Methods}{17}{subsection.1.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}PyTorch, Resource Accounting}{19}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@4}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@5}}
\newmarginnote{note.19.1}{{19}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Original Transformer}{19}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Transformer-model architecture}}{19}{figure.caption.10}\protected@file@percent }
\newlabel{fig: Transformer architecture}{{2.1}{19}{The Transformer-model architecture}{figure.caption.10}{}}
\newlabel{fig: Transformer architecture@cref}{{[figure][1][2]2.1}{[1][19][]19}{}{}{}}
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@5}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@6}}
\newmarginnote{note.21.1}{{21}{2752512sp}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Input Embedding}{21}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Input $\xrightarrow  {}$ Tokens $\xrightarrow  {}$ Embedding Vectors}}{21}{figure.caption.11}\protected@file@percent }
\newlabel{fig:Input Embedding Box}{{2.2}{21}{Input $\xrightarrow {}$ Tokens $\xrightarrow {}$ Embedding Vectors}{figure.caption.11}{}}
\newlabel{fig:Input Embedding Box@cref}{{[figure][2][2]2.2}{[1][21][]21}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Tokenization}{21}{figure.caption.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Embedding}{21}{figure.caption.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Static Token Embeddings vs. Contextualized Embeddings}{21}{tcb@cnt@Definition.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A language model operates on raw, static embeddings as its input and produces contextual text embeddings.}}{22}{figure.2.4}\protected@file@percent }
\newlabel{fig:static-embedding}{{2.3a}{22}{Static Embedding}{figure.caption.12}{}}
\newlabel{fig:static-embedding@cref}{{[subfigure][1][2,3]2.3a}{[1][21][]22}{}{}{}}
\newlabel{sub@fig:static-embedding}{{a}{22}{Static Embedding}{figure.caption.12}{}}
\newlabel{sub@fig:static-embedding@cref}{{[subfigure][1][2,3]2.3a}{[1][21][]22}{}{}{}}
\newlabel{fig:contextualized-embedding}{{2.3b}{22}{Contextualized Embedding}{figure.caption.12}{}}
\newlabel{fig:contextualized-embedding@cref}{{[subfigure][2][2,3]2.3b}{[1][21][]22}{}{}{}}
\newlabel{sub@fig:contextualized-embedding}{{b}{22}{Contextualized Embedding}{figure.caption.12}{}}
\newlabel{sub@fig:contextualized-embedding@cref}{{[subfigure][2][2,3]2.3b}{[1][21][]22}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces 静态嵌入 vs. 上下文化嵌入对比}}{22}{figure.caption.12}\protected@file@percent }
\newlabel{fig:embeddings-comparison}{{2.3}{22}{静态嵌入 vs. 上下文化嵌入对比}{figure.caption.12}{}}
\newlabel{fig:embeddings-comparison@cref}{{[figure][3][2]2.3}{[1][21][]22}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{2. Text and Sentence Embeddings}{22}{figure.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Word2Vec}{22}{tcb@cnt@Definition.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces CBOW和Skip-gram architecture示意图}}{22}{figure.caption.13}\protected@file@percent }
\newlabel{fig:Word2Vec}{{2.5}{22}{CBOW和Skip-gram architecture示意图}{figure.caption.13}{}}
\newlabel{fig:Word2Vec@cref}{{[figure][5][2]2.5}{[1][22][]22}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces 位置编码示意图}}{23}{figure.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Positional Encoding}{23}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1.固定正余弦位置编码（Vaswani原版）}{23}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2.可学习位置编码}{23}{Item.23}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces 两种常见位置编码方法的比较}}{23}{table.caption.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Transdormer Encoder Block from Vaswani et al. (2017)}}{24}{figure.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Encoder 结构}{24}{subsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Transformer Encoder Block from Vaswani et al. (2017)}}{24}{figure.caption.15}\protected@file@percent }
\newlabel{fig:Transformer Encoder Block}{{2.8}{24}{Transformer Encoder Block from Vaswani et al. (2017)}{figure.caption.15}{}}
\newlabel{fig:Transformer Encoder Block@cref}{{[figure][8][2]2.8}{[1][24][]24}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{多头自注意机制}{24}{figure.caption.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces We get better LLMs by doing attention multiple times in parallel, increasing the model’s capacity to attend to different types of information.}}{24}{figure.caption.16}\protected@file@percent }
\newlabel{fig:Multi-Head Attention}{{2.9}{24}{We get better LLMs by doing attention multiple times in parallel, increasing the model’s capacity to attend to different types of information}{figure.caption.16}{}}
\newlabel{fig:Multi-Head Attention@cref}{{[figure][9][2]2.9}{[1][24][]24}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Multi-Head Self-Attention计算图}}{25}{figure.caption.17}\protected@file@percent }
\newlabel{fig:Multi-Head Self-Attention计算图}{{2.10}{25}{Multi-Head Self-Attention计算图}{figure.caption.17}{}}
\newlabel{fig:Multi-Head Self-Attention计算图@cref}{{[figure][10][2]2.10}{[1][25][]25}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Step0:进行Self-Attention之前的输入和投影矩阵}{26}{equation.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Step0:进行Self-Attention之前的输入和投影矩阵}}{26}{figure.caption.18}\protected@file@percent }
\newlabel{fig:Step0:进行Self-Attention之前的输入和投影矩阵}{{2.11}{26}{Step0:进行Self-Attention之前的输入和投影矩阵}{figure.caption.18}{}}
\newlabel{fig:Step0:进行Self-Attention之前的输入和投影矩阵@cref}{{[figure][11][2]2.11}{[1][26][]26}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Step1:根据投影矩阵更新$Q$,$K$,$V$}{27}{figure.caption.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Step1:根据投影矩阵更新$Q$,$K$,$V$}}{27}{figure.caption.19}\protected@file@percent }
\newlabel{fig:Step1:根据投影矩阵更新$Q$,$K$,$V$}{{2.12}{27}{Step1:根据投影矩阵更新$Q$,$K$,$V$}{figure.caption.19}{}}
\newlabel{fig:Step1:根据投影矩阵更新$Q$,$K$,$V$@cref}{{[figure][12][2]2.12}{[1][27][]27}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Teacher Forcing 示意图}}{28}{figure.2.14}\protected@file@percent }
\newlabel{fig:TeacherForcing}{{2.14}{28}{Teacher Forcing 示意图}{figure.2.14}{}}
\newlabel{fig:TeacherForcing@cref}{{[figure][14][2]2.14}{[1][28][]28}{}{}{}}
\newlabel{def:teacher-forcing}{{2.1.3}{28}{Step2:计算相关性分数并进行Softmax归一化}{tcb@cnt@Definition.6}{}}
\newlabel{def:teacher-forcing@cref}{{[subsection][3][2,1]2.1.3}{[1][28][]28}{}{}{}}
\newlabel{def:auto-regressive}{{2.1.3}{28}{Step2:计算相关性分数并进行Softmax归一化}{tcb@cnt@Definition.7}{}}
\newlabel{def:auto-regressive@cref}{{[subsection][3][2,1]2.1.3}{[1][28][]28}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Step2:计算相关性分数并进行Softmax归一化}{28}{figure.caption.19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Step2:计算相关性分数并进行Softmax归一化}}{28}{figure.caption.20}\protected@file@percent }
\newlabel{fig:Step2:计算相关性分数并进行Softmax归一化}{{2.13}{28}{Step2:计算相关性分数并进行Softmax归一化}{figure.caption.20}{}}
\newlabel{fig:Step2:计算相关性分数并进行Softmax归一化@cref}{{[figure][13][2]2.13}{[1][28][]28}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Step3:用注意力权重对$𝑉$权求和，得到输出}{29}{tcb@cnt@Definition.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Step3:用注意力权重对$𝑉$权求和，得到输出}}{29}{figure.caption.21}\protected@file@percent }
\newlabel{fig:Step3:用注意力权重对$𝑉$权求和，得到输出}{{2.15}{29}{Step3:用注意力权重对$𝑉$权求和，得到输出}{figure.caption.21}{}}
\newlabel{fig:Step3:用注意力权重对$𝑉$权求和，得到输出@cref}{{[figure][15][2]2.15}{[1][29][]29}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces Multi-Head Attention的输出拼接示意图}}{30}{figure.2.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces 多头注意力输出整合并送入FFN}}{30}{figure.2.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step4: 多头注意力的输出合并}{30}{figure.caption.21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Multi-Head Attention中的$Q$,$K$,$V$在不同Head中是独立的}}{30}{figure.caption.22}\protected@file@percent }
\newlabel{fig:Multi-Head Attention计算图}{{2.16}{30}{Multi-Head Attention中的$Q$,$K$,$V$在不同Head中是独立的}{figure.caption.22}{}}
\newlabel{fig:Multi-Head Attention计算图@cref}{{[figure][16][2]2.16}{[1][30][]30}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Step5: Add \& Norm}{31}{Item.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Layer Normalization}{31}{Item.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces \href  {http://proceedings.mlr.press/v119/shen20e/shen20e.pdf}{Layer Normalization vs. Batch Normalization} 示意图}}{31}{figure.caption.23}\protected@file@percent }
\newlabel{fig:layernorm}{{2.19}{31}{\href {http://proceedings.mlr.press/v119/shen20e/shen20e.pdf}{Layer Normalization vs. Batch Normalization} 示意图}{figure.caption.23}{}}
\newlabel{fig:layernorm@cref}{{[figure][19][2]2.19}{[1][31][]31}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces Residual Connection 示意图}}{32}{figure.2.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Residual Connection}{32}{figure.caption.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Point-wise Feed-Forward Network}{33}{tcb@cnt@Definition.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces Point-wise Feed-Forward Network}}{33}{figure.caption.26}\protected@file@percent }
\newlabel{fig:Point-wise Feed-Forward Network}{{2.21}{33}{Point-wise Feed-Forward Network}{figure.caption.26}{}}
\newlabel{fig:Point-wise Feed-Forward Network@cref}{{[figure][21][2]2.21}{[1][33][]33}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces Mask Matrix $M$示意图}}{34}{figure.2.22}\protected@file@percent }
\newlabel{fig:Masked Multi-Head Self-Attention}{{2.22}{34}{Mask Matrix $M$示意图}{figure.2.22}{}}
\newlabel{fig:Masked Multi-Head Self-Attention@cref}{{[figure][22][2]2.22}{[1][34][]34}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Decoder 结构}{34}{subsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Masked Multi-Head Self-Attention}{34}{section*.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces Self-Attention vs. Masked Self-Attention}}{34}{figure.caption.29}\protected@file@percent }
\newlabel{fig:Self-Attention vs. Masked Self-Attention}{{2.23}{34}{Self-Attention vs. Masked Self-Attention}{figure.caption.29}{}}
\newlabel{fig:Self-Attention vs. Masked Self-Attention@cref}{{[figure][23][2]2.23}{[1][34][]34}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Encoder-Decoder Attention(cross-attention)}{35}{figure.caption.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Encoder-Decoder Attention (Cross-Attention)}{35}{figure.caption.29}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.24}{\ignorespaces Cross-Attention}}{35}{figure.caption.30}\protected@file@percent }
\newlabel{fig:Cross-Attention}{{2.24}{35}{Cross-Attention}{figure.caption.30}{}}
\newlabel{fig:Cross-Attention@cref}{{[figure][24][2]2.24}{[1][35][]35}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{训练与推理差异}{35}{figure.caption.30}\protected@file@percent }
\newlabel{def:logits}{{2.1.5}{36}{Output Projection \& Softmax}{tcb@cnt@Definition.9}{}}
\newlabel{def:logits@cref}{{[subsection][5][2,1]2.1.5}{[1][36][]36}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Output Projection \& Softmax}{36}{subsection.2.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.25}{\ignorespaces Linear + Softmax}}{36}{figure.caption.31}\protected@file@percent }
\newlabel{fig:Linear + Softmax}{{2.25}{36}{Linear + Softmax}{figure.caption.31}{}}
\newlabel{fig:Linear + Softmax@cref}{{[figure][25][2]2.25}{[1][36][]36}{}{}{}}
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@6}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@7}}
\newmarginnote{note.37.1}{{37}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Memory Accounting}{37}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Tensors Basics}{37}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Tensors Memory}{37}{subsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.27}{\ignorespaces FP32的一些特性}}{38}{figure.2.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{float32}{38}{tcb@cnt@Definition.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.26}{\ignorespaces float32 内存结构示意图}}{38}{figure.caption.32}\protected@file@percent }
\newlabel{fig:float32 内存结构示意图}{{2.26}{38}{float32 内存结构示意图}{figure.caption.32}{}}
\newlabel{fig:float32 内存结构示意图@cref}{{[figure][26][2]2.26}{[1][38][]38}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.30}{\ignorespaces 三种Data Type的性能比较}}{39}{figure.2.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{float16}{39}{figure.2.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.28}{\ignorespaces float16 内存结构示意图}}{39}{figure.caption.33}\protected@file@percent }
\newlabel{fig:float16 内存结构示意图}{{2.28}{39}{float16 内存结构示意图}{figure.caption.33}{}}
\newlabel{fig:float16 内存结构示意图@cref}{{[figure][28][2]2.28}{[1][39][]39}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{bfloat16}{39}{tcb@cnt@Definition.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.29}{\ignorespaces float16 内存结构示意图}}{39}{figure.caption.34}\protected@file@percent }
\newlabel{fig:float16 内存结构示意图}{{2.29}{39}{float16 内存结构示意图}{figure.caption.34}{}}
\newlabel{fig:float16 内存结构示意图@cref}{{[figure][29][2]2.29}{[1][39][]39}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{fp8}{40}{figure.2.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.31}{\ignorespaces fp8 内存结构示意图}}{40}{figure.caption.35}\protected@file@percent }
\newlabel{fig:fp8 内存结构示意图}{{2.31}{40}{fp8 内存结构示意图}{figure.caption.35}{}}
\newlabel{fig:fp8 内存结构示意图@cref}{{[figure][31][2]2.31}{[1][40][]40}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Comparison of common floating-point formats for deep learning}}{40}{table.caption.36}\protected@file@percent }
\newlabel{tab:fp_all_compare}{{2.2}{40}{Comparison of common floating-point formats for deep learning}{table.caption.36}{}}
\newlabel{tab:fp_all_compare@cref}{{[table][2][2]2.2}{[1][40][]40}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.32}{\ignorespaces FP32和FP16的性能对比}}{41}{figure.2.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Mixed Precision Training \href  {https://arxiv.org/pdf/1710.03740.pdf}{[Micikevicius et al., 2017]}}{41}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{技术背景}{41}{subsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.33}{\ignorespaces FP32 vs. FP16}}{41}{figure.caption.37}\protected@file@percent }
\newlabel{fig:FP32 vs. FP16}{{2.33}{41}{FP32 vs. FP16}{figure.caption.37}{}}
\newlabel{fig:FP32 vs. FP16@cref}{{[figure][33][2]2.33}{[1][41][]41}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{核心挑战——FP16的低精度和Narrow dynamic range:}{41}{figure.caption.37}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.35}{\ignorespaces Multibox SSD 网络训练时激活梯度的分布直方图}}{42}{figure.2.35}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{技术路径}{42}{figure.caption.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{FP32 Master Copy of Weights}{42}{Item.37}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.34}{\ignorespaces Mixed precision training iteration for a layer.}}{42}{figure.caption.38}\protected@file@percent }
\newlabel{Mixed precision training iteration for a layer.}{{2.34}{42}{Mixed precision training iteration for a layer}{figure.caption.38}{}}
\newlabel{Mixed precision training iteration for a layer.@cref}{{[figure][34][2]2.34}{[1][42][]42}{}{}{}}
\@writefile{toc}{\contentsline {subparagraph}{Loss Scaling}{42}{figure.caption.38}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.36}{\ignorespaces Loss Scaling}}{42}{figure.caption.39}\protected@file@percent }
\newlabel{Loss Scaling}{{2.36}{42}{Loss Scaling}{figure.caption.39}{}}
\newlabel{Loss Scaling@cref}{{[figure][36][2]2.36}{[1][42][]42}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.38}{\ignorespaces AMP code example}}{43}{figure.2.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Arithmetic Precision}{43}{figure.caption.39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.37}{\ignorespaces  FP16 存储、Tensor Core 内部 FP32 multiply-accumulate 的数据流示意。 输入存储为低精度以提高吞吐，计算与累加在硬件内部保持高精度以确保数值稳定性。 }}{43}{figure.caption.40}\protected@file@percent }
\newlabel{fig:arithmetic_precision}{{2.37}{43}{FP16 存储、Tensor Core 内部 FP32 multiply-accumulate 的数据流示意。 输入存储为低精度以提高吞吐，计算与累加在硬件内部保持高精度以确保数值稳定性。}{figure.caption.40}{}}
\newlabel{fig:arithmetic_precision@cref}{{[figure][37][2]2.37}{[1][43][]43}{}{}{}}
\@writefile{toc}{\contentsline {subparagraph}{AMP: Automatic Mixed Precision}{43}{figure.caption.40}\protected@file@percent }
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@7}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@8}}
\newmarginnote{note.44.1}{{44}{13468164sp}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Compute Accounting}{44}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Tensors On Gpus}{44}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.39}{\ignorespaces Move tensor from CPU to GPUs}}{44}{figure.caption.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Tensor Operations}{45}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Tensor Storage}{45}{section*.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.40}{\ignorespaces Tensor Storage}}{45}{figure.caption.43}\protected@file@percent }
\newlabel{fig:Tensor Storage}{{2.40}{45}{Tensor Storage}{figure.caption.43}{}}
\newlabel{fig:Tensor Storage@cref}{{[figure][40][2]2.40}{[1][45][]45}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Tensor Slicing}{45}{lstnumber.-2.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Elementwise Operations}{46}{lstnumber.-3.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Matrix Multiplication}{46}{lstnumber.-4.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Tensor Einops}{48}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Einops: Motivation}{48}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Jaxtyping: Basics}{48}{lstnumber.-6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Einsum = Named MatMul with Bookkeeping}{48}{lstnumber.-7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Einops: Reduce (聚合)}{49}{lstnumber.-8.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Einops: Rearrange + Einsum（展开/重排 + 线性变换）}{49}{lstnumber.-9.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Tensor Operations Flops}{50}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{矩阵乘法的 FLOPs}{50}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{其他常见操作的 FLOPs}{50}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Transformer 的近似 FLOPs}{50}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model FLOPs Utilization (MFU)}{50}{subsection.2.3.4}\protected@file@percent }
\newlabel{def:mfu}{{2.3.4}{50}{Model FLOPs Utilization (MFU)}{tcb@cnt@Definition.15}{}}
\newlabel{def:mfu@cref}{{[subsection][4][2,3]2.3.4}{[1][50][]50}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Gradients Basics}{51}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{示例：简单线性模型}{51}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{要点总结}{51}{lstnumber.-11.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Gradients Flops}{52}{subsection.2.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forward FLOPs}{52}{lstnumber.-12.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backward FLOPs (示例：$w_2$)}{52}{lstnumber.-12.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backward FLOPs（示例：$w_1$）}{52}{lstnumber.-12.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{总 FLOPs 公式}{52}{lstnumber.-12.9}\protected@file@percent }
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@8}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@9}}
\newmarginnote{note.53.1}{{53}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Models}{53}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Module Parameters \& Parameters Initialization}{53}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Parameter Initialization}{53}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Custom Deep Linear Model}{54}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design idea.}{54}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Code implementation.}{54}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: building and checking the model.}{54}{lstnumber.-15.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Parameter count formula.}{55}{lstnumber.-16.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Connecting to FLOPs.}{55}{lstnumber.-16.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mini workflow (from build to one training step)}{55}{lstnumber.-16.14}\protected@file@percent }
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@9}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@10}}
\newmarginnote{note.56.1}{{56}{13468164sp}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Training Loop And Best Practices}{56}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Data Loading for Language Modeling}{56}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Batch Sampling Utility}{57}{subsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What this does (step-by-step).}{57}{lstnumber.-20.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Usage.}{57}{lstnumber.-20.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Notes and practical tips.}{57}{lstnumber.-21.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Pinned memory for faster GPU transfer.}{58}{lstnumber.-21.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Pipeline overlap.}{58}{lstnumber.-22.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Optimizer}{59}{subsection.2.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recap and relationships.}{59}{subsection.2.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Code: AdaGrad on our model.}{59}{lstnumber.-23.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Memory accounting for one training step.}{61}{tcb@cnt@Definition.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FLOPs for one step.}{61}{tcb@cnt@Definition.16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Train-loop}{62}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{流程说明。}{62}{lstnumber.-25.33}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Checkpointing (Save/Load)}{63}{section.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{要点与实践提示。}{63}{lstnumber.-26.21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Modern Models' Architecture}}{65}{figure.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Architectures variations \& Hyperparameters \& Stability tricks}{65}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@10}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@11}}
\newmarginnote{note.65.1}{{65}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The Original Transformer \& Modern Variants}{65}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Review of the Original Transformer}{65}{subsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The original Transformer architecture}}{65}{figure.caption.45}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Pre-norm vs. Post-norm in Transformer}}{66}{figure.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Normalization Variants}{66}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Pre-norm vs. Post-norm}{66}{section*.46}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \relax }}{66}{figure.caption.47}\protected@file@percent }
\newlabel{fig:}{{3.4}{66}{\relax }{figure.caption.47}{}}
\newlabel{fig:@cref}{{[figure][4][3]3.4}{[1][66][]66}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Double Norm}{66}{figure.caption.47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{LayerNorm vs. RMSNorm}{66}{figure.caption.47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Activation Variants}{66}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Serial vs. Parallel layers}{66}{subsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}RoPE: Rotary Position Embeddings}{66}{subsection.3.1.5}\protected@file@percent }
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@11}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@12}}
\newmarginnote{note.67.1}{{67}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Hyperparameters}{67}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Feedforward Dimension Ratio}{67}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Head Dimension and Model Dimension}{67}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Aspect Ratio}{67}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Vocabulary Size}{67}{subsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Regularization}{67}{subsection.3.2.5}\protected@file@percent }
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@12}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@13}}
\newmarginnote{note.68.1}{{68}{13468164sp}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Stability tricks}{68}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Output Softmax Stability}{68}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Attention Softmax Stability}{68}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Logit Soft-Capping}{68}{subsection.3.3.3}\protected@file@percent }
\ttl@writefile{ptc}{\ttl@stoptoc{chapters@13}}
\ttl@writefile{ptc}{\ttl@starttoc{chapters@14}}
\newmarginnote{note.69.1}{{69}{2752512sp}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Attention Heads}{69}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Grouped-Query Attention \& Multi-Query Attention}{69}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Sparse/Sliding Window Attention}{69}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Hybrid Approaches}{69}{subsection.3.4.3}\protected@file@percent }
\ttl@finishall
\gdef \@abspage@last{69}
