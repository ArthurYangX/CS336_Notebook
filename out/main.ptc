\contentsline {chapter}{\numberline {1}Overview and Tokenization}{1}{chapter.1}%
\ttl@starttoc {chapters@1}
\contentsline {section}{\numberline {1.1}Why this course exists}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}The industrialization of language models }{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}What can we learn in this class that transfers to frontier models? }{2}{subsection.1.1.2}%
\ttl@stoptoc {chapters@1}
\ttl@starttoc {chapters@2}
\contentsline {section}{\numberline {1.2}Current Landscape}{3}{section.1.2}%
\ttl@stoptoc {chapters@2}
\ttl@starttoc {chapters@3}
\contentsline {section}{\numberline {1.3}Course Components}{5}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Basics}{6}{subsection.1.3.1}%
\contentsline {subsubsection}{Tokenization}{6}{subsection.1.3.1}%
\contentsline {subsubsection}{Architecture}{6}{tcb@cnt@Definition.2}%
\contentsline {subsubsection}{Training}{6}{figure.1.5}%
\contentsline {subsection}{\numberline {1.3.2}Systems}{7}{subsection.1.3.2}%
\contentsline {subsubsection}{Kernels}{7}{subsection.1.3.2}%
\contentsline {subsubsection}{Parallelism}{8}{figure.caption.2}%
\contentsline {subsubsection}{Inference}{8}{figure.caption.2}%
\contentsline {subsection}{\numberline {1.3.3}Scaling Laws}{9}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Data}{9}{subsection.1.3.4}%
\contentsline {subsubsection}{Evaluation}{9}{figure.caption.4}%
\contentsline {subsubsection}{Data curation}{10}{figure.caption.4}%
\contentsline {subsubsection}{Data processing}{10}{figure.caption.4}%
\contentsline {subsection}{\numberline {1.3.5}Alignment}{10}{subsection.1.3.5}%
\contentsline {paragraph}{Supervised Fine-Tuning (SFT)}{10}{figure.1.11}%
\contentsline {paragraph}{Learning from Feedback}{11}{figure.1.11}%
\ttl@stoptoc {chapters@3}
\ttl@starttoc {chapters@4}
\contentsline {section}{\numberline {1.4}Tokenization}{12}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Intro}{12}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Observations}{12}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Character Tokenizer}{13}{subsection.1.4.3}%
\contentsline {paragraph}{Problems}{13}{figure.1.15}%
\contentsline {subsection}{\numberline {1.4.4}Byte Tokenizer}{13}{subsection.1.4.4}%
\contentsline {paragraph}{Drawback}{13}{figure.1.16}%
\contentsline {subsection}{\numberline {1.4.5}Word Tokenizer}{14}{subsection.1.4.5}%
\contentsline {paragraph}{Drawback:}{14}{figure.1.17}%
\contentsline {subsection}{\numberline {1.4.6}BPE Tokenizer}{14}{subsection.1.4.6}%
\contentsline {subsection}{\numberline {1.4.7}Tokenizer Properties}{16}{subsection.1.4.7}%
\contentsline {paragraph}{There are three major groups of design choices that determine how the tokenizer will break down text:}{16}{subsection.1.4.7}%
\contentsline {paragraph}{The tokenization method}{16}{Item.15}%
\contentsline {paragraph}{The initialization parameters}{16}{Item.15}%
\contentsline {paragraph}{The domain of the data the tokenizer targets}{16}{Item.15}%
\contentsline {subsection}{\numberline {1.4.8}Summary of Tokenization Methods}{17}{subsection.1.4.8}%
\contentsline {chapter}{\numberline {2}PyTorch, Resource Accounting}{19}{chapter.2}%
\ttl@stoptoc {chapters@4}
\ttl@starttoc {chapters@5}
\contentsline {section}{\numberline {2.1}Original Transformer}{19}{section.2.1}%
\ttl@stoptoc {chapters@5}
\ttl@starttoc {chapters@6}
\contentsline {subsection}{\numberline {2.1.1}Input Embedding}{21}{subsection.2.1.1}%
\contentsline {subsubsection}{Tokenization}{21}{figure.caption.9}%
\contentsline {subsubsection}{Embedding}{21}{figure.caption.9}%
\contentsline {paragraph}{1. Static Token Embeddings vs. Contextualized Embeddings}{21}{tcb@cnt@Definition.4}%
\contentsline {paragraph}{2. Text and Sentence Embeddings}{22}{figure.2.4}%
\contentsline {paragraph}{3. Word2Vec}{22}{tcb@cnt@Definition.5}%
\contentsline {subsection}{\numberline {2.1.2}Positional Encoding}{23}{subsection.2.1.2}%
\contentsline {paragraph}{1.å›ºå®šæ­£ä½™å¼¦ä½ç½®ç¼–ç ï¼ˆVaswaniåŸç‰ˆï¼‰}{23}{subsection.2.1.2}%
\contentsline {paragraph}{2.å¯å­¦ä¹ ä½ç½®ç¼–ç }{23}{Item.23}%
\contentsline {subsection}{\numberline {2.1.3}Encoder ç»“æ„}{24}{subsection.2.1.3}%
\contentsline {subsubsection}{å¤šå¤´è‡ªæ³¨æ„æœºåˆ¶}{24}{figure.caption.13}%
\contentsline {paragraph}{Step0:è¿›è¡ŒSelf-Attentionä¹‹å‰çš„è¾“å…¥å’ŒæŠ•å½±çŸ©é˜µ}{26}{equation.2.1}%
\contentsline {paragraph}{Step1:æ ¹æ®æŠ•å½±çŸ©é˜µæ›´æ–°$Q$,$K$,$V$}{27}{figure.caption.16}%
\contentsline {paragraph}{Step2:è®¡ç®—ç›¸å…³æ€§åˆ†æ•°å¹¶è¿›è¡ŒSoftmaxå½’ä¸€åŒ–}{28}{figure.caption.17}%
\contentsline {paragraph}{Step3:ç”¨æ³¨æ„åŠ›æƒé‡å¯¹$ğ‘‰$æƒæ±‚å’Œï¼Œå¾—åˆ°è¾“å‡º}{29}{tcb@cnt@Definition.7}%
\contentsline {paragraph}{Step4: å¤šå¤´æ³¨æ„åŠ›çš„è¾“å‡ºåˆå¹¶}{30}{figure.caption.19}%
\contentsline {paragraph}{Step5: Add \& Norm}{31}{Item.30}%
\contentsline {subparagraph}{Layer Normalization}{31}{Item.30}%
\contentsline {subparagraph}{Residual Connection}{32}{figure.caption.21}%
\contentsline {subsubsection}{Point-wise Feed-Forward Network}{33}{tcb@cnt@Definition.8}%
\contentsline {subsection}{\numberline {2.1.4}Decoder ç»“æ„}{34}{subsection.2.1.4}%
\contentsline {subsubsection}{Masked Multi-Head Self-Attention}{34}{subsection.2.1.4}%
\contentsline {subsubsection}{Encoder-Decoder Attention(cross-attention)}{35}{figure.caption.26}%
\contentsline {subsubsection}{Encoder-Decoder Attention (Cross-Attention)}{35}{figure.caption.26}%
\contentsline {subsubsection}{è®­ç»ƒä¸æ¨ç†å·®å¼‚}{35}{figure.caption.27}%
\contentsline {subsection}{\numberline {2.1.5}Output Projection \& Softmax}{36}{subsection.2.1.5}%
\ttl@stoptoc {chapters@6}
\ttl@starttoc {chapters@7}
\contentsline {section}{\numberline {2.2}Memory Accounting}{37}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Tensors Basics}{37}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Tensors Memory}{37}{subsection.2.2.2}%
\contentsline {subsubsection}{float32}{38}{tcb@cnt@Definition.13}%
\contentsline {subsubsection}{float16}{39}{figure.2.27}%
\contentsline {subsubsection}{bfloat16}{39}{tcb@cnt@Definition.14}%
\contentsline {subsubsection}{fp8}{40}{figure.2.30}%
\contentsline {subsection}{\numberline {2.2.3}Mixed Precision Training \href {https://arxiv.org/pdf/1710.03740.pdf}{[Micikevicius et al., 2017]}}{41}{subsection.2.2.3}%
\contentsline {paragraph}{æŠ€æœ¯èƒŒæ™¯}{41}{subsection.2.2.3}%
\contentsline {paragraph}{æ ¸å¿ƒæŒ‘æˆ˜â€”â€”FP16çš„ä½ç²¾åº¦å’ŒNarrow dynamic range:}{41}{figure.caption.34}%
\contentsline {paragraph}{æŠ€æœ¯è·¯å¾„}{42}{figure.caption.34}%
\contentsline {subparagraph}{FP32 Master Copy of Weights}{42}{Item.37}%
\contentsline {subparagraph}{Loss Scaling}{42}{figure.caption.35}%
\contentsline {subparagraph}{Arithmetic Precision}{43}{figure.caption.36}%
\contentsline {subparagraph}{AMP: Automatic Mixed Precision}{43}{figure.caption.37}%
\ttl@stoptoc {chapters@7}
\ttl@starttoc {chapters@8}
\contentsline {section}{\numberline {2.3}Compute Accounting}{44}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Tensors On Gpus}{44}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Tensor Operations}{45}{subsection.2.3.2}%
\contentsline {subsubsection}{Tensor Storage}{45}{subsection.2.3.2}%
\contentsline {subsubsection}{Tensor Slicing}{45}{lstnumber.-2.11}%
\contentsline {subsubsection}{Elementwise Operations}{46}{lstnumber.-3.37}%
\contentsline {subsubsection}{Matrix Multiplication}{46}{lstnumber.-4.15}%
\contentsline {subsection}{\numberline {2.3.3}Tensor Einops}{48}{subsection.2.3.3}%
\contentsline {subsubsection}{Einops: Motivation}{48}{subsection.2.3.3}%
\contentsline {subsubsection}{Jaxtyping: Basics}{48}{lstnumber.-6.4}%
\contentsline {subsubsection}{Einsum = Named MatMul with Bookkeeping}{48}{lstnumber.-7.6}%
\contentsline {subsubsection}{Einops: Reduce (èšåˆ)}{49}{lstnumber.-8.17}%
\contentsline {subsubsection}{Einops: Rearrange + Einsumï¼ˆå±•å¼€/é‡æ’ + çº¿æ€§å˜æ¢ï¼‰}{49}{lstnumber.-9.10}%
\contentsline {subsection}{\numberline {2.3.4}Tensor Operations Flops}{50}{subsection.2.3.4}%
\contentsline {paragraph}{çŸ©é˜µä¹˜æ³•çš„ FLOPs}{50}{subsection.2.3.4}%
\contentsline {paragraph}{å…¶ä»–å¸¸è§æ“ä½œçš„ FLOPs}{50}{subsection.2.3.4}%
\contentsline {paragraph}{Transformer çš„è¿‘ä¼¼ FLOPs}{50}{subsection.2.3.4}%
\contentsline {paragraph}{Model FLOPs Utilization (MFU)}{50}{subsection.2.3.4}%
\contentsline {subsection}{\numberline {2.3.5}Gradients Basics}{51}{subsection.2.3.5}%
\contentsline {paragraph}{ç¤ºä¾‹ï¼šç®€å•çº¿æ€§æ¨¡å‹}{51}{subsection.2.3.5}%
\contentsline {paragraph}{è¦ç‚¹æ€»ç»“}{51}{lstnumber.-11.19}%
\contentsline {subsection}{\numberline {2.3.6}Gradients Flops}{52}{subsection.2.3.6}%
\contentsline {paragraph}{Forward FLOPs}{52}{lstnumber.-12.9}%
\contentsline {paragraph}{Backward FLOPs (ç¤ºä¾‹ï¼š$w_2$)}{52}{lstnumber.-12.9}%
\contentsline {paragraph}{Backward FLOPsï¼ˆç¤ºä¾‹ï¼š$w_1$ï¼‰}{52}{lstnumber.-12.9}%
\contentsline {paragraph}{æ€» FLOPs å…¬å¼}{52}{lstnumber.-12.9}%
\ttl@stoptoc {chapters@8}
\ttl@starttoc {chapters@9}
\contentsline {section}{\numberline {2.4}Models}{53}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Module Parameters \& Parameters Initialization}{53}{subsection.2.4.1}%
\contentsline {paragraph}{Parameter Initialization}{53}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Custom Deep Linear Model}{54}{subsection.2.4.2}%
\contentsline {paragraph}{Design idea.}{54}{subsection.2.4.2}%
\contentsline {paragraph}{Code implementation.}{54}{subsection.2.4.2}%
\contentsline {paragraph}{Example: building and checking the model.}{54}{lstnumber.-15.28}%
\contentsline {paragraph}{Parameter count formula.}{55}{lstnumber.-16.14}%
\contentsline {paragraph}{Connecting to FLOPs.}{55}{lstnumber.-16.14}%
\contentsline {paragraph}{Mini workflow (from build to one training step)}{55}{lstnumber.-16.14}%
\ttl@stoptoc {chapters@9}
\ttl@starttoc {chapters@10}
\contentsline {section}{\numberline {2.5}Training Loop And Best Practices}{56}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Data Loading for Language Modeling}{56}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}Batch Sampling Utility}{57}{subsection.2.5.2}%
\contentsline {paragraph}{What this does (step-by-step).}{57}{lstnumber.-20.12}%
\contentsline {paragraph}{Usage.}{57}{lstnumber.-20.12}%
\contentsline {paragraph}{Notes and practical tips.}{57}{lstnumber.-21.4}%
\contentsline {paragraph}{Pinned memory for faster GPU transfer.}{58}{lstnumber.-21.4}%
\contentsline {paragraph}{Pipeline overlap.}{58}{lstnumber.-22.3}%
\contentsline {subsection}{\numberline {2.5.3}Optimizer}{59}{subsection.2.5.3}%
\contentsline {paragraph}{Recap and relationships.}{59}{subsection.2.5.3}%
\contentsline {paragraph}{Code: AdaGrad on our model.}{59}{lstnumber.-23.4}%
\contentsline {paragraph}{Memory accounting for one training step.}{61}{tcb@cnt@Definition.16}%
\contentsline {paragraph}{FLOPs for one step.}{61}{tcb@cnt@Definition.16}%
\contentsline {section}{\numberline {2.6}Train-loop}{62}{section.2.6}%
\contentsline {paragraph}{æµç¨‹è¯´æ˜ã€‚}{62}{lstnumber.-25.33}%
\contentsline {section}{\numberline {2.7}Checkpointing (Save/Load)}{63}{section.2.7}%
\contentsline {paragraph}{è¦ç‚¹ä¸å®è·µæç¤ºã€‚}{63}{lstnumber.-26.21}%
\contentsline {chapter}{\numberline {3}Architectures variations \& Hyperparameters \& Stability tricks}{65}{chapter.3}%
\ttl@stoptoc {chapters@10}
\ttl@starttoc {chapters@11}
\contentsline {section}{\numberline {3.1}The Original Transformer \& Modern Variants}{65}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Review of the Original Transformer}{65}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Normalization Variants}{66}{subsection.3.1.2}%
\contentsline {subsubsection}{Pre-norm vs. Post-norm in Transformer}{66}{figure.3.3}%
\contentsline {paragraph}{1. Pre-LayerNorm å¯ä»¥å»é™¤æ¨¡å‹å¯¹äºwarm-upé˜¶æ®µçš„ä¾èµ–}{66}{figure.caption.41}%
\contentsline {paragraph}{2. Pre-LayerNorm å¯ä»¥ç¼“è§£æ¢¯åº¦çˆ†ç‚¸é—®é¢˜}{68}{tcb@cnt@Proposition.1}%
\contentsline {paragraph}{3. Pre-LayerNorm å¯ä»¥æé«˜æ•´ä½“çš„ç¨³å®šæ€§å¹¶æé«˜Learning RateåŠ é€Ÿæ”¶æ•›}{68}{figure.caption.44}%
\contentsline {subsubsection}{Double Norm}{69}{figure.caption.45}%
\contentsline {subsubsection}{LayerNorm vs. RMSNorm}{70}{figure.caption.46}%
\contentsline {subsection}{\numberline {3.1.3}Activation Variants}{72}{subsection.3.1.3}%
\contentsline {paragraph}{A few of the common activations}{72}{subsection.3.1.3}%
\contentsline {paragraph}{Gated activations(*GLU)}{72}{AMS.51}%
\contentsline {subsection}{\numberline {3.1.4}Serial vs. Parallel layers}{73}{subsection.3.1.4}%
\contentsline {subsection}{\numberline {3.1.5}RoPE: Rotary Position Embeddings}{74}{subsection.3.1.5}%
\contentsline {paragraph}{A few of commom embeddings}{74}{subsection.3.1.5}%
\contentsline {paragraph}{High level thought process: Relativity }{74}{subsection.3.1.5}%
\contentsline {paragraph}{RoPE: rotary position embeddings}{75}{subsection.3.1.5}%
\contentsline {subsubsection}{General form}{76}{figure.caption.53}%
\contentsline {subsection}{\numberline {3.1.6}Properties of RoPE}{77}{subsection.3.1.6}%
\contentsline {paragraph}{é•¿ç¨‹è¡°å‡ (Long-term decay).}{77}{subsection.3.1.6}%
\contentsline {paragraph}{ç»“åˆçº¿æ€§æ³¨æ„åŠ› (RoPE with linear attention).}{77}{subsection.3.1.6}%
\contentsline {paragraph}{è¯´æ˜.}{77}{equation.3.4}%
\ttl@stoptoc {chapters@11}
\ttl@starttoc {chapters@12}
\contentsline {section}{\numberline {3.2}Hyperparameters}{78}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Feedforward Dimension Ratio}{78}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Head Dimension and Model Dimension}{78}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Aspect Ratio}{78}{subsection.3.2.3}%
\contentsline {subsection}{\numberline {3.2.4}Vocabulary Size}{78}{subsection.3.2.4}%
\contentsline {subsection}{\numberline {3.2.5}Regularization}{78}{subsection.3.2.5}%
\ttl@stoptoc {chapters@12}
\ttl@starttoc {chapters@13}
\contentsline {section}{\numberline {3.3}Stability tricks}{79}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Output Softmax Stability}{79}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Attention Softmax Stability}{79}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Logit Soft-Capping}{79}{subsection.3.3.3}%
\ttl@stoptoc {chapters@13}
\ttl@starttoc {chapters@14}
\contentsline {section}{\numberline {3.4}Attention Heads}{80}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Grouped-Query Attention \& Multi-Query Attention}{80}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Sparse/Sliding Window Attention}{80}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Hybrid Approaches}{80}{subsection.3.4.3}%
\contentsfinish 
